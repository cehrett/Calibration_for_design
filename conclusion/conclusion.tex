\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{natbib} 
\usepackage{amsmath, amssymb}
\usepackage{setspace} 
\usepackage[margin=1.25in]{geometry}

\date{}
\setcounter{page}{128}
\begin{document}
	
\def\spacingset#1{\renewcommand{\baselinestretch}%
	{#1}\small\normalsize} \spacingset{1}


\title{\bf Conclusion}
\author{Carl Ehrett\\
	Watt Family Innovation Center, \\Clemson University}
\maketitle

\spacingset{2} 



%\documentclass[10pt,a4paper]{article}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{graphicx}
%\usepackage{natbib}
%\author{Carl Ehrett}
%\title{Conclusion}
%
%
%
%\begin{document}
%	
%\maketitle

\section{Benefits}

The research presented in the previous chapters addresses two distinct desiderata related to model-assisted design.
Firstly, there is the desideratum of undertaking model-assisted design in a way that accounts for all forms of uncertainty -- uncertainty due to the model inputs, uncertainty due to the stochastic nature of the objective function, and/or uncertainty due to observation error of the outputs.
All of these sources of uncertainty can be modeled and included in the Bayesian framework used to employ our methodology.
The resulting posterior distribution of the design inputs quantifies uncertainty as to what inputs could lead to optimal system behavior.
The corresponding posterior predictive distributions quantifies uncertainty as to that resulting system behavior, including uncertainty of the entire Pareto front of the system.
In contrast with approaches such as that of \citet{Olalotiti-Lawal2015}, who provide similar uncertainty quantification of design input settings using a distribution they contrive, our method provides the uncertainty in a posterior distribution that is directly dictated by what is known about the computer model itself, by one's prior knowledge about the appropriate design settings, and by the priorities of decision-makers.

Our method furthermore evades the need to be able to evaluate the objective function adaptively.
This requirement is a limitation shared by most other Bayesian optimization (BO) methods.
In this way, our method may be employed in scenarios where researchers are confined to the usage of pre-existing data sets, or in scenarios where the experimental design used for data-gathering must satisfy priorities other than that of engineering design.

Secondly, there is the desideratum of unifying procedures for calibration and design.
Typically these two tasks are undertaken separately; a model would be calibrated and then the calibrated model would be put to use for model-assisted design.
However, design priorities arise for models that stand in need of calibration, and wedding the frameworks for calibration and design allows for a single use of a dataset to satisfy both sets of goals.

\section{Summary of chapter two}

In chapter 2, we focused on model-assisted design, assuming there the possession of an already-calibrated computer model.
We there considered the problem of engineering design from the perspective of, and using tools from the field of, computer model calibration under uncertainty.
Specifically, we approached model-assisted design from the Bayesian framework for model calibration developed by \citet{Kennedy2001}, which we here refer to as KOH.
We used this approach to undertake model-assisted design on a multi-objective system, quantifying remaining uncertainty regarding the optimal values of the design inputs and the resulting model output.

To do this, we relied on a method we call Counterfactual Bayes.
Ordinarily, under KOH, one has access to a set of experimental observations that one uses to calibrate a model, so that the model output approximates those observations well.
In the case of design, we wish to induce the model output to be \textit{optimal}; we do not have access to any particular set of real experimental observations that are relevant to that goal.
However, we argued, we may apply the KOH framework to design by reasoning using hypothetical -- i.e.\ counterfactual -- observations which would only occur if the design settings \textit{were} optimal.
By selecting such \textit{target outcomes}, we are able to apply KOH and thereby discover distributions of design inputs that induce the model to approximate the hypothetical target outcomes.
Since the target outcomes could be observed only when the design settings are optimal, our posterior distributions of design inputs achieving the target outcomes are \textit{de facto} distributions of \textit{optimal} design inputs.
Thus we showed that KOH can be enlisted through Counterfactual Bayes as a powerful methodology for model-assisted design with quantification of all relevant uncertainty.
In our discussion we included guidance as to the appropriate choice of target outcomes.
Specifically, we demonstrated how, with little added computational cost, an initial ``rough estimate'' of the system Pareto front can be generated and used to select effective target outcomes.

To accommodate models of high computational expense, we employed GP surrogate models.
We offered guidance on the selection of prior distributions for model inputs and for GP hyperparameters, as there are some aspects of these choices that are particular to the use of KOH with target outcomes.
We then described an algorithm for the repeated application of our methodology in order to estimate not merely the optimal design settings with respect to some one goal, but instead to estimate the entire Pareto front for the system of interest.
This allows decision-makers full flexibility in selecting a design to meet any set of priorities.

We demonstrated our methodology on a simulated example, where we were able to display the results of the procedure using the known optimum of the system.
We also demonstrated our methodology, including the algorithm for full Pareto front estimation, on an application of material design for a wind turbine blade.
We included a comparison showing that our resulting estimated Pareto front agrees with an estimate provided by NSGA-II \citep{Deb2002}, and that our estimate (unlike that of NSGA-II) is able to provide credible bands quantifying uncertainty remaining in the Pareto front location.

We concluded that our method captures much of what is attractive in other BO methods, without requiring the ability to sample the objective function adaptively.
Our method thus is a useful addition to the set of tools available to a researcher wishing to undertake model-assisted multi-objective design under uncertainty.

\section{Summary of chapter three}

In chapter three, we broadened our methodology to include simultaneous calibration and design.
Whereas these two procedures are typically carried out separately, we show that our adaptation of the Bayesian KOH framework is able to accommodate both tasks in a unified framework.
This unified framework is a computationally efficient method of undertaking calibration and design with quantification of relevant uncertainties.

In KOH, one calibrates a model using a set of experimental observations.
If the model is computationally expensive, then one also uses a set of ``observations'' of the model instead of evaluating the model as part of the calibration procedure.
Thus KOH weds experimental observations and model observations for the purpose of calibration.
In chapter two, we replaced the model observations with target outcomes for the purpose of model-assisted design.
In chapter three, we expand the KOH framework so that it weds all three of these: experimental observations, model observations, and target outcomes.
We described the joint distribution of the concatenation of these three vectors, allowing for the exploration of the joint posterior distribution of the calibration and design parameters.
We made clear in what way the resulting framework successfully separates calibration from design -- so that, e.g., the posterior distribution of the calibration parameters would be drawn to their \textit{true} values (rather than values that would optimally yield the target outcomes) and the posterior distribution of the design inputs would similarly be drawn to their optimal values.
We furthermore described the modularization \citep{Liu2009} that we employ in order to prevent the calibration parameters from being unduly influenced by the target outcomes.

Having established our methodology for dual calibration and design, we demonstrated using a simulated data set, in which the ``true'' calibration parameter and optimal design input were known.
In order to demonstrate the flexibility of the method and its robustness to model bias, we considered seven different versions of the simulated system: a version without model bias, and a small and large version of each of three different discrepancies of different forms.
Using MCMC to explore the posterior distributions of the calibration and design inputs, we showed the resulting success of the methodology in each of these cases, while also noting the (expected) degradation in performance as model bias grows large.

To further demonstrate the usefulness of the method, we showed how it may be of special value when the calibration parameter is functionally dependent upon the design input and adaptive sampling of the high-fidelity model is possible.
Toward this end, we considered another simulated system, in which such dependence occurs.
We described an algorithm for wedding our dual calibration and design to an adaptive sampling strategy that induces the high-cost observations to be clustered in the region of greatest interest from the perspective of design.
The result is that the model calibration is grounded on observations that occur when the value of the calibration parameter is near to its value at the optimal state of the system.
We again demonstrated the results of this approach using seven different discrepancy cases.

Finally, in addition to the simulated systems described above, we applied our methodology to the vibration isolation design application.
We demonstrated the ability of our framework to identify plausible posterior distributions on the calibration parameter of leaf spring elastic modulus and the design parameter of the gain factor.
Having demonstrated the results of our methodology, we concluded that our expanded alteration of the KOH framework is able to provide unified calibration and design with similarly unified uncertainty quantification.
In this way the researcher sees not merely the marginal uncertainty of the calibration parameters and that of the design inputs, but also sees their joint posterior distribution and the associated uncertainty.
The uncertainty of the posterior predictive distribution of model behavior is thus captured as well.


\section{Recommendations}

We have demonstrated the novel benefits of the present work's method.
However, it is worthwhile to note the areas in which the method may have limited applicability.
In general, the weaknesses of the present approach are the weaknesses shared with the KOH framework generally.
Specifically, when relying on GP surrogates, the computational infelicities of GPs apply.
GPs may struggle with high-dimensional data, and also with large data sets.
In general, to use a data set of size $n$ for a GP, one must invert a matrix of size $n$.
Happily, while the present approach shares the challenges of KOH in this area, it can similarly partake in the large body of work devoted to alleviating or mitigating these problems.
Works described in Chapter 1 such as that of  \citet{Higdon2008a}, \citet{Bhat2010}, \citet{Paulo2012}, \citet{Drignei2012}, \citet{Pratola2013}, and \citet{Higdon2013} are all applicable to the present work to improve computational efficiency and stability.

A benefit of the present approach is that, in contrast with many other Bayesian approaches to model-assisted design, it does not require the ability to sample adaptively from the objective function.
Conversely, however, when adaptive sampling is possible, it has appreciable benefits in reducing the total number of objective function evaluations needed for design \citep{Jones1998,Vazquez2009,Bect2012,Chevalier2014}.
Nonetheless, as described in Chapter 3, these BO approaches do not accommodate the case in which simultaneous calibration and design are undertaken.
The present approach may still be advantageous here, especially when, as in Chapter 3, there is a known or suspected functional dependency of the calibration parameters upon the design inputs.

When one's data allows for a GP surrogate of reasonable computational efficiency (or when one's objective function is of low enough computational efficiency that a surrogate is not required), and when one either wishes to undertake simultaneous calibration and design (either for efficiency or because of a suspected functional relationship of calibration parameters on design inputs) or else one is constrained not to employ adaptive sampling of the objective function, then we recommend the application of the method presented in this work.

\section{Future directions}

We remarked above that the framework advocated here is compatible with broad efforts to increase the scalability of MCMC and of GPs.
A fruitful next step for research in this area would be to effect such integration and to explore its effectiveness in relevant applications.
This can take a number of forms, as there exist a wide array of strategies for improving the computational feasibility and stability of MCMC and of GPs in high-dimensional and large-data contexts.
It would also be fruitful to apply the present approach to cases in which the design space contains proper subsets that are infeasible, by imposing regularization to constrain the solution space (e.g. as in \cite{Calvetti2014}).
Similarly, whereas in the present work we focus on unconstrained problems, it would be instructive to combine our approach with available methods for applying GPs that respect known constraints \citep{Golchi2015,Wang2016,Maatouk2017,Ding2019}.
Another potential area for future research is the union of our dual calibration and design approach with existing BO methods for selecting new sampling locations, such as the EGO approach of \citet{Jones1998} or the SUR approach of \citet{Vazquez2009}, \citet{Bect2012} and \citet{Chevalier2014}.

\bibliographystyle{apalike}

\bibliography{lit_review}
	
\end{document}