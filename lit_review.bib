@article{Atamturktur2017,
abstract = {In experiment-based validation, uncertainties and systematic biases in model predictions are reduced by either increasing the amount of experimental evidence available for model calibration—thereby mitigating prediction uncertainty—or increasing the rigor in the definition of physics and/or engineering principles—thereby mitigating prediction bias. Hence, decision makers must regularly choose between either allocating resources for experimentation or further code development. The authors propose a decision-making framework to assist in resource allocation strictly from the perspective of predictive maturity and demonstrate the application of this framework on a nontrivial problem of predicting the plastic deformation of polycrystals.},
annotate = {To read},
author = {Atamturktur, S and Hegenderfer, J and Williams, B and Egeberg, M and Lebensohn, R A and Unal, C},
doi = {10.1080/15376494.2013.828819},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Atamturktur et al. - 2017 - Mechanics of Advanced Materials and Structures A Resource Allocation Framework for Experiment- Based Validat.pdf:pdf},
issn = {1537-6532},
keywords = {Bayesian inference,material plasticity models,model calibration,predictive maturity,uncertainty quantification,viscoplastic self-consistent},
title = {{Mechanics of Advanced Materials and Structures A Resource Allocation Framework for Experiment- Based Validation of Numerical Models A Resource Allocation Framework for Experiment-Based Validation of Numerical Models}},
url = {http://www.tandfonline.com/action/journalInformation?journalCode=umcm20 http://dx.doi.org/10.1080/15376494.2013.828819},
year = {2017}
}
@article{Bastos2009,
abstract = {Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different...},
annotate = {Gaussian process emulators, their advantages and disadvantages, are discussed along with diagnostics to assess successful emulation.},
author = {Bastos, Leonardo S. and O'Hagan, Anthony},
doi = {10.1198/TECH.2009.08019},
file = {:C$\backslash$:/Users/carle/Desktop/Diagnostics for Gaussian Process Emulators.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian inference,Computer experiments,Diagnostics,Emulation,Gaussian process},
month = {nov},
number = {4},
pages = {425--438},
publisher = {Taylor {\&} Francis},
title = {{Diagnostics for Gaussian Process Emulators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2009.08019},
volume = {51},
year = {2009}
}
@article{Bayarri,
abstract = {A key question in evaluation of computer models is Does the computer model adequately represent reality? A six-step process for computer model validation is set out in Bayarri et al. [Technometrics 49 (2007) 138-154] (and briefly summarized below), based on comparison of computer model runs with field data of the process being modeled. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different, but related scenarios. Two complications that frequently arise in practice are the need to deal with highly irregular functional data and the need to acknowledge and incorporate uncertainty in the inputs. We develop methodology to deal with both complications. A key part of the approach utilizes a wavelet representation of the functional data, applies a hierarchical version of the scalar validation methodology to the wavelet coefficients, and transforms back, to ultimately compare computer model output with field output. The generality of the methodology is only limited by the capability of a combination of computational tools and the appropriateness of decompositions of the sort (wavelets) employed here. The methods and analyses we present are illustrated with a test bed dynamic stress analysis for a particular engineering system.},
annotate = {r2017-07-12; 2017-02-03
Six steps for their technique: 1. define problem 2. establish evaluation criteria 3. design experiments 4. approximate computer model output 5. analyze the combo of field and computer run data 6. feed back to revise the model, perform additional experiments, and so on.
Current paper attempts to deal with 3 new problems: 1. irregular function output 2. uncertainty in inputs 3. prediction in altered/new settings
The authors exhort the use of "tolerance bounds" as measures of model accuracy. One reason is that tolerance bounds account for model bias. See Bayarri et al [4] for details. Overview: sec. 2 describes example problem. Sec. 3 formulates the statistical problem and assumptions. Sec. 4 has methods of analysis. Sec. 5, results.},
author = {Bayarri, M. J. and Berger, J. O. and Cafeo, J. and Garcia-Donato, G. and Liu, F. and Palomo, J. and Parthasarathy, R. J. and Paulo, R. and Sacks, J. and Walsh, D.},
doi = {10.2307/25464566},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayarri et al. - 2007 - Computer Model Validation with Functional Output.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {Approximation,Calibration,Computer modeling,Experiment design,Modeling,Parametric models,Simulations,Statistical models,Statistics,Vehicles},
pages = {1874--1906},
publisher = {Institute of Mathematical Statistics},
title = {{Computer Model Validation with Functional Output}},
url = {http://www.jstor.org/stable/25464566},
volume = {35},
year = {2007}
}
@article{Bayarri2007,
abstract = {We present a framework that enables computer model evaluation oriented toward answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based on Bayesian and likelihood methodology. The Bayesian methodology is particularly well suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models, combining multiple sources of information, and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is implemented in a test bed example of resistance spot welding, to provide context for each of the six steps in the proposed validation process.},
annotate = {r2018-01-10; 2017-07-11
The authors advance the method of using "modularization" which Berger and Bayarri focus on more thoroughly in their 2009 paper with Liu. The paper includes useful citations on dealing with high-dimensional output data (p. 141). The authors note (p.145) that a non-constant mean function may be called for if you want extrapolation from your GP model. There is some discussion of the trade-offs between using MLEs for hyperparameters or using a full Bayesian analysis; the authors tentatively weigh in for the former option. The authors assert (p. 150) that they have shown that, contrary to common opinion, data can be used simultaneously for calibration/tuning and for model validation, by "incorporating the posterior distribution of the tuning parameters in the overall assessment of uncertainties."},
author = {Bayarri, Maria J and Berger, James O and Paulo, Rui and Sacks, Jerry and Cafeo, John A and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
doi = {10.1198/004017007000000092},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayarri et al. - 2007 - A Framework for Validation of Computer Models.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian analysis,Identifiability,Model discrepancy,Prediction},
month = {may},
number = {2},
pages = {138--154},
publisher = {Taylor {\&} Francis},
title = {{A Framework for Validation of Computer Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017007000000092},
volume = {49},
year = {2007}
}
@article{Brown2016,
abstract = {Standard methods in computer model calibration treat the calibration parameters as constant throughout the domain of control inputs. In many applications, systematic variation may cause the best values for the calibration parameters to change between different settings. When not accounted for in the code, this variation can make the computer model inadequate. In this article, we propose a framework for modeling the calibration parameters as functions of the control inputs to account for a computer model's incomplete system representation in this regard while simultaneously allowing for possible constraints imposed by prior expert opinion. We demonstrate how inappropriate modeling assumptions can mislead a researcher into thinking a calibrated model is in need of an empirical discrepancy term when it is only needed to allow for a functional dependence of the calibration parameters on the inputs. We apply our approach to plastic deformation of a visco-plastic self-consistent material in which the critical resolved shear stress is known to vary with temperature.},
annotate = {r2018-01-25; 2017-05-11
The notion of state-aware calibration is introduced and discussed. There are several useful tips for computation, especially on pages 6 and 7.},
archivePrefix = {arXiv},
arxivId = {1602.06202},
author = {Brown, D. Andrew and Atamturktur, Sez},
doi = {10.5705/ss.202015.0344},
eprint = {1602.06202},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Atamturktur - 2016 - Nonparametric Functional Calibration of Computer Models.pdf:pdf},
month = {feb},
title = {{Nonparametric Functional Calibration of Computer Models}},
url = {http://arxiv.org/abs/1602.06202 http://dx.doi.org/10.5705/ss.202015.0344},
year = {2016}
}
@article{Brynjarsdottir2014,
abstract = {Science-based simulation models are widely used to predict the behavior of complex physical systems. It is also common to use observations of the physical system to solve the inverse problem, i.e. to learn about the values of parameters within the model, a process often called calibration. The main goal of calibration is usually to improve the predictive performance of the simulator but the values of the parameters in the model may also be of intrinsic scientific interest in their own right. In order to make appropriate use of observations of the physical system it is impor-tant to recognise model discrepancy, the difference between reality and the simulator output. We illustrate through a simple example that an analysis that does not account for model discrepancy may lead to biased and over-confident parameter estimates and predictions. The challenge with incorporating model discrepancy in statistical inverse problems is the confounding with calibration parameters, which will only be resolved with mean-ingful priors. For our simple example, we model the model-discrepancy via a Gaus-sian Process and demonstrate that by accounting for model discrepancy our prediction within the range of data is correct. However, only with realistic priors on the model discrepancy do we uncover the true parameter values. Through theoretical arguments we show that these findings are typical of the general problem of learning about physical parameters and the underlying physical system using science-based mechanistic models.},
annotate = {r2018-01-31; 2017-09-08
Looks like in order to take the lesson of this paper to heart, one would have to be really thoughtful about the physical ground of the discrepancy function, in crafting as narrow constraints as possible. No black boxes here.},
author = {Brynjarsd{\'{o}}ttir, Jenni and Ohagan, Anthony},
doi = {10.1088/0266-5611/30/11/114007},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brynjarsd{\'{o}}ttir, Ohagan - 2014 - Learning about physical parameters The importance of model discrepancy(2).pdf:pdf},
issn = {13616420},
journal = {Inverse Problems},
keywords = {calibration,computer models,extrapolation,model form error,simulation model,structural uncertainty,uncertainty quantification},
number = {11},
title = {{Learning about physical parameters: The importance of model discrepancy}},
volume = {30},
year = {2014}
}
@article{Chib1995,
abstract = {We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.},
annotate = {r2018-01
Useful tutorial on the Metropolis-Hastings algorithm which also very clearly lays out its theoretical underpinnings. Its relationship to the Accept/Reject method is described clearly.},
author = {Chib, Siddhartha and Greenberg, Edward},
doi = {10.2307/2684568},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chib, Greenberg - 1995 - Understanding the Metropolis-Hastings Algorithm.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
month = {nov},
number = {4},
pages = {327},
publisher = {Taylor {\&} Francis, Ltd.American Statistical Association},
title = {{Understanding the Metropolis-Hastings Algorithm}},
url = {http://www.jstor.org/stable/2684568?origin=crossref},
volume = {49},
year = {1995}
}
@book{Gelman2013,
abstract = {Third edition. "Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"-- Part I: Fundamentals of Bayesian inference. Probability and inference -- Single-parameter models -- Introduction to multiparameter models -- Asymptotics and connections to non-Bayesian approaches -- Hierarchical models -- Part II: Fundamentals of Bayesian data analysis. Model checking -- Evaluating, comparing, and expanding models -- Modeling accounting for data collection -- Decision analysis -- Part III: Advanced computation. Introduction to Bayesian computation -- Basics of Markov chain simulation -- Computationally efficient Markov chain simulation -- Modal and distributional approximations -- Part IV: Regression models. Introduction to regression models -- Hierarchical linear models -- Generalized linear models -- Models for robust inference -- Models for missing data -- Part V: Nonlinear and nonparametric models. Parametric nonlinear models -- Basis function models -- Gaussian process models -- Finite mixture models -- Dirichlet process models -- A. Standard probability distributions -- B. Outline of proofs of limit theorems -- Computation in R and Stan.},
address = {London},
annotate = {Comprehensive reference resource for Bayesian data analysis methodologies.},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
edition = {3rd},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis.pdf:pdf;:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis(2).pdf:pdf},
isbn = {9781439840962},
pages = {675},
publisher = {CRC Press},
title = {{Bayesian data analysis}},
year = {2013}
}
@article{Goldstein2009,
abstract = {We describe an approach, termed reified analysis, for linking the behaviour of mathematical models with inferences about the physical systems which the models represent. We describe the logical basis for the approach, based on coherent assessment of the implications of deficiencies in the mathematical model. We show how the statistical analysis may be carried out by specifying stochastic relationships between the model that we have, improved versions of the model that we might construct, and the system itself. We illustrate our approach with an example concerning the potential shutdown of the Thermohaline circulation in the Atlantic Ocean.},
annotate = {To read},
author = {Goldstein, Michael and Rougier, Jonathan},
doi = {10.1016/J.JSPI.2008.07.019},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstein, Rougier - 2009 - Reified Bayesian modelling and inference for physical systems(2).pdf:pdf},
issn = {0378-3758},
journal = {Journal of Statistical Planning and Inference},
month = {mar},
number = {3},
pages = {1221--1239},
publisher = {North-Holland},
title = {{Reified Bayesian modelling and inference for physical systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0378375808003303},
volume = {139},
year = {2009}
}
@article{Gramacy2008,
abstract = {Motivated by a computer experiment for the design of a rocket booster, this article explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details that make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, we show that our approach is effective in other arenas as well.},
annotate = {To read},
author = {Gramacy, Robert B and Lee, Herbert K. H},
doi = {10.1198/016214508000000689},
file = {:C$\backslash$:/Users/carle/Desktop/Bayesian Treed Gaussian Process Models With an Application to Computer Modeling.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer simulator,Heteroscedasticity,Nonparametric regression,Nonstationary spatial model,Recursive partitioning},
month = {sep},
number = {483},
pages = {1119--1130},
publisher = {Taylor {\&} Francis},
title = {{Bayesian Treed Gaussian Process Models With an Application to Computer Modeling}},
url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000689},
volume = {103},
year = {2008}
}
@article{Haario2005,
annotate = {To read},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.1007/BF02789703},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario, Saksman, Tamminen - 2005 - Componentwise adaptation for high dimensional MCMC(2).pdf:pdf},
issn = {0943-4062},
journal = {Computational Statistics},
month = {jun},
number = {2},
pages = {265--273},
publisher = {Springer-Verlag},
title = {{Componentwise adaptation for high dimensional MCMC}},
url = {http://link.springer.com/10.1007/BF02789703},
volume = {20},
year = {2005}
}
@article{Han2009,
abstract = {This article introduces a Bayesian methodology for the prediction for computer experiments having quantitative and qualitative inputs. The proposed model is a hierarchical Bayesian model with conditional Gaussian stochastic process components. For each of the qualitative inputs, our model assumes that the outputs corresponding to different levels of the qualitative input have “similar” functional behavior in the quantitative inputs. The predictive accuracy of this method is compared with the predictive accuracies of alternative proposals in examples. The method is illustrated in a biomechanical engineering application.},
annotate = {To read},
author = {Han, Gang and Santner, Thomas J. and Notz, William I. and Bartel, Donald L.},
doi = {10.1198/tech.2009.07132},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2009 - Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Gaussian stochastic process model,Hierarchical Bayesian model,Product Gaussian correlation,Root mean squared prediction error},
month = {aug},
number = {3},
pages = {278--288},
publisher = {Taylor {\&} Francis},
title = {{Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2009.07132},
volume = {51},
year = {2009}
}
@article{Harari2017,
abstract = {We revisit the problem of determining the sample size for a Gaussian process emulator and provide a data analytic tool for exact sample size calculations that goes beyond the n = 10d rule of thumb and is based on an IMSPE-related criterion. This allows us to tie sample size and prediction accuracy to the anticipated roughness of the simulated data, and to propose an experimental process for computer experiments, with extension to a robust scheme.},
annotate = {r2017-07-10
Who would have thought the n=10d rule would turn out to be an oversimplification? Here's an IMPSE-based alternative, along with a good takedown of the 10d rule.},
author = {Harari, Ofir and Bingham, Derek and Dean, Angela and Higdon, David and Author, Corresponding and Higdon, Dave},
doi = {10.5705/ss.202016.0217},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harari et al. - 2017 - Preprint Computer Experiments Prediction Accuracy, Sample Size and Model Complexity Revisited Complete List of Au.pdf:pdf},
journal = {Statistica Sinica},
keywords = {Preprint},
mendeley-tags = {Preprint},
title = {{Computer Experiments: Prediction Accuracy, Sample Size and Model Complexity Revisited}},
url = {http://www.stat.sinica.edu.tw/statistica/},
year = {2017}
}
@article{Heaton2017,
abstract = {The Gaussian process is an indispensable tool for spatial data analysts. The onset of the "big data" era, however, has lead to the traditional Gaussian process being computationally infeasible for modern spatial data. As such, various alternatives to the full Gaussian process that are more amenable to handling big spatial data have been proposed. These modern methods often exploit low rank structures and/or multi-core and multi-threaded computing environments to facilitate computation. This study provides, first, an introductory overview of several methods for analyzing large spatial data. Second, this study describes the results of a predictive competition among the described methods as implemented by different groups with strong expertise in the methodology. Specifically, each research group was provided with two training datasets (one simulated and one observed) along with a set of prediction locations. Each group then wrote their own implementation of their method to produce predictions at the given location and each which was subsequently run on a common computing environment. The methods were then compared in terms of various predictive diagnostics. Supplementary materials regarding implementation details of the methods and code are available for this article online.},
annotate = {r2018-01-09
Very nice overview of methods for analyzing large spatial data, both established and new innovations, and good comparison of them with code available. The article is not so helpful for understanding these methods, though the references here are a valuable collection toward that end.},
archivePrefix = {arXiv},
arxivId = {1710.05013},
author = {Heaton, Matthew J. and Datta, Abhirup and Finley, Andrew and Furrer, Reinhard and Guhaniyogi, Rajarshi and Gerber, Florian and Gramacy, Robert B. and Hammerling, Dorit and Katzfuss, Matthias and Lindgren, Finn and Nychka, Douglas W. and Sun, Furong and Zammit-Mangion, Andrew},
eprint = {1710.05013},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heaton et al. - 2017 - Methods for Analyzing Large Spatial Data A Review and Comparison.pdf:pdf},
month = {oct},
title = {{Methods for Analyzing Large Spatial Data: A Review and Comparison}},
url = {http://arxiv.org/abs/1710.05013},
year = {2017}
}
@article{Higdon2008a,
abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis repre...},
annotate = {r2017-03-30
The authors' approach is based on Kennedy and O'Hagan 2001; latter use GP models. The current authors extend it to allow for highly multivariate output, keeping computational costs low enough for MCMC to be workable. High dimensionality for them still means less than 100. More than that is "beyond the scope of the approach given here". Although K{\&}O'H's approach was not "fully Bayesian", the current authors give it a "fully Bayesian" description, which better matches their own extension.},
author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
doi = {10.1198/016214507000000888},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2008 - Computer Model Calibration Using High-Dimensional Output.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer experiments,Functional data analysis,Gaussian process,Prediction,Predictive science,Uncertainty quantification},
month = {jun},
number = {482},
pages = {570--583},
publisher = {Taylor {\&} Francis},
title = {{Computer Model Calibration Using High-Dimensional Output}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214507000000888},
volume = {103},
year = {2008}
}
@article{Higdon2004,
abstract = {We develop a statistical approach for characterizing uncertainty in predictions that are made with the aid of a computer simulation model. Typically, the computer simulation code models a physical system and requires a set of inputs---some known and specified, others unknown. A limited amount of field data from the true physical system is available to inform us about the unknown inputs and also to inform us about the uncertainty that is associated with a simulation-based prediction. The approach given here allows for the following:uncertainty regarding model inputs (i.e., calibration); accounting for uncertainty due to limitations on the number of simulations that can be carried out; discrepancy between the simulation code and the actual physical system; uncertainty in the observation process that yields the actual field data on the true physical system. The resulting analysis yields predictions and their associated uncertainties while accounting for multiple sources of uncertainty. We use a Bayesian form...},
annotate = {r2018-01-15
Found this extremely helpful. There's a lot of overlap with the 2007 flyer plate paper, but here it is, in my opinion, explained more clearly and directly. After reading this, the flyer plate paper makes more sense to me. Anyway, it is mostly helpful in just laying out clearly the basics of the kind of analysis they are doing in the 2007 paper, and the kind we are doing in NSF-DEMS.},
author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
doi = {10.1137/S1064827503426693},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2004 - Combining Field Data and Computer Simulations for Calibration and Prediction.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {60G15,62F15,62M30,62P30,62P35,Gaussian process,calibration,computer experiments,model validation,predictability,simulator science,uncertainty quantification},
month = {jan},
number = {2},
pages = {448--466},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Combining Field Data and Computer Simulations for Calibration and Prediction}},
url = {http://epubs.siam.org/doi/10.1137/S1064827503426693},
volume = {26},
year = {2004}
}
@article{Joseph2015,
abstract = {Engineering model development involves several simplifying assumptions for the purpose of mathematical tractability, which are often not realistic in practice. This leads to discrepancies in the model predictions. A commonly used statistical approach to overcome this problem is to build a statistical model for the discrepancies between the engineering model and observed data. In contrast, an engineering approach would be to find the causes of discrepancy and fix the engineering model using first principles. However, the engineering approach is time consuming, whereas the statistical approach is fast. The drawback of the statistical approach is that it treats the engineering model as a black box and therefore, the statistically adjusted models lack physical interpretability. This article proposes a new framework for model calibration and statistical adjustment. It tries to open up the black box using simple main effects analysis and graphical plots and introduces statistical models inside the engineering m...},
annotate = {To read},
author = {Joseph, V. Roshan and Yan, Huan},
doi = {10.1080/00401706.2014.902773},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph, Yan - 2015 - Engineering-Driven Statistical Adjustment and Calibration.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiments,Functional ANOVA,Gaussian process,Nonlinear regression.},
month = {apr},
number = {2},
pages = {257--267},
publisher = {Taylor {\&} Francis},
title = {{Engineering-Driven Statistical Adjustment and Calibration}},
url = {http://www.tandfonline.com/doi/full/10.1080/00401706.2014.902773},
volume = {57},
year = {2015}
}
@article{Karagiannis2017,
abstract = {Computer models, aiming at simulating a complex real system, are often calibrated in the light of data to improve performance. Standard calibration methods assume that the optimal values of calibration parameters are invariant to the model inputs. In several real world applications where models involve complex parametrizations whose optimal values depend on the model inputs, such an assumption can be too restrictive and may lead to misleading results. We propose a fully Bayesian methodology that produces input dependent optimal values for the calibration parameters, as well as it characterizes the associated uncertainties via posterior distributions. Central to methodology is the idea of formulating the calibration parameter as a step function whose uncertain structure is modeled properly via a binary treed process. Our method is particularly suitable to address problems where the computer model requires the selection of a sub-model from a set of competing ones, but the choice of the ‘best' sub-model may change with the input values. The method produces a selection probability for each sub-model given the input. We propose suitable reversible jump operations to facilitate the challenging computations. We assess the performance of our method against benchmark examples, and use it to analyze a real world application with a large-scale climate model.},
annotate = {2018-01-30
Similar to state-aware, but this seems to focus on discrete input-dependent parameters.},
author = {Karagiannis, Georgios and Konomi, Bledar A. and Lin, Guang},
doi = {10.1016/J.SPASTA.2017.08.002},
file = {:C$\backslash$:/Users/carle/Desktop/1-s2.0-S2211675317301215-main.pdf:pdf},
journal = {Spatial Statistics},
month = {aug},
publisher = {Elsevier},
title = {{On the Bayesian calibration of expensive computer models with input dependent parameters}},
url = {https://www.sciencedirect.com/science/article/pii/S2211675317301215},
year = {2017}
}
@article{Kennedy2001,
annotate = {r2018-01-11; 2016-05-30
Very thorough and detailed development of the foundation of Bayesian calibration of computer models. Useful walkthrough of that foundation.},
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Liu2009,
annotate = {r2018-01-09
"Modularizing", ie keeping separate the distinct components of a Bayesian model, can be a useful way to fudge the analysis to keep poor components of the model from ruining the whole thing. The paper is particularly helpful in the treatment of discrepancy in a model. Useful references are given on pages 123 and 127; the latter gives four papers responsible for the "introduction" of Gaussian process response-surface methodology (GASP) for constructing emulators.},
author = {Liu, F. and Bayarri, M. J. and Berger, J. O.},
doi = {10.1214/09-BA404},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Bayarri, Berger - 2009 - Modularization in Bayesian analysis, with emphasis on analysis of computer models(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Complex computer models,Confounding,Emulators,Identifiability,MCMC mixing,Partial likelihood,Random effects},
month = {mar},
number = {1},
pages = {119--150},
publisher = {International Society for Bayesian Analysis},
title = {{Modularization in Bayesian analysis, with emphasis on analysis of computer models}},
url = {http://projecteuclid.org/euclid.ba/1340370392},
volume = {4},
year = {2009}
}
@article{Liu2008,
abstract = {A major question for the application of computer models is Does the computer model adequately represent reality? Viewing the computer models as a potentially biased representation of reality, Bayarri et al. [M. Bayarri, J. Berger, R. Paulo, J. Sacks, J. Cafeo, J. Cavendish, C. Lin, J. Tu, A framework for validation of computer models, Technometrics 49 (2) (2007) 138–154] develop the simulator assessment and validation engine (SAVE) method as a general framework for answering this question. In this paper, we apply the SAVE method to the challenge problem which involves a thermal computer model designed for certain devices. We develop a statement of confidence that the devices modeled can be applied in intended situations.},
annotate = {This paper is essentially a case study in applying the methodology they promulgate in their 2007 paper "A framework for validation of computer models". They call this model the SAVE method: simulator assessment and validation engine.},
author = {Liu, F. and Bayarri, M.J. and Berger, J.O. and Paulo, R. and Sacks, J.},
doi = {10.1016/J.CMA.2007.05.032},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2008 - A Bayesian analysis of the thermal challenge problem.pdf:pdf},
issn = {0045-7825},
journal = {Computer Methods in Applied Mechanics and Engineering},
month = {may},
number = {29-32},
pages = {2457--2466},
publisher = {North-Holland},
title = {{A Bayesian analysis of the thermal challenge problem}},
url = {https://www.sciencedirect.com/science/article/pii/S0045782507005075},
volume = {197},
year = {2008}
}
@article{Loeppky2006,
abstract = {Computer models to simulate physical phenomena are now widely available in engineering and science. Before relying on a computer model, a natural first step is often to compare its output with physical or field data, to assess whether the computer model reliably represents the real world. Field data, when available, can also be used to calibrate or tune unknown parameters in the computer model. Calibration is particularly problematic in the presence of systematic discrepancies between the computer model and field observations. We introduce a likelihood alternative to previous Bayesian methodology for estimation of calibration or tuning parameters. In an important special case, we show that maximum likelihood estimation will asymptotically find values of the calibration parameter that give an unbiased computer model, if such a model exists. However, the calibration parameters are not necessarily estimable. We also show in some settings that calibration or tuning need to take into account the end-use prediction strategy. Depending on the strategy, the choice of the parameter values may be critical or unimportant.},
annotate = {To read},
author = {Loeppky, Jason L. and Bingham, Derek and Welch, William J.},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loeppky, Bingham, Welch - 2006 - Computer Model Calibration or Tuning in Practice.pdf:pdf},
journal = {Technometrics},
title = {{Computer Model Calibration or Tuning in Practice}},
url = {https://www.researchgate.net/profile/Jason{\_}Loeppky/publication/228936502{\_}Computer{\_}model{\_}calibration{\_}or{\_}tuning{\_}in{\_}practice/links/0c960525da9e07f2d1000000.pdf},
year = {2006}
}
@article{McKay1979,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
annotate = {Latin hypercube sampling is introduced and discussed.},
author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
doi = {10.1080/00401706.1979.10489755},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McKay, Beckman, Conover - 1979 - Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Co.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
month = {may},
number = {2},
pages = {239--245},
publisher = {Taylor {\&} Francis Group},
title = {{Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1979.10489755},
volume = {21},
year = {1979}
}
@article{Mori1973,
abstract = {Having noted an important role of image stress in work hardening of dispersion hardened materials, (1,3) the present paper discusses a method of calculating the average internal stress in the matrix of a material containing inclusions with transformation strain. It is shown that the average stress in the matrix is uniform throughout the material and independent of the position of the domain where the average treatment is carried out. It is also shown that the actual stress in the matrix is the average stress plus the locally fluctuating stress, the average of which vanishes in the matrix. Average elastic energy is also considered by taking into account the effects of the interaction among the inclusions and of the presence of the free boundary. Ayant remarqu{\'{e}} que la force image joue un r{\^{o}}le important dans la consolidation des mat{\'{e}}riaux durcis par dispersion, les auteurs proposent ici une m{\'{e}}thode de calcul de la contrainte interne moyenne dans la matrice d'un mat{\'{e}}riau contenant des inclusions pr{\'{e}}sentant des d{\'{e}}formations dues {\`{a}} une transformation, et montrent que la contrainte moyenne dans la matrice est uniforme {\`{a}} travers le mat{\'{e}}riau et ind{\'{e}}pendante de la position de la zone dans laquelle le traitement moyen est effectu{\'{e}}. Ils montrent aussi que la contrainte r{\'{e}}elle dans la matrice est {\'{e}}gale {\`{a}} la somme de la contrainte moyenne et de la contrainte locale variable dont la moyenne pour toute la matrice tend vers z{\'{e}}ro. L'{\'{e}}nergie {\'{e}}lastique moyenne est {\'{e}}galement calcul{\'{e}}e en tenant compte des effets d'interaction entre les inclusions et de la pr{\'{e}}sence du joint libre. Nachdem die gro$\beta$e Bedeutung der Bildkraft f{\"{u}}r die Verfestigung von dispersions geh{\"{a}}rteten Materialien(1,3) betont wurde, diskutiert die vorliegende Arbeit eine Methode zur Beschreibung der mittleren inneren Spannung in der Matrix eines Materials, das Einschl{\"{u}}sse mit Umwandlungsverspannungen enth{\"{a}}lt, Es wird gezeigt, da$\beta$ die mittlere Spannung in der Matrix im ganzen Material gleichf{\"{o}}rmig und unabh{\"{a}}ngig von der Lage des Bereichs ist, f{\"{u}}r den die Behandlung durchgef{\"{u}}hrt wurde. Au$\beta$erdem wird gezeigt, da$\beta$ die aktuelle Spannung in der Matrix gleich der mittleren Spannung plus einer lokal fluktuierenden Spannung ist, deren Mittelwert {\"{u}}ber die gesamte Matrix verschwindet. Die mittlere elastische Energie wird ebenfalls diskutiert unter Ber{\"{u}}cksichtigung der Wechselwirkungseffekte zwischen den Einschl{\"{u}}ssen und der Gegenwart der freien Oberfl{\"{a}}che.},
annotate = {The Mori-Tanaka model is introduced and discussed.},
author = {Mori, T and Tanaka, K},
doi = {10.1016/0001-6160(73)90064-3},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mori, Tanaka - 1973 - Average stress in matrix and average elastic energy of materials with misfitting inclusions.pdf:pdf},
issn = {0001-6160},
journal = {Acta Metallurgica},
month = {may},
number = {5},
pages = {571--574},
publisher = {Pergamon},
title = {{Average stress in matrix and average elastic energy of materials with misfitting inclusions}},
url = {https://www.sciencedirect.com/science/article/pii/0001616073900643?via{\%}3Dihub},
volume = {21},
year = {1973}
}
@article{Muehlenstaedt2017,
annotate = {r2017-07-10
The authors propose an appropriate distance metric and demonstrate its use in creating LHC designs for experiments with functional input. I wonder how much its utility depends on the degree of knowledge we have about the shape of the functional input.},
author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
doi = {10.1007/s11222-016-9672-z},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muehlenstaedt, Fruth, Roustant - 2017 - Computer experiments with functional inputs and scalar outputs by a norm-based approach.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
month = {jul},
number = {4},
pages = {1083--1097},
publisher = {Springer US},
title = {{Computer experiments with functional inputs and scalar outputs by a norm-based approach}},
url = {http://link.springer.com/10.1007/s11222-016-9672-z},
volume = {27},
year = {2017}
}
@article{OHagan2006,
abstract = {The Bayesian approach to quantifying, analysing and reducing uncertainty in the application of complex process models is attracting increasing attention amongst users of such models. The range and power of the Bayesian methods is growing and there is already a sizeable literature on these methods. However, most of it is in specialist statistical journals. The purpose of this tutorial is to introduce the more general reader to the Bayesian approach.},
annotate = {r2018-01-11
Helpful tutorial, though very entry-level. But I found his discussion of sensitivity analysis helpful, and also his list of ongoing areas of research related to Bayesian analysis of computer codes.},
author = {O'Hagan, A.},
doi = {10.1016/J.RESS.2005.11.025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan - 2006 - Bayesian analysis of computer code outputs A tutorial.pdf:pdf},
issn = {0951-8320},
journal = {Reliability Engineering {\&} System Safety},
month = {oct},
number = {10-11},
pages = {1290--1300},
publisher = {Elsevier},
title = {{Bayesian analysis of computer code outputs: A tutorial}},
url = {https://www.sciencedirect.com/science/article/pii/S0951832005002383},
volume = {91},
year = {2006}
}
@article{Oakley2004,
annotate = {To read},
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
doi = {10.1111/j.1467-9868.2004.05304.x},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oakley, O'Hagan - 2004 - Probabilistic sensitivity analysis of complex models a Bayesian approach(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Bayesian inference,Computer model,Gaussian process,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {751--769},
publisher = {Blackwell Publishing},
title = {{Probabilistic sensitivity analysis of complex models: a Bayesian approach}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2004.05304.x},
volume = {66},
year = {2004}
}
@misc{OHagan1978,
abstract = {The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space X of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of X. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables.},
annotate = {r2017-04-20
O'Hagan walks the reader through the use of Gaussian process models for smoothing, curve fitting, and prediction.},
author = {O'Hagan, A. and Kingman, J. F. C.},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
doi = {10.2307/2984861},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan, Kingman - 1978 - Curve Fitting and Optimal Design for Prediction(2).pdf:pdf},
pages = {1--42},
publisher = {WileyRoyal Statistical Society},
title = {{Curve Fitting and Optimal Design for Prediction}},
url = {http://www.jstor.org/stable/2984861},
volume = {40},
year = {1978}
}

@article{Paulo2012,
abstract = {The problem of calibrating computer models that produce multivariate output is considered, with a particular emphasis on the situation where the model is computationally demanding. The proposed methodology builds on Gaussian process-based response-surface approximations to each of the components of the output of the computer model to produce an emulator of the multivariate output. This emulator is then combined in a statistical model involving field observations, which is then used to produce calibration strategies for the parameters of the computer model. The results of applying this methodology to a simulated example and to a real application are presented.},
annotate = {To read},
author = {Paulo, Rui and Garc{\'{i}}a-Donato, Gonzalo and Palomo, Jes{\'{u}}s},
doi = {10.1016/j.csda.2012.05.023},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulo, Garc{\'{i}}a-Donato, Palomo - 2012 - Calibration of computer models with multivariate output.pdf:pdf},
journal = {Computational Statistics and Data Analysis},
pages = {3959--3974},
title = {{Calibration of computer models with multivariate output}},
url = {www.elsevier.com/locate/csda},
volume = {56},
year = {2012}
}
@article{Qian2008a,
abstract = {Standard practice when analyzing data from different types of experiments is to treat data from each type separately. By borrowing strength across multiple sources, an integrated analysis can produce bet-ter results. Careful adjustments must be made to incorporate the systematic differences among various experiments. Toward this end, some Bayesian hierarchical Gaussian process models are proposed. The heterogeneity among different sources is accounted for by performing flexible location and scale ad-justments. The approach tends to produce prediction closer to that from the high-accuracy experiment. The Bayesian computations are aided by the use of Markov chain Monte Carlo and sample average ap-proximation algorithms. The proposed method is illustrated with two examples, one with detailed and approximate finite elements simulations for mechanical material design and the other with physical and computer experiments for modeling a food processor.},
annotate = {r2017-08-22
Clearly stated model for integrating high and low data. Nice conditional distributions are given. The authors explore many tweaks to make the model suit various situations.},
author = {Qian, Peter Z G and Jeff, C F and Stewart, Wu H Milton},
doi = {10.1198/004017008000000082},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Jeff, Stewart - Unknown - Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.pdf:pdf},
journal = {Technometrics},
keywords = {Computer experiments,Gaussian process,Kriging,Markov chain Monte Carlo,Sto-chastic programming},
number = {2},
pages = {192--204},
title = {{Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments}},
url = {https://www.tandfonline.com/doi/pdf/10.1198/004017008000000082},
volume = {50},
year = {2008}
}
@article{Qian2008,
abstract = {Modeling experiments with qualitative and quantitative factors is an important issue in computer modeling. We propose a framework for building Gaussian process models that incorporate both types of factors. The key to the development of these new models is an approach for constructing correlation functions with qualitative and quantitative factors. An iterative estimation procedure is developed for the proposed models. Modern optimization techniques are used in the estimation to ensure the validity of the constructed correlation functions. The proposed method is illustrated with an example involving a known function and a real example for modeling the thermal distribution of a data center.},
annotate = {r2017-07-13
The authors describe and provide theoretical grounding for a framework that allows one to use qualitative input to determine which model to apply to the quantitative input.},
author = {Qian, Peter Z. G and Wu, Huaiqing and Wu, C. F. Jeff},
doi = {10.1198/004017008000000262},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Wu, Wu - 2008 - Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Cokriging,Design of experiments,Kriging,Multivariate Gaussian process,Semidefinite programming},
month = {aug},
number = {3},
pages = {383--396},
publisher = {Taylor {\&} Francis},
title = {{Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017008000000262},
volume = {50},
year = {2008}
}
@article{Sacks1989,
abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic - rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. WIth this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
annotate = {Foundational with respect to much of the work done in computer model calibration.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
eprint = {1011.1669},
file = {:C$\backslash$:/Users/carle/Desktop/2245858.pdf:pdf},
isbn = {0387954201},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,computer-aided design,experimental design,kriging,response surface,spatial statistics},
number = {4},
pages = {409--423},
pmid = {20948974},
title = {{Design and Analysis of Computer Experiments}},
url = {http://projecteuclid.org/euclid.ss/1177012413},
volume = {4},
year = {1989}
}
@book{Santner2003a,
abstract = {The computer has become an increasingly popular tool for exploring the relationship between a measured response and factors thought to affect the response. In many cases, the basis of a computer model is a mathematical theory that implicitly relates the response to the factors. A computer model becomes possible given suitable numerical methods for accurately solving the mathematical system and appropriate computer hardware and software to implement the numerical methods. For example, in many engineering applications, the relationship is described by a dynamical system and the numerical method is a finite element code. The resulting computer "simulator" can generate the response corresponding to any given set of values of the factors. This allows one to use the code to conduct a "computer experiment" to explore the relationship between the response and the factors. In some cases, computer experimentation is feasible when a properly designed physical experiment (the gold standard for establishing cause and effect) is impossible; the number of input variables may be too large to consider performing a physical experiment, or power studies may show it is economically prohibitive to run an experiment on the scale required to answer a given research question. This book describes methods for designing and analyzing experiments that are conducted using a computer code rather than a physical experiment. It discusses how to select the values of the factors at which to run the code (the design of the computer experiment) in light of the research objectives of the experimenter. It also provides techniques for analyzing the resulting data so as to achieve these research goals. It illustrates these methods with code that is available to the reader at the companion web site for the book. Thomas Santner has been a professor in the Department of Statistics at The Ohio State University since 1990. At Ohio State, he has served as department Chair and Director of the department's Statistical Consulting Service. Previously, he was a professor in the School of Operations Research and Industrial Engineering at Cornell University. He is a Fellow of the American Statistical Association and the Institute of Mathematical Statistics, and is an elected ordinary member of the International Statistical Institute. He visited Ludwig Maximilians Universitt in Munich, Germany on a Fulbright Scholarship in 1996-97. Brian Williams has been an Associate Statistician at the RAND Corporation since 2000. His research interests include experimental design, computer experiments, Bayesian inference, spatial statistics and statistical computing. He holds a Ph. D. in statistics from The Ohio State University. William Notz is a professor in the Department of Statistics at The Ohio State University. At Ohio State, he has served as acting department chair, associate dean of the College of Mathematical and Physical Sciences, and as director of the department's Statistical Consulting Service. He has also served as Editor of the journal Technometrics and is a Fellow of the American Statistical Association. Physical Experiments and Computer Experiments -- Basic Elements of Computer Experiments -- Analyzing Output from Computer Experiments-Predicting Output from Training Data -- Space Filling Designs for Computer Experiments -- Criteria Based Designs for Computer Experiments -- Other Issues.},
annotate = {r2017-07
Ch. 1: Gives some examples of computer models, and discusses the relevant differences between computer and physical experiments. For example, replication is ordinarily not an issue for computer experiments, as it is in physical experiments.
Ch. 2: Includes a basic introduction to Gaussian process modeling.
Ch. 3: Discusses basics of making predictions using Gaussian process models: BLUPs, MSPE, etc.
Ch. 4: Continuation of chapter 3. Focuses on predictive distributions, rather than point estimates.
Ch. 5: Discusses latin hypercube sampling, stratified sampling, uniform designs, and other space-filling designs.
Ch. 6: Discusses entropy-based designs, MSPE-based designs, and other optimizing designs.
Ch. 7: Discusses various methods of sensitivity analysis of computer models.},
author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
isbn = {1475737998},
pages = {286},
publisher = {Springer New York},
title = {{The Design and Analysis of Computer Experiments}},
url = {https://books.google.com/books?id=W2faBwAAQBAJ{\&}dq=The+design+and+analysis+of+computer+experiments{\&}lr={\&}source=gbs{\_}navlinks{\_}s},
year = {2003}
}
@article{Storlie2015,
abstract = {It has become commonplace to use complex computer models to predict outcomes in regions where data do not exist. Typically these models need to be calibrated and validated using some experimental data, which often consists of multiple correlated outcomes. In addition, some of the model parameters may be categorical in nature, such as a pointer variable to alternate models (or submodels) for some of the physics of the system. Here, we present a general approach for calibration in such situations where an emulator of the computationally demanding models and a discrepancy term from the model to reality are represented within a Bayesian smoothing spline (BSS) ANOVA framework. The BSS-ANOVA framework has several advantages over the traditional Gaussian process, including ease of handling categorical inputs and correlated outputs, and improved computational efficiency. Finally, this framework is then applied to the problem that motivated its design; a calibration of a computational fluid dynamics (CFD) model of...},
annotate = {r2017-07-13
This work sees itself as an advance on the work of Qian et al 2008 and Zhou et al 2011 in that it focuses specifically on calibration (rather than sensitivity analysis or uncertainty analysis). The authors support the use of "Bayesian Smoothing Spline ANOVA", which may offer some advantages in computational efficiency. But the authors also, for comparison, extend the approach of Higdon 2008 for use with qualitative input.},
author = {Storlie, Curtis B. and Lane, William A. and Ryan, Emily M. and Gattiker, James R. and Higdon, David M.},
doi = {10.1080/01621459.2014.979993},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2015 - Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Sp.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Categorical inputs,Emulator,Inverse problem,Model calibration,Multiple outputs,Uncertainty quantification},
month = {jan},
number = {509},
pages = {68--82},
publisher = {Taylor {\&} Francis},
title = {{Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Spline ANOVA}},
url = {http://www.tandfonline.com/doi/full/10.1080/01621459.2014.979993},
volume = {110},
year = {2015}
}
@article{Storlie2013,
annotate = {To read},
author = {Storlie, Curtis B. and Reich, Brian J. and Helton, Jon C. and Swiler, Laura P. and Sallaberry, Cedric J.},
doi = {10.1016/j.ress.2012.11.018},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2013 - Analysis of computationally demanding models with continuous and categorical inputs.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {may},
pages = {30--41},
title = {{Analysis of computationally demanding models with continuous and categorical inputs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832012002487},
volume = {113},
year = {2013}
}
@article{Tuo2017,
abstract = {Identification of model parameters in computer simulations is an important topic in computer experiments. We propose a new method, called the projected kernel calibration method, to estimate these model parameters. The proposed method is proven to be asymptotic normal and semi-parametric efficient. As a frequentist method, the proposed method is as efficient as the {\$}L{\_}2{\$} calibration method proposed by Tuo and Wu [Ann. Statist. 43 (2015) 2331-2352]. On the other hand, the proposed method has a natural Bayesian version, which the {\$}L{\_}2{\$} method does not have. This Bayesian version allows users to calculate the credible region of the calibration parameters without using a large sample approximation. We also show that, the inconsistency problem of the calibration method proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat. Methodol. 63 (2001) 425-464] can be rectified by a simple modification of the kernel matrix.},
annotate = {To read},
archivePrefix = {arXiv},
arxivId = {1705.03422},
author = {Tuo, Rui},
eprint = {1705.03422},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuo - 2017 - Adjustments to Computer Models via Projected Kernel Calibration.pdf:pdf},
month = {may},
title = {{Adjustments to Computer Models via Projected Kernel Calibration}},
url = {http://arxiv.org/abs/1705.03422},
year = {2017}
}
@article{Williams2006,
annotate = {r2018-01-12; 2017-11-01
Section 2.3 was especially helpful for building a GP emulator of a computationally expensive emulator. The rest of the paper deals with related topics such as using the emulator for sensitivity analysis. I just wonder if the sort of approach they use here is going to be appropriate for us, given how many observations we have. They use only 20 simulation observations, we use 504 times three simulator outputs.},
author = {Williams, Brian and Higdon, Dave and Gattiker, Jim and Moore, Leslie and McKay, Michael and Keller-McNulty, Sallie},
doi = {10.1214/06-BA125},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams et al. - 2006 - Combining experimental data and computer simulations, with an application to flyer plate experiments(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Gaussian process,calibration,computer experiments,flyer plate experiments,model validation,predictability,predictive science,sensitivity analysis,uncertainty quantification},
month = {dec},
number = {4},
pages = {765--792},
publisher = {International Society for Bayesian Analysis},
title = {{Combining experimental data and computer simulations, with an application to flyer plate experiments}},
url = {http://projecteuclid.org/euclid.ba/1340370942},
volume = {1},
year = {2006}
}
@article{Wong2017,
annotate = {To read},
author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
doi = {10.1111/rssb.12182},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Storlie, Lee - 2014 - A Frequentist Approach to Computer Model Calibration(2).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Bootstrap,Inverse problem,Model misspecification,Semiparametric modelling,Surrogate model,Uncertainty analysis},
month = {mar},
number = {2},
pages = {635--648},
title = {{A frequentist approach to computer model calibration}},
url = {http://doi.wiley.com/10.1111/rssb.12182},
volume = {79},
year = {2017}
}
@article{Zhang2015,
abstract = {ABSTRACTIn this article, we review and reexamine approaches to modeling computer experiments with qualitative and quantitative input variables. For those not familiar with models for computer experiments, we begin by showing, in a simple setting, that a standard model for computer experiments can be viewed as a generalization of regression models. We then review models that include both quantitative and quantitative variables and present some alternative parameterizations. Two are based on indicator functions and allow one to use standard quantitative inputs-only models. Another parameterization provides additional insight into possible underlying factorial structure. Finally, we use two examples to illustrate the benefits of these alternative models},
annotate = {Very helpful overview focusing on the work of Han et al 2009, Qian et al 2008, and Zhou et al 2011.},
author = {Zhang, Yulei and Notz, William I.},
doi = {10.1080/08982112.2015.968039},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Notz - 2015 - Computer Experiments with Qualitative and Quantitative Variables A Review and Reexamination.pdf:pdf},
issn = {0898-2112},
journal = {Quality Engineering},
keywords = {GaSP model,Gaussian correlation function,best linear unbiased predictor,computer experiments,indicator variables,qualitative input},
month = {jan},
number = {1},
pages = {2--13},
publisher = {Taylor {\&} Francis},
title = {{Computer Experiments with Qualitative and Quantitative Variables: A Review and Reexamination}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08982112.2015.968039},
volume = {27},
year = {2015}
}
@article{Zhou2011,
abstract = {We propose a flexible yet computationally efficient approach for building Gaussian process models for computer experiments with both qualitative and quantitative factors. This approach uses the hypersphere parameterization to model the correlations of the qualitative factors, thus avoiding the need of directly solving optimization problems with positive definite constraints. The effectiveness of the proposed method is successfully illustrated by several examples.},
annotate = {r2017-07-14
The computational efficiency of the technique described in Qian et al 2008 is vastly improved here. However, this version still requires m(m-1)/2 parameters when the qualitative factor has m levels. For a single factor of 1000 materials, that is almost 500,000 variables; for two factors of 20 and 50 levels resp., it is 1415 parameters. Restrictions of the covariance matrix may allow reduction in the number of parameters without too far mitigating the utility of the framework.},
author = {Zhou, Qiang and Qian, Peter Z. G. and Zhou, Shiyu},
doi = {10.1198/TECH.2011.10025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Qian, Zhou - 2011 - A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiment,Hypersphere decomposition,Kriging},
month = {aug},
number = {3},
pages = {266--273},
publisher = {Taylor {\&} Francis},
title = {{A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.10025},
volume = {53},
year = {2011}
}
