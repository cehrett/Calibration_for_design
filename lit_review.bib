@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, Stuart and Geman, Donald},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geman, Geman - 1984 - IEEE Transactions on Pattern Analysis and Machine Intelligence.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {721--741},
title = {{IEEE Transactions on Pattern Analysis and Machine Intelligence}},
url = {http://www.dam.brown.edu/people/documents/stochasticrelaxation.pdf},
volume = {6},
year = {1984}
}
@article{Gelman1992,
abstract = {The Gibbs sampler, Metropolis' algorithm, and simi-lar iterative simulation methods are related to rejection sampling and importance sampling, two methods which have been traditionally thought of as non-iterative. We explore connections between importance sampling, iter-ative simulation, and importance-weighted resampling (SIR), and present new algorithms that combine aspects of importance sampling, Metropolis' algorithm, and the Gibbs sampler.},
author = {Gelman, Andrew},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 1992 - Iterative and Non-Iterative Simulation Algorithms.pdf:pdf},
journal = {Computinlg Science and Statistics (Interface Proceedings)},
pages = {433--438},
title = {{Iterative and Non-Iterative Simulation Algorithms}},
url = {http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/347.pdf},
volume = {24},
year = {1992}
}
@article{Bornstein1976,
abstract = {The pace of life},
author = {Bornstein, Marc H. and Bornstein, Helen G.},
doi = {10.1038/259557a0},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/BORNSTEIN, BORNSTEIN - 1976 - The pace of life.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {5544},
pages = {557--559},
publisher = {Nature Publishing Group},
title = {{The pace of life}},
url = {http://www.nature.com/doifinder/10.1038/259557a0},
volume = {259},
year = {1976}
}
@article{Sorokowska2017,
abstract = {Human spatial behavior has been the focus of hundreds of previous research studies. However, the conclusions and generalizability of previous studies on interpersonal distance preferences were limited by some important methodological and sampling issues. The objective of the present study was to compare preferred interpersonal distances across the world and to overcome the problems observed in previous studies. We present an extensive analysis of interpersonal distances over a large data set (N = 8,943 participants from 42 countries). We attempted to relate the preferred social, personal, and intimate distances observed in each country to a set of individual characteristics of the participants, and some attributes of their cultures. Our study indicates that individual characteristics (age and gender) influence interpersonal space preferences and that some variation in results can be explained by temperature in a given region. We also present objective values of preferred interpersonal distances in differe...},
author = {Sorokowska, Agnieszka and Sorokowski, Piotr and Hilpert, Peter and Cantarero, Katarzyna and Frackowiak, Tomasz and Ahmadi, Khodabakhsh and Alghraibeh, Ahmad M. and Aryeetey, Richmond and Bertoni, Anna and Bettache, Karim and Blumen, Sheyla and B{\l}a{\.{z}}ejewska, Marta and Bortolini, Tiago and Butovskaya, Marina and Castro, Felipe Nalon and Cetinkaya, Hakan and Cunha, Diana and David, Daniel and David, Oana A. and Dileym, Fahd A. and {Dom{\'{i}}nguez Espinosa}, Alejandra del Carmen and Donato, Silvia and Dronova, Daria and Dural, Seda and Fialov{\'{a}}, Jitka and Fisher, Maryanne and Gulbetekin, Evrim and {Hamamcıoğlu Akkaya}, Aslıhan and Hromatko, Ivana and Iafrate, Raffaella and Iesyp, Mariana and James, Bawo and Jaranovic, Jelena and Jiang, Feng and Kimamo, Charles Obadiah and Kjelvik, Grete and Ko{\c{c}}, Fırat and Laar, Amos and {de Ara{\'{u}}jo Lopes}, F{\'{i}}via and Macbeth, Guillermo and Marcano, Nicole M. and Martinez, Rocio and Mesko, Norbert and Molodovskaya, Natalya and Moradi, Khadijeh and Motahari, Zahrasadat and M{\"{u}}hlhauser, Alexandra and Natividade, Jean Carlos and Ntayi, Joseph and Oberzaucher, Elisabeth and Ojedokun, Oluyinka and Omar-Fauzee, Mohd Sofian Bin and Onyishi, Ike E. and Paluszak, Anna and Portugal, Alda and Razumiejczyk, Eugenia and Realo, Anu and Relvas, Ana Paula and Rivas, Maria and Rizwan, Muhammad and Salki{\v{c}}evi{\'{c}}, Svjetlana and Sarm{\'{a}}ny-Schuller, Ivan and Schmehl, Susanne and Senyk, Oksana and Sinding, Charlotte and Stamkou, Eftychia and Stoyanova, Stanislava and {\v{S}}ukolov{\'{a}}, Denisa and Sutresna, Nina and Tadinac, Meri and Teras, Andero and {Tinoco Ponciano}, Edna L{\'{u}}cia and Tripathi, Ritu and Tripathi, Nachiketa and Tripathi, Mamta and Uhryn, Olja and Yamamoto, Maria Em{\'{i}}lia and Yoo, Gyesook and Pierce, John D.},
doi = {10.1177/0022022117698039},
issn = {0022-0221},
journal = {Journal of Cross-Cultural Psychology},
keywords = {cultural psychology,culture,interpersonal distance,spatial behavior},
month = {may},
number = {4},
pages = {577--592},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Preferred Interpersonal Distances: A Global Comparison}},
url = {http://journals.sagepub.com/doi/10.1177/0022022117698039},
volume = {48},
year = {2017}
}
@article{Thompson1995,
abstract = {The computer model 'SIMULEX' is designed to simulate the escape movement of thousands of individual people through large, geometri-cally complex building spaces. The model is intended for use both as a research and design tool to analyse the evacuation of large populations through a wide range of building environments. The computer program assigns a variety of attributes to each individual in the building population. These attributes include a co-ordinate position, angle of orientation, and a walking speed for each person. Specific algorithms that facilitate the simulation of escape movement include distance mapping, wayfinding, overtaking, route deviation, and adjustments to individual speeds due to the proximity of crowd members. These algorithms contribute to a computer package that displays the building plan and the position and progress of individual building occupants as they walk towards, and through the exits.},
author = {Thompson, Peter A and Marchant, Eric W},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Marchant - 1995 - A Computer Model for the Evacuation of Large Building Populations.pdf:pdf},
isbn = {0379-7112(95)00019-4},
journal = {Fire Safety Journal},
pages = {131--148},
title = {{A Computer Model for the Evacuation of Large Building Populations}},
url = {https://ac.els-cdn.com/037971129500019P/1-s2.0-037971129500019P-main.pdf?{\_}tid=ff6d6436-fcde-4a4c-a599-3562d0129240{\&}acdnat=1522335585{\_}cf9429d0f531f551cc6db4d0697d1163},
volume = {24},
year = {1995}
}
@article{Hastings1970,
abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
author = {Hastings, W.K.},
journal = {Biometrika},
month = {apr},
number = {1},
pages = {97--109},
title = {{Monte Carlo sampling methods using Markov chains and their applications}},
url = {https://academic.oup.com/biomet/article-abstract/57/1/97/2721936},
volume = {57},
year = {1970}
}
@incollection{Neal,
abstract = {Gaussian processes are a natural way of specifying prior distributions over functions of one or more input variables. When such a function defines the mean response in a regression model with Gaussian errors, inference can be done using matrix computations, which are feasible for datasets of up to about a thousand cases. The covariance function of the Gaussian process can be given a hierarchical prior, which allows the model to discover high-level properties of the data, such as which inputs are relevant to predicting the response. Inference for these covariance hyperparameters can be done using Markov chain sampling. Classification models can be defined using Gaussian processes for underlying latent values, which can also be sampled within the Markov chain. Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique. This may be partly due to a confusion between the properties of the function being modeled and the properties of the best predictor for this unknown function. In this paper, I hope to persuade you that Gaussian processes are a fruitful way of defining prior distributions for flexible regression and classification models in which the regression or class probability functions are not limited to simple parametric forms. The basic idea goes back many years in a regression context, but is nevertheless not widely appreciated. The use of general Gaussian process models for classification is more recent, and to my knowledge the work presented here is the first that implements an exact Bayesian approach. One attraction of Gaussian processes is the variety of covariance functions one can choose from, which lead to functions with different degrees of smoothness, or different sorts of additive structure. I will describe some of these possibilities, while also noting the limitations of Gaussian processes. I then discuss computations for Gaussian process models, starting with the basic matrix operations involved. For classification models, one must integrate over the ''latent values'' underlying each case. I show how this can be done using Markov chain Monte Carlo methods. In a full-fledged Bayesian treatment, one must also integrate over the posterior distribution for the hyperparameters of the covariance function, which can also be done using Markov chain sampling. I show how this all works for a synthetic three-way classification problem. The methods I describe are implemented in software that is available from my web page, at http://www.cs.utoronto.ca/radford/. Gaussian processes and related methods have been used in various contexts for many years. Despite this past usage, and despite the fundamental simplicity of the idea, Gaussian process models appear to have been little appreciated by most Bayesians. I speculate that this could be partly due to a confusion between the properties one expects of the true function being modeled and those of the best predictor for this unknown function. Clarity in this respect is particularly important when thinking of flexible models such as those based on Gaussian processes.},
address = {New York},
annotate = {r2018-04-05
The paper is a pretty basic introduction to the use of Gaussian processes for regression and classification. There's a nice example near the beginning building the concept of GP regression from a case of linear regression. Correspondingly, there is some discussion of covariate matrices producing linear estimates.},
author = {Neal, Radford M},
booktitle = {Bayesian Statistics 6},
editor = {Bernardo, J M and Berger, J O and Dawid, A P and Smith, A F M},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - Unknown - Regression and Classification Using Gaussian Process Priors.pdf:pdf},
pages = {475--501},
publisher = {Oxford University Press},
title = {{Regression and Classification Using Gaussian Process Priors}},
url = {http://www.cs.toronto.edu/{~}radford/ftp/val6gp.pdf},
volume = {6},
year = {1998}
}
@article{Savitsky2011,
abstract = {This paper presents a unified treatment of Gaussian process mod-els that extends to data from the exponential dispersion family and to survival data. Our specific interest is in the analysis of data sets with predictors that have an a priori unknown form of possibly nonlinear associations to the re-sponse. The modeling approach we describe incorporates Gaussian processes in a generalized linear model framework to obtain a class of nonparametric regression models where the covariance matrix depends on the predictors. We consider, in particular, continuous, categorical and count responses. We also look into models that account for survival outcomes. We explore alterna-tive covariance formulations for the Gaussian process prior and demonstrate the flexibility of the construction. Next, we focus on the important problem of selecting variables from the set of possible predictors and describe a gen-eral framework that employs mixture priors. We compare alternative MCMC strategies for posterior inference and achieve a computationally efficient and practical approach. We demonstrate performances on simulated and bench-mark data sets.},
annotate = {r2018-03-22
This paper provides a thorough and helpful discussion of Gaussian process regression before coming to focus specifically on the problem of variable selection in a Gaussian process regression. This is valuable because it allows one to undertake variable selection when one has little notion of what might be the parametric form of the relationship of the potential covariates to the response. The selection is accomplished via spike-and-slab mixture priors. The authors discuss two distinct MCMC schemas for carrying out the variable selection, comparing them with each other and with other methods.},
author = {Savitsky, Terrance and Vannucci, Marina and Sha, Naijun},
doi = {10.1214/11-STS354},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Savitsky, Vannucci, Sha - 2011 - Variable Selection for Nonparametric Gaussian Process Priors Models and Computational Strategies.pdf:pdf},
journal = {Statistical Science},
keywords = {Bayesian variable selection,Gaussian processes,MCMC,generalized linear models,latent variables,nonparametric regression,survival data},
number = {1},
pages = {130--149},
title = {{Variable Selection for Nonparametric Gaussian Process Priors: Models and Computational Strategies}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=16240215469252dc{\&}attid=0.2{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ{\_}oO5xZ1vTd1RjySHOmyEdI9Fs9roTk900L4w7A4AOqQqJ7gx-ee8n0NjU-NJqgOTx6kYDzgjORBJI9uYjIFhrR6s6d{\_}3b{\_}f29rI},
volume = {26},
year = {2011}
}
@article{Shang2013,
abstract = {A generalized Gaussian process model (GGPM) is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the ex-ponential family distribution (EFD). In this paper, we consider efficient algorithms for approximate inference on GGPMs using the general form of the EFD. A particular GP model and its associ-ated inference algorithms can then be formed by changing the parameters of the EFD, thus greatly simplifying its creation for task-specific output domains. We demonstrate the efficacy of this frame-work by creating several new GP models for regressing to non-negative reals and to real intervals. We also consider a closed-form Taylor approximation for efficient inference on GGPMs, and elab-orate on its connections with other model-specific heuristic closed-form approximations. Finally, we present a comprehensive set of experiments to compare approximate inference algorithms on a wide variety of GGPMs.},
annotate = {r2018-03-28
It's not clear to me what sort of value the work in this paper has. It's nice to provide an umbrella generalization of existing methods, but what does that get us, exactly? The paper is both very big and very difficult to read and I very much lament the authors' decision not to include illustrative examples of simple cases of GP regression/classification seen through the lens of their umbrella methodology.},
author = {Shang, Lifeng and Chan, Antoni B},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang, Chan - 2013 - On Approximate Inference for Generalized Gaussian Process Models.pdf:pdf},
journal = {Technical Report -City University of Hong Kong},
keywords = {Bayesian generalized linear models,Gaussian processes,approximate inference,exponential family,non-parametric regression},
title = {{On Approximate Inference for Generalized Gaussian Process Models}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=16240215469252dc{\&}attid=0.1{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ9Ez1jf30QAg{\_}EeS0yMiGWRGMl72wEwIYmWA9PXn99Yh5xJHY0PJxOB-zXBBRTdN3IzVsgIjtCzYl8PQuBsikm1PnEaV3vmk7KdY},
year = {2013}
}
@article{Reich2011,
abstract = {Numerous studies have linked ambient air pollution and adverse health outcomes. Many studies of this nature relate outdoor pollution levels measured at a few monitoring sta-tions with health outcomes. Recently, computational methods have been developed to model the distribution of personal exposures, rather than ambient concentration, and then relate the exposure distribution to the health outcome. Although these methods show great promise, they are limited by the computational demands of the exposure model. We propose a method to alle-viate these computational burdens with the eventual goal of implementing a national study of the health effects of air pollution exposure. Our approach is to develop a statistical emulator for the exposure model, i.e. we use Bayesian density estimation to predict the conditional exposure distribution as a function of several variables, such as temperature, human activity and physical characteristics of the pollutant. This poses a challenging statistical problem because there are many predictors of the exposure distribution and density estimation is notoriously difficult in high dimensions. To overcome this challenge, we use stochastic search variable selection to identify a subset of the variables that have more than just additive effects on the mean of the exposure distribution. We apply our method to emulate an ozone exposure model in Philadelphia.},
author = {Reich, Brian J and Kalendra, Eric and Storlie, Curtis B and Bondell, Howard D and Fuentes, Montserrat},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reich et al. - 2011 - Variable selection for high dimensional Bayesian density estimation application to human exposure simulation.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
keywords = {Air pollution,Bayesian non-parametrics,High dimensional data,Kernel stick breaking prior,Stochastic computer models},
mendeley-tags = {Air pollution,Bayesian non-parametrics,High dimensional data,Kernel stick breaking prior,Stochastic computer models},
number = {1},
pages = {47--66},
title = {{Variable selection for high dimensional Bayesian density estimation: application to human exposure simulation}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=1623f0be4274fbc4{\&}attid=0.1{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ{\_}UbAt2BrwgVUj1lcHJUEkOShp8PtN89-aNlaNvQsCJaW2NITzwkEcsCVdionWFCTle6sjGOgMazUya82qZdOB9qxa5Mkq3nnX4z},
volume = {61},
year = {2011}
}
@article{Kennedy2006,
abstract = {In this paper we present a number of recent applications in which an emulator of a computer code is created using a Gaussian process model. Tools are then applied to the emulator to perform sensitivity analysis and uncertainty analysis. Sensitivity analysis is used both as an aid to model improvement and as a guide to how much the output uncertainty might be reduced by learning about specific inputs. Uncertainty analysis allows us to reflect output uncertainty due to unknown input parameters, when the finished code is used for prediction. The computer codes themselves are currently being developed within the UK Centre for Terrestrial Carbon Dynamics.},
author = {Kennedy, Marc C. and Anderson, Clive W. and Conti, Stefano and O'Hagan, Anthony},
doi = {10.1016/J.RESS.2005.11.028},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy et al. - 2006 - Case studies in Gaussian process modelling of computer codes.pdf:pdf},
issn = {0951-8320},
journal = {Reliability Engineering {\&} System Safety},
month = {oct},
number = {10-11},
pages = {1301--1309},
publisher = {Elsevier},
title = {{Case studies in Gaussian process modelling of computer codes}},
url = {https://www.sciencedirect.com/science/article/pii/S0951832005002395},
volume = {91},
year = {2006}
}
@article{Currin1991,
annotate = {r2018-03-09
The article discusses the use of Gaussian process emulators of computer code, with emphasis on design and on selection of a correlation function. Some interesting points about correlation functions are made, such as how the appropriate choice of correlation function yield prediction functions that are either linear or cubic splines in each dimension. The article includes some nice tables comparing the results using various correlation functions, compared also against some polynomial models fitted by least squares.},
author = {Currin, Carla and Mitchell, Toby and Morris, Max and Ylvisaker, Don},
doi = {10.1080/01621459.1991.10475138},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Currin et al. - 1991 - Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experime.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {dec},
number = {416},
pages = {953--963},
title = {{Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475138},
volume = {86},
year = {1991}
}
@article{Thompson2008,
author = {Thompson, Ambler and Taylor, Barry N.},
journal = {Special Publication (NIST SP) - 811},
title = {{Guide for the Use of the International System of Units (SI)}},
url = {https://www.nist.gov/publications/guide-use-international-system-units-si},
year = {2008}
}
@article{Qian2008a,
abstract = {Standard practice when analyzing data from different types of experiments is to treat data from each type separately. By borrowing strength across multiple sources, an integrated analysis can produce bet-ter results. Careful adjustments must be made to incorporate the systematic differences among various experiments. Toward this end, some Bayesian hierarchical Gaussian process models are proposed. The heterogeneity among different sources is accounted for by performing flexible location and scale ad-justments. The approach tends to produce prediction closer to that from the high-accuracy experiment. The Bayesian computations are aided by the use of Markov chain Monte Carlo and sample average ap-proximation algorithms. The proposed method is illustrated with two examples, one with detailed and approximate finite elements simulations for mechanical material design and the other with physical and computer experiments for modeling a food processor.},
annotate = {r2017-08-22
Clearly stated model for integrating high and low data. Nice conditional distributions are given. The authors explore many tweaks to make the model suit various situations.},
author = {Qian, Peter Z G and Jeff, C F and Stewart, Wu H Milton},
doi = {10.1198/004017008000000082},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Jeff, Stewart - Unknown - Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.pdf:pdf},
journal = {Technometrics},
keywords = {Computer experiments,Gaussian process,Kriging,Markov chain Monte Carlo,Sto-chastic programming},
number = {2},
pages = {192--204},
title = {{Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments}},
url = {https://www.tandfonline.com/doi/pdf/10.1198/004017008000000082},
volume = {50},
year = {2008}
}
@article{Bastos2009,
abstract = {Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different...},
annotate = {r2018-03-20
Gaussian process emulators, their advantages and disadvantages, are discussed along with diagnostics to assess successful emulation. The paper discusses both numerical and graphical diagnostics. There is helpful discussion about how to identify what specific problems might be causing poor diagnostic results. Some of the techniques discussed in the paper include the use of standardized prediction errors (from a validation set), the Mahalanobis distance between the emulator and simulator outputs, and variance decompositions. Early in the paper there is a very helpful bit of background/foundation of Gaussian process emulation, including guidance on how to proceed when the observation variance is unknown and must be estimated (and the resulting Student process distribution).},
author = {Bastos, Leonardo S. and O'Hagan, Anthony},
doi = {10.1198/TECH.2009.08019},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastos, O'Hagan - 2009 - Diagnostics for Gaussian Process Emulators.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian inference,Computer experiments,Diagnostics,Emulation,Gaussian process},
month = {nov},
number = {4},
pages = {425--438},
publisher = {Taylor {\&} Francis},
title = {{Diagnostics for Gaussian Process Emulators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2009.08019},
volume = {51},
year = {2009}
}
@article{Liu2008,
abstract = {A major question for the application of computer models is Does the computer model adequately represent reality? Viewing the computer models as a potentially biased representation of reality, Bayarri et al. [M. Bayarri, J. Berger, R. Paulo, J. Sacks, J. Cafeo, J. Cavendish, C. Lin, J. Tu, A framework for validation of computer models, Technometrics 49 (2) (2007) 138–154] develop the simulator assessment and validation engine (SAVE) method as a general framework for answering this question. In this paper, we apply the SAVE method to the challenge problem which involves a thermal computer model designed for certain devices. We develop a statement of confidence that the devices modeled can be applied in intended situations.},
annotate = {This paper is essentially a case study in applying the methodology they promulgate in their 2007 paper "A framework for validation of computer models". They call this model the SAVE method: simulator assessment and validation engine.},
author = {Liu, F. and Bayarri, M.J. and Berger, J.O. and Paulo, R. and Sacks, J.},
doi = {10.1016/J.CMA.2007.05.032},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2008 - A Bayesian analysis of the thermal challenge problem.pdf:pdf},
issn = {0045-7825},
journal = {Computer Methods in Applied Mechanics and Engineering},
month = {may},
number = {29-32},
pages = {2457--2466},
publisher = {North-Holland},
title = {{A Bayesian analysis of the thermal challenge problem}},
url = {https://www.sciencedirect.com/science/article/pii/S0045782507005075},
volume = {197},
year = {2008}
}
@article{McKay1979,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
annotate = {Latin hypercube sampling is introduced and discussed.},
author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
doi = {10.1080/00401706.1979.10489755},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McKay, Beckman, Conover - 1979 - Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Co.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
month = {may},
number = {2},
pages = {239--245},
publisher = {Taylor {\&} Francis Group},
title = {{Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1979.10489755},
volume = {21},
year = {1979}
}
@article{Mori1973,
abstract = {Having noted an important role of image stress in work hardening of dispersion hardened materials, (1,3) the present paper discusses a method of calculating the average internal stress in the matrix of a material containing inclusions with transformation strain. It is shown that the average stress in the matrix is uniform throughout the material and independent of the position of the domain where the average treatment is carried out. It is also shown that the actual stress in the matrix is the average stress plus the locally fluctuating stress, the average of which vanishes in the matrix. Average elastic energy is also considered by taking into account the effects of the interaction among the inclusions and of the presence of the free boundary.},
annotate = {The Mori-Tanaka model is introduced and discussed.},
author = {Mori, T and Tanaka, K},
doi = {10.1016/0001-6160(73)90064-3},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mori, Tanaka - 1973 - Average stress in matrix and average elastic energy of materials with misfitting inclusions.pdf:pdf},
issn = {0001-6160},
journal = {Acta Metallurgica},
month = {may},
number = {5},
pages = {571--574},
publisher = {Pergamon},
title = {{Average stress in matrix and average elastic energy of materials with misfitting inclusions}},
url = {https://www.sciencedirect.com/science/article/pii/0001616073900643?via{\%}3Dihub},
volume = {21},
year = {1973}
}
@article{Sacks1989,
abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic - rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. WIth this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
annotate = {r2018-03-13
Foundational with respect to much of the work done in computer model calibration. The case is made that despite their deterministic nature, computer models are a proper subject of statistical study, for which we may perform uncertainty analysis. The authors walk through the process of building a Gaussian process emulator of computer code for this purpose. Various covariance functions are discussed. The authors fit covariance function parameters through MLE. Roughly half the paper is devoted to questions of design.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
eprint = {1011.1669},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sacks et al. - 1989 - Design and Analysis of Computer Experiments.pdf:pdf},
isbn = {0387954201},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,computer-aided design,experimental design,kriging,response surface,spatial statistics},
number = {4},
pages = {409--423},
pmid = {20948974},
title = {{Design and Analysis of Computer Experiments}},
url = {http://projecteuclid.org/euclid.ss/1177012413},
volume = {4},
year = {1989}
}
@article{Chib1995,
abstract = {We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.},
annotate = {r2018-01
Useful tutorial on the Metropolis-Hastings algorithm which also very clearly lays out its theoretical underpinnings. Its relationship to the Accept/Reject method is described clearly.},
author = {Chib, Siddhartha and Greenberg, Edward},
doi = {10.2307/2684568},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chib, Greenberg - 1995 - Understanding the Metropolis-Hastings Algorithm.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
month = {nov},
number = {4},
pages = {327},
publisher = {Taylor {\&} Francis, Ltd.American Statistical Association},
title = {{Understanding the Metropolis-Hastings Algorithm}},
url = {http://www.jstor.org/stable/2684568?origin=crossref},
volume = {49},
year = {1995}
}
@article{Goldstein2009,
abstract = {We describe an approach, termed reified analysis, for linking the behaviour of mathematical models with inferences about the physical systems which the models represent. We describe the logical basis for the approach, based on coherent assessment of the implications of deficiencies in the mathematical model. We show how the statistical analysis may be carried out by specifying stochastic relationships between the model that we have, improved versions of the model that we might construct, and the system itself. We illustrate our approach with an example concerning the potential shutdown of the Thermohaline circulation in the Atlantic Ocean.},
annotate = {To read},
author = {Goldstein, Michael and Rougier, Jonathan},
doi = {10.1016/J.JSPI.2008.07.019},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstein, Rougier - 2009 - Reified Bayesian modelling and inference for physical systems(2).pdf:pdf},
issn = {0378-3758},
journal = {Journal of Statistical Planning and Inference},
month = {mar},
number = {3},
pages = {1221--1239},
publisher = {North-Holland},
title = {{Reified Bayesian modelling and inference for physical systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0378375808003303},
volume = {139},
year = {2009}
}
@article{Gramacy2008,
abstract = {Motivated by a computer experiment for the design of a rocket booster, this article explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details that make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, we show that our approach is effective in other arenas as well.},
annotate = {To read},
author = {Gramacy, Robert B and Lee, Herbert K. H},
doi = {10.1198/016214508000000689},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer simulator,Heteroscedasticity,Nonparametric regression,Nonstationary spatial model,Recursive partitioning},
month = {sep},
number = {483},
pages = {1119--1130},
publisher = {Taylor {\&} Francis},
title = {{Bayesian Treed Gaussian Process Models With an Application to Computer Modeling}},
url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000689},
volume = {103},
year = {2008}
}
@article{Brynjarsdottir2014,
abstract = {Science-based simulation models are widely used to predict the behavior of complex physical systems. It is also common to use observations of the physical system to solve the inverse problem, i.e. to learn about the values of parameters within the model, a process often called calibration. The main goal of calibration is usually to improve the predictive performance of the simulator but the values of the parameters in the model may also be of intrinsic scientific interest in their own right. In order to make appropriate use of observations of the physical system it is impor-tant to recognise model discrepancy, the difference between reality and the simulator output. We illustrate through a simple example that an analysis that does not account for model discrepancy may lead to biased and over-confident parameter estimates and predictions. The challenge with incorporating model discrepancy in statistical inverse problems is the confounding with calibration parameters, which will only be resolved with mean-ingful priors. For our simple example, we model the model-discrepancy via a Gaus-sian Process and demonstrate that by accounting for model discrepancy our prediction within the range of data is correct. However, only with realistic priors on the model discrepancy do we uncover the true parameter values. Through theoretical arguments we show that these findings are typical of the general problem of learning about physical parameters and the underlying physical system using science-based mechanistic models.},
annotate = {r2018-01-31; 2017-09-08
Looks like in order to take the lesson of this paper to heart, one would have to be really thoughtful about the physical ground of the discrepancy function, in crafting as narrow constraints as possible. No black boxes here.},
author = {Brynjarsd{\'{o}}ttir, Jenni and O'Hagan, Anthony},
doi = {10.1088/0266-5611/30/11/114007},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brynjarsd{\'{o}}ttir, Ohagan - 2014 - Learning about physical parameters The importance of model discrepancy(2).pdf:pdf},
issn = {13616420},
journal = {Inverse Problems},
keywords = {calibration,computer models,extrapolation,model form error,simulation model,structural uncertainty,uncertainty quantification},
number = {11},
title = {{Learning about physical parameters: The importance of model discrepancy}},
volume = {30},
year = {2014}
}
@article{Karagiannis2017,
abstract = {Computer models, aiming at simulating a complex real system, are often calibrated in the light of data to improve performance. Standard calibration methods assume that the optimal values of calibration parameters are invariant to the model inputs. In several real world applications where models involve complex parametrizations whose optimal values depend on the model inputs, such an assumption can be too restrictive and may lead to misleading results. We propose a fully Bayesian methodology that produces input dependent optimal values for the calibration parameters, as well as it characterizes the associated uncertainties via posterior distributions. Central to methodology is the idea of formulating the calibration parameter as a step function whose uncertain structure is modeled properly via a binary treed process. Our method is particularly suitable to address problems where the computer model requires the selection of a sub-model from a set of competing ones, but the choice of the ‘best' sub-model may change with the input values. The method produces a selection probability for each sub-model given the input. We propose suitable reversible jump operations to facilitate the challenging computations. We assess the performance of our method against benchmark examples, and use it to analyze a real world application with a large-scale climate model.},
annotate = {2018-01-30
Similar to state-aware, but this seems to focus on discrete input-dependent parameters.},
author = {Karagiannis, Georgios and Konomi, Bledar A. and Lin, Guang},
doi = {10.1016/J.SPASTA.2017.08.002},
file = {:C$\backslash$:/Users/carle/Desktop/1-s2.0-S2211675317301215-main.pdf:pdf},
journal = {Spatial Statistics},
month = {aug},
publisher = {Elsevier},
title = {{On the Bayesian calibration of expensive computer models with input dependent parameters}},
url = {https://www.sciencedirect.com/science/article/pii/S2211675317301215},
year = {2017}
}
@book{Gelman2013,
abstract = {Third edition. "Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"-- Part I: Fundamentals of Bayesian inference. Probability and inference -- Single-parameter models -- Introduction to multiparameter models -- Asymptotics and connections to non-Bayesian approaches -- Hierarchical models -- Part II: Fundamentals of Bayesian data analysis. Model checking -- Evaluating, comparing, and expanding models -- Modeling accounting for data collection -- Decision analysis -- Part III: Advanced computation. Introduction to Bayesian computation -- Basics of Markov chain simulation -- Computationally efficient Markov chain simulation -- Modal and distributional approximations -- Part IV: Regression models. Introduction to regression models -- Hierarchical linear models -- Generalized linear models -- Models for robust inference -- Models for missing data -- Part V: Nonlinear and nonparametric models. Parametric nonlinear models -- Basis function models -- Gaussian process models -- Finite mixture models -- Dirichlet process models -- A. Standard probability distributions -- B. Outline of proofs of limit theorems -- Computation in R and Stan.},
address = {London},
annotate = {Comprehensive reference resource for Bayesian data analysis methodologies.},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
edition = {3rd},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis.pdf:pdf;:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis(2).pdf:pdf},
isbn = {9781439840962},
pages = {675},
publisher = {CRC Press},
title = {{Bayesian data analysis}},
year = {2013}
}
@book{Santner2003a,
abstract = {The computer has become an increasingly popular tool for exploring the relationship between a measured response and factors thought to affect the response. In many cases, the basis of a computer model is a mathematical theory that implicitly relates the response to the factors. A computer model becomes possible given suitable numerical methods for accurately solving the mathematical system and appropriate computer hardware and software to implement the numerical methods. For example, in many engineering applications, the relationship is described by a dynamical system and the numerical method is a finite element code. The resulting computer "simulator" can generate the response corresponding to any given set of values of the factors. This allows one to use the code to conduct a "computer experiment" to explore the relationship between the response and the factors. In some cases, computer experimentation is feasible when a properly designed physical experiment (the gold standard for establishing cause and effect) is impossible; the number of input variables may be too large to consider performing a physical experiment, or power studies may show it is economically prohibitive to run an experiment on the scale required to answer a given research question. This book describes methods for designing and analyzing experiments that are conducted using a computer code rather than a physical experiment. It discusses how to select the values of the factors at which to run the code (the design of the computer experiment) in light of the research objectives of the experimenter. It also provides techniques for analyzing the resulting data so as to achieve these research goals. It illustrates these methods with code that is available to the reader at the companion web site for the book. Thomas Santner has been a professor in the Department of Statistics at The Ohio State University since 1990. At Ohio State, he has served as department Chair and Director of the department's Statistical Consulting Service. Previously, he was a professor in the School of Operations Research and Industrial Engineering at Cornell University. He is a Fellow of the American Statistical Association and the Institute of Mathematical Statistics, and is an elected ordinary member of the International Statistical Institute. He visited Ludwig Maximilians Universitt in Munich, Germany on a Fulbright Scholarship in 1996-97. Brian Williams has been an Associate Statistician at the RAND Corporation since 2000. His research interests include experimental design, computer experiments, Bayesian inference, spatial statistics and statistical computing. He holds a Ph. D. in statistics from The Ohio State University. William Notz is a professor in the Department of Statistics at The Ohio State University. At Ohio State, he has served as acting department chair, associate dean of the College of Mathematical and Physical Sciences, and as director of the department's Statistical Consulting Service. He has also served as Editor of the journal Technometrics and is a Fellow of the American Statistical Association. Physical Experiments and Computer Experiments -- Basic Elements of Computer Experiments -- Analyzing Output from Computer Experiments-Predicting Output from Training Data -- Space Filling Designs for Computer Experiments -- Criteria Based Designs for Computer Experiments -- Other Issues.},
address = {New York},
annotate = {r2017-07
Ch. 1: Gives some examples of computer models, and discusses the relevant differences between computer and physical experiments. For example, replication is ordinarily not an issue for computer experiments, as it is in physical experiments.
Ch. 2: Includes a basic introduction to Gaussian process modeling.
Ch. 3: Discusses basics of making predictions using Gaussian process models: BLUPs, MSPE, etc.
Ch. 4: Continuation of chapter 3. Focuses on predictive distributions, rather than point estimates.
Ch. 5: Discusses latin hypercube sampling, stratified sampling, uniform designs, and other space-filling designs.
Ch. 6: Discusses entropy-based designs, MSPE-based designs, and other optimizing designs.
Ch. 7: Discusses various methods of sensitivity analysis of computer models.},
author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
file = {:C$\backslash$:/Users/carle/Desktop/master-driver.pdf:pdf},
isbn = {1475737998},
pages = {286},
publisher = {Springer},
title = {{The Design and Analysis of Computer Experiments}},
year = {2003}
}
@article{Williams2006,
abstract = {A flyer plate experiment involves forcing a plane shock wave through stationary test samples of material and measuring the free surface velocity of the target as a function of time. These experiments are conducted to learn about the behavior of materials subjected to high strain rate environments. Computer simulations of flyer plate experiments are conducted with a two-dimensional hydro- dynamic code developed under the Advanced Strategic Computing (ASC) program at Los Alamos National Laboratory. This code incorporates physical models that contain parameters having uncertain values. The objectives of the analyses pre- sented in this paper are to assess the sensitivity of free surface velocity to variations in the uncertain inputs, to constrain the values of these inputs to be consistent with experiment, and to predict free surface velocity based on the constrained inputs. We implement a Bayesian approach that combines detailed physics simulations with experimental data for the desired statistical inference (Kennedy and O'Hagan 2001; Higdon, Kennedy, Cavendish, Cafeo, and Ryne 2004).},
annotate = {r2018-01-12; 2017-11-01
Section 2.3 was especially helpful for building a GP emulator of a computationally expensive emulator. The rest of the paper deals with related topics such as using the emulator for sensitivity analysis. I just wonder if the sort of approach they use here is going to be appropriate for us, given how many observations we have. They use only 20 simulation observations, we use 504 times three simulator outputs.},
author = {Williams, Brian and Higdon, Dave and Gattiker, Jim and Moore, Leslie and McKay, Michael and Keller-McNulty, Sallie},
doi = {10.1214/06-BA125},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams et al. - 2006 - Combining experimental data and computer simulations, with an application to flyer plate experiments(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Gaussian process,calibration,computer experiments,flyer plate experiments,model validation,predictability,predictive science,sensitivity analysis,uncertainty quantification},
month = {dec},
number = {4},
pages = {765--792},
publisher = {International Society for Bayesian Analysis},
title = {{Combining experimental data and computer simulations, with an application to flyer plate experiments}},
url = {http://projecteuclid.org/euclid.ba/1340370942},
volume = {1},
year = {2006}
}
@misc{OHagan1978,
abstract = {The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space X of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of X. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables.},
annotate = {r2018-03-16; 2017-04-20
O'Hagan walks the reader through the use of Gaussian process models for smoothing, curve fitting, and prediction. Much time is spent on design; it is the focus of the work. However the fundamental mathematics of GP models is laid out probably as clearly and thoroughly as it is possible to do. This includes the formulae for multivariate GP models, as well as GP models with uninformative prior means.},
author = {O'Hagan, A.},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
doi = {10.2307/2984861},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan, Kingman - 1978 - Curve Fitting and Optimal Design for Prediction(2).pdf:pdf},
pages = {1--42},
publisher = {WileyRoyal Statistical Society},
title = {{Curve Fitting and Optimal Design for Prediction}},
url = {http://www.jstor.org/stable/2984861},
volume = {40},
year = {1978}
}
@article{Liu2009,
annotate = {r2018-01-09
"Modularizing", ie keeping separate the distinct components of a Bayesian model, can be a useful way to fudge the analysis to keep poor components of the model from ruining the whole thing. The paper is particularly helpful in the treatment of discrepancy in a model. Useful references are given on pages 123 and 127; the latter gives four papers responsible for the "introduction" of Gaussian process response-surface methodology (GASP) for constructing emulators.},
author = {Liu, F. and Bayarri, M. J. and Berger, J. O.},
doi = {10.1214/09-BA404},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Bayarri, Berger - 2009 - Modularization in Bayesian analysis, with emphasis on analysis of computer models(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Complex computer models,Confounding,Emulators,Identifiability,MCMC mixing,Partial likelihood,Random effects},
month = {mar},
number = {1},
pages = {119--150},
publisher = {International Society for Bayesian Analysis},
title = {{Modularization in Bayesian analysis, with emphasis on analysis of computer models}},
url = {http://projecteuclid.org/euclid.ba/1340370392},
volume = {4},
year = {2009}
}
@article{Heaton2017,
abstract = {The Gaussian process is an indispensable tool for spatial data analysts. The onset of the "big data" era, however, has lead to the traditional Gaussian process being computationally infeasible for modern spatial data. As such, various alternatives to the full Gaussian process that are more amenable to handling big spatial data have been proposed. These modern methods often exploit low rank structures and/or multi-core and multi-threaded computing environments to facilitate computation. This study provides, first, an introductory overview of several methods for analyzing large spatial data. Second, this study describes the results of a predictive competition among the described methods as implemented by different groups with strong expertise in the methodology. Specifically, each research group was provided with two training datasets (one simulated and one observed) along with a set of prediction locations. Each group then wrote their own implementation of their method to produce predictions at the given location and each which was subsequently run on a common computing environment. The methods were then compared in terms of various predictive diagnostics. Supplementary materials regarding implementation details of the methods and code are available for this article online.},
annotate = {r2018-01-09
Very nice overview of methods for analyzing large spatial data, both established and new innovations, and good comparison of them with code available. The article is not so helpful for understanding these methods, though the references here are a valuable collection toward that end.},
journal = {unpublished},
archivePrefix = {arXiv},
arxivId = {1710.05013},
author = {Heaton, Matthew J. and Datta, Abhirup and Finley, Andrew and Furrer, Reinhard and Guhaniyogi, Rajarshi and Gerber, Florian and Gramacy, Robert B. and Hammerling, Dorit and Katzfuss, Matthias and Lindgren, Finn and Nychka, Douglas W. and Sun, Furong and Zammit-Mangion, Andrew},
eprint = {1710.05013},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heaton et al. - 2017 - Methods for Analyzing Large Spatial Data A Review and Comparison.pdf:pdf},
month = {oct},
title = {{Methods for Analyzing Large Spatial Data: A Review and Comparison}},
url = {http://arxiv.org/abs/1710.05013},
year = {2017}
}
@article{Haario2005,
annotate = {To read},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.1007/BF02789703},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario, Saksman, Tamminen - 2005 - Componentwise adaptation for high dimensional MCMC(2).pdf:pdf},
issn = {0943-4062},
journal = {Computational Statistics},
month = {jun},
number = {2},
pages = {265--273},
publisher = {Springer-Verlag},
title = {{Componentwise adaptation for high dimensional MCMC}},
url = {http://link.springer.com/10.1007/BF02789703},
volume = {20},
year = {2005}
}
@article{Brown2016,
abstract = {Standard methods in computer model calibration treat the calibration parameters as constant throughout the domain of control inputs. In many applications, systematic variation may cause the best values for the calibration parameters to change between different settings. When not accounted for in the code, this variation can make the computer model inadequate. In this article, we propose a framework for modeling the calibration parameters as functions of the control inputs to account for a computer model's incomplete system representation in this regard while simultaneously allowing for possible constraints imposed by prior expert opinion. We demonstrate how inappropriate modeling assumptions can mislead a researcher into thinking a calibrated model is in need of an empirical discrepancy term when it is only needed to allow for a functional dependence of the calibration parameters on the inputs. We apply our approach to plastic deformation of a visco-plastic self-consistent material in which the critical resolved shear stress is known to vary with temperature.},
annotate = {r2018-01-25; 2017-05-11
The notion of state-aware calibration is introduced and discussed. There are several useful tips for computation, especially on pages 6 and 7.},
archivePrefix = {arXiv},
arxivId = {1602.06202},
author = {Brown, D. Andrew and Atamturktur, Sez},
doi = {10.5705/ss.202015.0344},
eprint = {1602.06202},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Atamturktur - 2016 - Nonparametric Functional Calibration of Computer Models.pdf:pdf},
journal = {Statistica Sinica},
month = {feb},
pages = {721--742},
title = {{Nonparametric Functional Calibration of Computer Models}},
url = {http://arxiv.org/abs/1602.06202 http://dx.doi.org/10.5705/ss.202015.0344},
volume = {28},
year = {2018}
}
@article{Paulo2012,
abstract = {The problem of calibrating computer models that produce multivariate output is considered, with a particular emphasis on the situation where the model is computationally demanding. The proposed methodology builds on Gaussian process-based response-surface approximations to each of the components of the output of the computer model to produce an emulator of the multivariate output. This emulator is then combined in a statistical model involving field observations, which is then used to produce calibration strategies for the parameters of the computer model. The results of applying this methodology to a simulated example and to a real application are presented.},
annotate = {To read},
author = {Paulo, Rui and Garc{\'{i}}a-Donato, Gonzalo and Palomo, Jes{\'{u}}s},
doi = {10.1016/j.csda.2012.05.023},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulo, Garc{\'{i}}a-Donato, Palomo - 2012 - Calibration of computer models with multivariate output.pdf:pdf},
journal = {Computational Statistics and Data Analysis},
pages = {3959--3974},
title = {{Calibration of computer models with multivariate output}},
url = {www.elsevier.com/locate/csda},
volume = {56},
year = {2012}
}
@article{Ohagan2006,
abstract = {The Bayesian approach to quantifying, analysing and reducing uncertainty in the application of complex process models is attracting increasing attention amongst users of such models. The range and power of the Bayesian methods is growing and there is already a sizeable literature on these methods. However, most of it is in specialist statistical journals. The purpose of this tutorial is to introduce the more general reader to the Bayesian approach.},
annotate = {r2018-01-11
Helpful tutorial, though very entry-level. But I found his discussion of sensitivity analysis helpful, and also his list of ongoing areas of research related to Bayesian analysis of computer codes.},
author = {O'Hagan, Anthony},
doi = {10.1016/j.ress.2005.11.025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan - 2006 - Bayesian analysis of computer code outputs A tutorial.pdf:pdf},
journal = {Reliability Engineering and System Safety},
keywords = {Bayesian statistics,Calibration,Dimensionality reduction,Emulator,Gaussian process,Roughness,Screening,Sensitivity analysis,Smoothness,Uncertainty analysis,Validation},
pages = {1290--1300},
title = {{Bayesian analysis of computer code outputs: A tutorial}},
url = {https://ac.els-cdn.com/S0951832005002383/1-s2.0-S0951832005002383-main.pdf?{\_}tid=5fff22dc-f649-11e7-b6db-00000aacb35d{\&}acdnat=1515618247{\_}514575f75406f47f4a7989abf17ff2a5},
volume = {91},
year = {2006}
}
@article{Loeppky2006,
abstract = {Computer models to simulate physical phenomena are now widely available in engineering and science. Before relying on a computer model, a natural first step is often to compare its output with physical or field data, to assess whether the computer model reliably represents the real world. Field data, when available, can also be used to calibrate or tune unknown parameters in the computer model. Calibration is particularly problematic in the presence of systematic discrepancies between the computer model and field observations. We introduce a likelihood alternative to previous Bayesian methodology for estimation of calibration or tuning parameters. In an important special case, we show that maximum likelihood estimation will asymptotically find values of the calibration parameter that give an unbiased computer model, if such a model exists. However, the calibration parameters are not necessarily estimable. We also show in some settings that calibration or tuning need to take into account the end-use prediction strategy. Depending on the strategy, the choice of the parameter values may be critical or unimportant.},
annotate = {r2018-04-06
Very clearly written. The authors show that MLE calibration is asymptotically unbiased -- not in the sense that the calibration parameter estimate is unbiased, but in the sense that the calibration parameter estimate provides that the discrepancy function is 0; also this asymptotically the case in that it holds if you can draw indefinitely many times from the simulation.},
author = {Loeppky, Jason L. and Bingham, Derek and Welch, William J.},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Loeppky, Bingham, Welch - 2006 - Computer Model Calibration or Tuning in Practice.pdf:pdf},
journal = {Technometrics},
title = {{Computer Model Calibration or Tuning in Practice}},
url = {https://www.researchgate.net/profile/Jason{\_}Loeppky/publication/228936502{\_}Computer{\_}model{\_}calibration{\_}or{\_}tuning{\_}in{\_}practice/links/0c960525da9e07f2d1000000.pdf},
year = {2006}
}
@article{Han2009,
abstract = {This article introduces a Bayesian methodology for the prediction for computer experiments having quantitative and qualitative inputs. The proposed model is a hierarchical Bayesian model with conditional Gaussian stochastic process components. For each of the qualitative inputs, our model assumes that the outputs corresponding to different levels of the qualitative input have “similar” functional behavior in the quantitative inputs. The predictive accuracy of this method is compared with the predictive accuracies of alternative proposals in examples. The method is illustrated in a biomechanical engineering application.},
annotate = {To read},
author = {Han, Gang and Santner, Thomas J. and Notz, William I. and Bartel, Donald L.},
doi = {10.1198/tech.2009.07132},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2009 - Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Gaussian stochastic process model,Hierarchical Bayesian model,Product Gaussian correlation,Root mean squared prediction error},
month = {aug},
number = {3},
pages = {278--288},
publisher = {Taylor {\&} Francis},
title = {{Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2009.07132},
volume = {51},
year = {2009}
}
@article{Zhang2015,
abstract = {ABSTRACTIn this article, we review and reexamine approaches to modeling computer experiments with qualitative and quantitative input variables. For those not familiar with models for computer experiments, we begin by showing, in a simple setting, that a standard model for computer experiments can be viewed as a generalization of regression models. We then review models that include both quantitative and quantitative variables and present some alternative parameterizations. Two are based on indicator functions and allow one to use standard quantitative inputs-only models. Another parameterization provides additional insight into possible underlying factorial structure. Finally, we use two examples to illustrate the benefits of these alternative models},
annotate = {Very helpful overview focusing on the work of Han et al 2009, Qian et al 2008, and Zhou et al 2011.},
author = {Zhang, Yulei and Notz, William I.},
doi = {10.1080/08982112.2015.968039},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Notz - 2015 - Computer Experiments with Qualitative and Quantitative Variables A Review and Reexamination.pdf:pdf},
issn = {0898-2112},
journal = {Quality Engineering},
keywords = {GaSP model,Gaussian correlation function,best linear unbiased predictor,computer experiments,indicator variables,qualitative input},
month = {jan},
number = {1},
pages = {2--13},
publisher = {Taylor {\&} Francis},
title = {{Computer Experiments with Qualitative and Quantitative Variables: A Review and Reexamination}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08982112.2015.968039},
volume = {27},
year = {2015}
}
@article{Storlie2013,
annotate = {To read},
author = {Storlie, Curtis B. and Reich, Brian J. and Helton, Jon C. and Swiler, Laura P. and Sallaberry, Cedric J.},
doi = {10.1016/j.ress.2012.11.018},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2013 - Analysis of computationally demanding models with continuous and categorical inputs.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
month = {may},
pages = {30--41},
title = {{Analysis of computationally demanding models with continuous and categorical inputs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832012002487},
volume = {113},
year = {2013}
}
@article{Zhou2011,
abstract = {We propose a flexible yet computationally efficient approach for building Gaussian process models for computer experiments with both qualitative and quantitative factors. This approach uses the hypersphere parameterization to model the correlations of the qualitative factors, thus avoiding the need of directly solving optimization problems with positive definite constraints. The effectiveness of the proposed method is successfully illustrated by several examples.},
annotate = {r2017-07-14
The computational efficiency of the technique described in Qian et al 2008 is vastly improved here. However, this version still requires m(m-1)/2 parameters when the qualitative factor has m levels. For a single factor of 1000 materials, that is almost 500,000 variables; for two factors of 20 and 50 levels resp., it is 1415 parameters. Restrictions of the covariance matrix may allow reduction in the number of parameters without too far mitigating the utility of the framework.},
author = {Zhou, Qiang and Qian, Peter Z. G. and Zhou, Shiyu},
doi = {10.1198/TECH.2011.10025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Qian, Zhou - 2011 - A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiment,Hypersphere decomposition,Kriging},
month = {aug},
number = {3},
pages = {266--273},
publisher = {Taylor {\&} Francis},
title = {{A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.10025},
volume = {53},
year = {2011}
}
@article{Qian2008,
abstract = {Modeling experiments with qualitative and quantitative factors is an important issue in computer modeling. We propose a framework for building Gaussian process models that incorporate both types of factors. The key to the development of these new models is an approach for constructing correlation functions with qualitative and quantitative factors. An iterative estimation procedure is developed for the proposed models. Modern optimization techniques are used in the estimation to ensure the validity of the constructed correlation functions. The proposed method is illustrated with an example involving a known function and a real example for modeling the thermal distribution of a data center.},
annotate = {r2017-07-13
The authors describe and provide theoretical grounding for a framework that allows one to use qualitative input to determine which model to apply to the quantitative input.},
author = {Qian, Peter Z. G and Wu, Huaiqing and Wu, C. F. Jeff},
doi = {10.1198/004017008000000262},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Wu, Wu - 2008 - Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Cokriging,Design of experiments,Kriging,Multivariate Gaussian process,Semidefinite programming},
month = {aug},
number = {3},
pages = {383--396},
publisher = {Taylor {\&} Francis},
title = {{Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017008000000262},
volume = {50},
year = {2008}
}
@article{Oakley2004,
annotate = {To read},
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
doi = {10.1111/j.1467-9868.2004.05304.x},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oakley, O'Hagan - 2004 - Probabilistic sensitivity analysis of complex models a Bayesian approach(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Bayesian inference,Computer model,Gaussian process,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {751--769},
publisher = {Blackwell Publishing},
title = {{Probabilistic sensitivity analysis of complex models: a Bayesian approach}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2004.05304.x},
volume = {66},
year = {2004}
}
@article{Muehlenstaedt2017,
annotate = {r2017-07-10
The authors propose an appropriate distance metric and demonstrate its use in creating LHC designs for experiments with functional input. I wonder how much its utility depends on the degree of knowledge we have about the shape of the functional input.},
author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
doi = {10.1007/s11222-016-9672-z},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muehlenstaedt, Fruth, Roustant - 2017 - Computer experiments with functional inputs and scalar outputs by a norm-based approach.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
month = {jul},
number = {4},
pages = {1083--1097},
publisher = {Springer US},
title = {{Computer experiments with functional inputs and scalar outputs by a norm-based approach}},
url = {http://link.springer.com/10.1007/s11222-016-9672-z},
volume = {27},
year = {2017}
}
@article{Harari2017,
abstract = {We revisit the problem of determining the sample size for a Gaussian process emulator and provide a data analytic tool for exact sample size calculations that goes beyond the n = 10d rule of thumb and is based on an IMSPE-related criterion. This allows us to tie sample size and prediction accuracy to the anticipated roughness of the simulated data, and to propose an experimental process for computer experiments, with extension to a robust scheme.},
annotate = {r2017-07-10
Who would have thought the n=10d rule would turn out to be an oversimplification? Here's an IMPSE-based alternative, along with a good takedown of the 10d rule.},
author = {Harari, Ofir and Bingham, Derek and Dean, Angela and Higdon, David and Author, Corresponding and Higdon, Dave},
doi = {10.5705/ss.202016.0217},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harari et al. - 2017 - Preprint Computer Experiments Prediction Accuracy, Sample Size and Model Complexity Revisited Complete List of Au.pdf:pdf},
journal = {Statistica Sinica},
keywords = {Preprint},
mendeley-tags = {Preprint},
title = {{Computer Experiments: Prediction Accuracy, Sample Size and Model Complexity Revisited}},
url = {http://www.stat.sinica.edu.tw/statistica/},
year = {2017}
}
@article{Joseph2015,
abstract = {Engineering model development involves several simplifying assumptions for the purpose of mathematical tractability, which are often not realistic in practice. This leads to discrepancies in the model predictions. A commonly used statistical approach to overcome this problem is to build a statistical model for the discrepancies between the engineering model and observed data. In contrast, an engineering approach would be to find the causes of discrepancy and fix the engineering model using first principles. However, the engineering approach is time consuming, whereas the statistical approach is fast. The drawback of the statistical approach is that it treats the engineering model as a black box and therefore, the statistically adjusted models lack physical interpretability. This article proposes a new framework for model calibration and statistical adjustment. It tries to open up the black box using simple main effects analysis and graphical plots and introduces statistical models inside the engineering m...},
annotate = {To read},
author = {Joseph, V. Roshan and Yan, Huan},
doi = {10.1080/00401706.2014.902773},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph, Yan - 2015 - Engineering-Driven Statistical Adjustment and Calibration.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiments,Functional ANOVA,Gaussian process,Nonlinear regression.},
month = {apr},
number = {2},
pages = {257--267},
publisher = {Taylor {\&} Francis},
title = {{Engineering-Driven Statistical Adjustment and Calibration}},
url = {http://www.tandfonline.com/doi/full/10.1080/00401706.2014.902773},
volume = {57},
year = {2015}
}
@article{Wong2017,
annotate = {To read},
author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
doi = {10.1111/rssb.12182},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Storlie, Lee - 2014 - A Frequentist Approach to Computer Model Calibration(2).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Bootstrap,Inverse problem,Model misspecification,Semiparametric modelling,Surrogate model,Uncertainty analysis},
month = {mar},
number = {2},
pages = {635--648},
title = {{A frequentist approach to computer model calibration}},
url = {http://doi.wiley.com/10.1111/rssb.12182},
volume = {79},
year = {2017}
}
@article{Storlie2015,
abstract = {It has become commonplace to use complex computer models to predict outcomes in regions where data do not exist. Typically these models need to be calibrated and validated using some experimental data, which often consists of multiple correlated outcomes. In addition, some of the model parameters may be categorical in nature, such as a pointer variable to alternate models (or submodels) for some of the physics of the system. Here, we present a general approach for calibration in such situations where an emulator of the computationally demanding models and a discrepancy term from the model to reality are represented within a Bayesian smoothing spline (BSS) ANOVA framework. The BSS-ANOVA framework has several advantages over the traditional Gaussian process, including ease of handling categorical inputs and correlated outputs, and improved computational efficiency. Finally, this framework is then applied to the problem that motivated its design; a calibration of a computational fluid dynamics (CFD) model of...},
annotate = {r2017-07-13
This work sees itself as an advance on the work of Qian et al 2008 and Zhou et al 2011 in that it focuses specifically on calibration (rather than sensitivity analysis or uncertainty analysis). The authors support the use of "Bayesian Smoothing Spline ANOVA", which may offer some advantages in computational efficiency. But the authors also, for comparison, extend the approach of Higdon 2008 for use with qualitative input.},
author = {Storlie, Curtis B. and Lane, William A. and Ryan, Emily M. and Gattiker, James R. and Higdon, David M.},
doi = {10.1080/01621459.2014.979993},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2015 - Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Sp.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Categorical inputs,Emulator,Inverse problem,Model calibration,Multiple outputs,Uncertainty quantification},
month = {jan},
number = {509},
pages = {68--82},
publisher = {Taylor {\&} Francis},
title = {{Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Spline ANOVA}},
url = {http://www.tandfonline.com/doi/full/10.1080/01621459.2014.979993},
volume = {110},
year = {2015}
}
@article{Higdon2008a,
abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis repre...},
annotate = {r2017-03-30
The authors' approach is based on Kennedy and O'Hagan 2001; latter use GP models. The current authors extend it to allow for highly multivariate output, keeping computational costs low enough for MCMC to be workable. High dimensionality for them still means less than 100. More than that is "beyond the scope of the approach given here". Although K{\&}O'H's approach was not "fully Bayesian", the current authors give it a "fully Bayesian" description, which better matches their own extension.},
author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
doi = {10.1198/016214507000000888},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2008 - Computer Model Calibration Using High-Dimensional Output.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer experiments,Functional data analysis,Gaussian process,Prediction,Predictive science,Uncertainty quantification},
month = {jun},
number = {482},
pages = {570--583},
publisher = {Taylor {\&} Francis},
title = {{Computer Model Calibration Using High-Dimensional Output}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214507000000888},
volume = {103},
year = {2008}
}
@article{Higdon2004,
abstract = {We develop a statistical approach for characterizing uncertainty in predictions that are made with the aid of a computer simulation model. Typically, the computer simulation code models a physical system and requires a set of inputs---some known and specified, others unknown. A limited amount of field data from the true physical system is available to inform us about the unknown inputs and also to inform us about the uncertainty that is associated with a simulation-based prediction. The approach given here allows for the following:uncertainty regarding model inputs (i.e., calibration); accounting for uncertainty due to limitations on the number of simulations that can be carried out; discrepancy between the simulation code and the actual physical system; uncertainty in the observation process that yields the actual field data on the true physical system. The resulting analysis yields predictions and their associated uncertainties while accounting for multiple sources of uncertainty. We use a Bayesian form...},
annotate = {r2018-01-15
Found this extremely helpful. There's a lot of overlap with the 2007 flyer plate paper, but here it is, in my opinion, explained more clearly and directly. After reading this, the flyer plate paper makes more sense to me. Anyway, it is mostly helpful in just laying out clearly the basics of the kind of analysis they are doing in the 2007 paper, and the kind we are doing in NSF-DEMS.},
author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
doi = {10.1137/S1064827503426693},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2004 - Combining Field Data and Computer Simulations for Calibration and Prediction.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {60G15,62F15,62M30,62P30,62P35,Gaussian process,calibration,computer experiments,model validation,predictability,simulator science,uncertainty quantification},
month = {jan},
number = {2},
pages = {448--466},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Combining Field Data and Computer Simulations for Calibration and Prediction}},
url = {http://epubs.siam.org/doi/10.1137/S1064827503426693},
volume = {26},
year = {2004}
}
@article{Bayarri,
abstract = {A key question in evaluation of computer models is Does the computer model adequately represent reality? A six-step process for computer model validation is set out in Bayarri et al. [Technometrics 49 (2007) 138-154] (and briefly summarized below), based on comparison of computer model runs with field data of the process being modeled. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and being able to adapt to different, but related scenarios. Two complications that frequently arise in practice are the need to deal with highly irregular functional data and the need to acknowledge and incorporate uncertainty in the inputs. We develop methodology to deal with both complications. A key part of the approach utilizes a wavelet representation of the functional data, applies a hierarchical version of the scalar validation methodology to the wavelet coefficients, and transforms back, to ultimately compare computer model output with field output. The generality of the methodology is only limited by the capability of a combination of computational tools and the appropriateness of decompositions of the sort (wavelets) employed here. The methods and analyses we present are illustrated with a test bed dynamic stress analysis for a particular engineering system.},
annotate = {r2017-07-12; 2017-02-03
Six steps for their technique: 1. define problem 2. establish evaluation criteria 3. design experiments 4. approximate computer model output 5. analyze the combo of field and computer run data 6. feed back to revise the model, perform additional experiments, and so on.
Current paper attempts to deal with 3 new problems: 1. irregular function output 2. uncertainty in inputs 3. prediction in altered/new settings
The authors exhort the use of "tolerance bounds" as measures of model accuracy. One reason is that tolerance bounds account for model bias. See Bayarri et al [4] for details. Overview: sec. 2 describes example problem. Sec. 3 formulates the statistical problem and assumptions. Sec. 4 has methods of analysis. Sec. 5, results.},
author = {Bayarri, M. J. and Berger, J. O. and Cafeo, J. and Garcia-Donato, G. and Liu, F. and Palomo, J. and Parthasarathy, R. J. and Paulo, R. and Sacks, J. and Walsh, D.},
doi = {10.2307/25464566},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayarri et al. - 2007 - Computer Model Validation with Functional Output.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {Approximation,Calibration,Computer modeling,Experiment design,Modeling,Parametric models,Simulations,Statistical models,Statistics,Vehicles},
pages = {1874--1906},
publisher = {Institute of Mathematical Statistics},
title = {{Computer Model Validation with Functional Output}},
url = {http://www.jstor.org/stable/25464566},
volume = {35},
year = {2007}
}
@article{Bayarri2007,
abstract = {We present a framework that enables computer model evaluation oriented toward answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based on Bayesian and likelihood methodology. The Bayesian methodology is particularly well suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models, combining multiple sources of information, and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is implemented in a test bed example of resistance spot welding, to provide context for each of the six steps in the proposed validation process.},
annotate = {r2018-01-10; 2017-07-11
The authors advance the method of using "modularization" which Berger and Bayarri focus on more thoroughly in their 2009 paper with Liu. The paper includes useful citations on dealing with high-dimensional output data (p. 141). The authors note (p.145) that a non-constant mean function may be called for if you want extrapolation from your GP model. There is some discussion of the trade-offs between using MLEs for hyperparameters or using a full Bayesian analysis; the authors tentatively weigh in for the former option. The authors assert (p. 150) that they have shown that, contrary to common opinion, data can be used simultaneously for calibration/tuning and for model validation, by "incorporating the posterior distribution of the tuning parameters in the overall assessment of uncertainties."},
author = {Bayarri, Maria J and Berger, James O and Paulo, Rui and Sacks, Jerry and Cafeo, John A and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
doi = {10.1198/004017007000000092},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayarri et al. - 2007 - A Framework for Validation of Computer Models.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian analysis,Identifiability,Model discrepancy,Prediction},
month = {may},
number = {2},
pages = {138--154},
publisher = {Taylor {\&} Francis},
title = {{A Framework for Validation of Computer Models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017007000000092},
volume = {49},
year = {2007}
}
@article{Kennedy2001,
annotate = {r2018-01-11; 2016-05-30
Very thorough and detailed development of the foundation of Bayesian calibration of computer models. Useful walkthrough of that foundation.

2018-04-16
The authors assert that theirs is the first attempt to model all the sources of uncertainty in calibration of computer models. This despite the fact that theirs is not a full Bayesian analysis; they use MLEs for some hyperparameters. The authors are explicit in considering calibration to be a matter of finding ``best-fitting" values for calibration parameters; that is, calibration is for them hopelessly confounded with minimizing model inadequacy. (For the same reason, they even suggest performing model calibration on parameters whose true physical value is actually known.) In the examples considered by the authors, they integrate by crude quadrature (rather than MCMC).},
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
month = {aug},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Tuo2017,
abstract = {Identification of model parameters in computer simulations is an important topic in computer experiments. We propose a new method, called the projected kernel calibration method, to estimate these model parameters. The proposed method is proven to be asymptotic normal and semi-parametric efficient. As a frequentist method, the proposed method is as efficient as the {\$}L{\_}2{\$} calibration method proposed by Tuo and Wu [Ann. Statist. 43 (2015) 2331-2352]. On the other hand, the proposed method has a natural Bayesian version, which the {\$}L{\_}2{\$} method does not have. This Bayesian version allows users to calculate the credible region of the calibration parameters without using a large sample approximation. We also show that, the inconsistency problem of the calibration method proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat. Methodol. 63 (2001) 425-464] can be rectified by a simple modification of the kernel matrix.},
annotate = {To read},
archivePrefix = {arXiv},
journal = {unpublished},
arxivId = {1705.03422},
author = {Tuo, Rui},
eprint = {1705.03422},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuo - 2017 - Adjustments to Computer Models via Projected Kernel Calibration.pdf:pdf},
month = {may},
title = {{Adjustments to Computer Models via Projected Kernel Calibration}},
url = {http://arxiv.org/abs/1705.03422},
year = {2017}
}
@article{Metropolis1953,
abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
month = {jun},
number = {6},
pages = {1087--1092},
publisher = {American Institute of PhysicsAIP},
title = {{Equation of state calculations by fast computing machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
