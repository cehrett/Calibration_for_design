@article{Atamturktur2015,
address = {San Diego, CA},
annote = {r2018-05-11
The method of state-aware calibration is described and demonstrated. The method for sampling the state-aware parameter is a bit confusing to me.},
author = {Atamturktur, Sez and Brown, D. Andrew},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Atamturktur, Brown - 2015 - State-aware calibration for inferring systematic bias in computer models of complex systems.pdf:pdf},
journal = {NAFEMS World Congress Proceedings, June 21-24},
title = {{State-aware calibration for inferring systematic bias in computer models of complex systems}},
year = {2015}
}
@article{Atamturktur2017,
abstract = {In experiment-based validation, uncertainties and systematic biases in model predictions are reduced by either increasing the amount of experimental evidence available for model calibration—thereby mitigating prediction uncertainty—or increasing the rigor in the definition of physics and/or engineering principles—thereby mitigating prediction bias. Hence, decision makers must regularly choose between either allocating resources for experimentation or further code development. The authors propose a decision-making framework to assist in resource allocation strictly from the perspective of predictive maturity and demonstrate the application of this framework on a nontrivial problem of predicting the plastic deformation of polycrystals.},
annote = {To read},
author = {Atamturktur, Sez and Hegenderfer, J and Williams, B and Egeberg, M and Lebensohn, R A and Unal, C},
doi = {10.1080/15376494.2013.828819},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Atamturktur et al. - 2017 - Mechanics of Advanced Materials and Structures A Resource Allocation Framework for Experiment- Based Validat.pdf:pdf},
issn = {1537-6532},
journal = {Mechanics of Advanced Materials and Structures},
keywords = {Bayesian inference,material plasticity models,model calibration,predictive maturity,uncertainty quantification,viscoplastic self-consistent},
number = {8},
pages = {641--654},
title = {{Mechanics of advanced materials and structures: a resource allocation framework for experiment-based validation of numerical models}},
url = {http://www.tandfonline.com/action/journalInformation?journalCode=umcm20 http://dx.doi.org/10.1080/15376494.2013.828819},
volume = {22},
year = {2017}
}
@article{Au2003,
author = {Au, Siu-Kui and Beck, James L.},
doi = {10.1061/(ASCE)0733-9399(2003)129:8(901)},
issn = {0733-9399},
journal = {Journal of Engineering Mechanics},
number = {8},
pages = {901--917},
title = {{Subset simulation and its application to seismic risk based on dynamic analysis}},
url = {http://ascelibrary.org/doi/10.1061/{\%}28ASCE{\%}290733-9399{\%}282003{\%}29129{\%}3A8{\%}28901{\%}29},
volume = {129},
year = {2003}
}
@article{Au2001a,
abstract = {A new simulation approach, called ‘subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made sufficiently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and efficiently estimated by means of simulation. The conditional probabilities cannot be estimated efficiently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and efficient in computing small probabilities. The efficiency of the method is demonstrated by calculating the first-excursion probabilities for a linear oscillator subjected to white noise excitation and for a five-story nonlinear hysteretic shear building under uncertain seismic excitation.},
author = {Au, Siu-Kui and Beck, James L.},
doi = {10.1016/S0266-8920(01)00019-4},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Au, Beck - 2001 - Estimation of small failure probabilities in high dimensions by subset simulation.pdf:pdf},
issn = {0266-8920},
journal = {Probabilistic Engineering Mechanics},
number = {4},
pages = {263--277},
publisher = {Elsevier},
title = {{Estimation of small failure probabilities in high dimensions by subset simulation}},
url = {https://www.sciencedirect.com/science/article/pii/S0266892001000194},
volume = {16},
year = {2001}
}
@article{Bastos2009,
abstract = {Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different...},
annote = {r2018-03-20
Gaussian process emulators, their advantages and disadvantages, are discussed along with diagnostics to assess successful emulation. The paper discusses both numerical and graphical diagnostics. There is helpful discussion about how to identify what specific problems might be causing poor diagnostic results. Some of the techniques discussed in the paper include the use of standardized prediction errors (from a validation set), the Mahalanobis distance between the emulator and simulator outputs, and variance decompositions. Early in the paper there is a very helpful bit of background/foundation of Gaussian process emulation, including guidance on how to proceed when the observation variance is unknown and must be estimated (and the resulting Student process distribution).},
author = {Bastos, Leonardo S. and O'Hagan, Anthony},
doi = {10.1198/TECH.2009.08019},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastos, O'Hagan - 2009 - Diagnostics for Gaussian Process Emulators.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian inference,Computer experiments,Diagnostics,Emulation,Gaussian process},
number = {4},
pages = {425--438},
publisher = {Taylor {\&} Francis},
title = {{Diagnostics for Gaussian process emulators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2009.08019},
volume = {51},
year = {2009}
}
@article{Bayarri2007a,
author = {Bayarri, Maria J. and Berger, J. O. and Cafeo, J.},
doi = {10.2307/25464566},
journal = {The Annals of Statistics},
keywords = {Approximation,Calibration,Computer modeling,Experiment design,Modeling,Parametric models,Simulations,Statistical models,Statistics,Vehicles},
pages = {1874--1906},
publisher = {Institute of Mathematical Statistics},
title = {{Computer model validation with functional output}},
url = {http://www.jstor.org/stable/25464566},
volume = {35},
year = {2007a}
}
@article{Bayarri2007b,
author = {Bayarri, Maria J. and Berger, James O and Paulo, Rui and Sacks, Jerry and Cafeo, John A and Cavendish, James and Lin, Chin-Hsu and Tu, Jian},
doi = {10.1198/004017007000000092},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bayarri et al. - 2007 - A Framework for Validation of Computer Models.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian analysis,Identifiability,Model discrepancy,Prediction},
number = {2},
pages = {138--154},
publisher = {Taylor {\&} Francis},
title = {{A framework for validation of computer models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017007000000092},
volume = {49},
year = {2007b}
}
@article{Bornstein1976,
abstract = {The pace of life},
author = {Bornstein, Marc H. and Bornstein, Helen G.},
doi = {10.1038/259557a0},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/BORNSTEIN, BORNSTEIN - 1976 - The pace of life.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {5544},
pages = {557--559},
publisher = {Nature Publishing Group},
title = {{The pace of life}},
url = {http://www.nature.com/doifinder/10.1038/259557a0},
volume = {259},
year = {1976}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, de Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Model.pdf:pdf},
title = {{A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Brown2016,
abstract = {Standard methods in computer model calibration treat the calibration parameters as constant throughout the domain of control inputs. In many applications, systematic variation may cause the best values for the calibration parameters to change between different settings. When not accounted for in the code, this variation can make the computer model inadequate. In this article, we propose a framework for modeling the calibration parameters as functions of the control inputs to account for a computer model's incomplete system representation in this regard while simultaneously allowing for possible constraints imposed by prior expert opinion. We demonstrate how inappropriate modeling assumptions can mislead a researcher into thinking a calibrated model is in need of an empirical discrepancy term when it is only needed to allow for a functional dependence of the calibration parameters on the inputs. We apply our approach to plastic deformation of a visco-plastic self-consistent material in which the critical resolved shear stress is known to vary with temperature.},
annote = {r2018-01-25; 2017-05-11
The notion of state-aware calibration is introduced and discussed. There are several useful tips for computation, especially on pages 6 and 7.},
archivePrefix = {arXiv},
arxivId = {1602.06202},
author = {Brown, D. Andrew and Atamturktur, Sez},
doi = {10.5705/ss.202015.0344},
eprint = {1602.06202},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Atamturktur - 2016 - Nonparametric Functional Calibration of Computer Models.pdf:pdf},
journal = {Statistica Sinica},
pages = {721--742},
title = {{Nonparametric functional calibration of computer models}},
url = {http://arxiv.org/abs/1602.06202 http://dx.doi.org/10.5705/ss.202015.0344},
volume = {28},
year = {2018}
}
@article{Brynjarsdottir2014,
abstract = {Science-based simulation models are widely used to predict the behavior of complex physical systems. It is also common to use observations of the physical system to solve the inverse problem, i.e. to learn about the values of parameters within the model, a process often called calibration. The main goal of calibration is usually to improve the predictive performance of the simulator but the values of the parameters in the model may also be of intrinsic scientific interest in their own right. In order to make appropriate use of observations of the physical system it is impor-tant to recognise model discrepancy, the difference between reality and the simulator output. We illustrate through a simple example that an analysis that does not account for model discrepancy may lead to biased and over-confident parameter estimates and predictions. The challenge with incorporating model discrepancy in statistical inverse problems is the confounding with calibration parameters, which will only be resolved with mean-ingful priors. For our simple example, we model the model-discrepancy via a Gaus-sian Process and demonstrate that by accounting for model discrepancy our prediction within the range of data is correct. However, only with realistic priors on the model discrepancy do we uncover the true parameter values. Through theoretical arguments we show that these findings are typical of the general problem of learning about physical parameters and the underlying physical system using science-based mechanistic models.},
annote = {r2018-01-31; 2017-09-08
Looks like in order to take the lesson of this paper to heart, one would have to be really thoughtful about the physical ground of the discrepancy function, in crafting as narrow constraints as possible. No black boxes here.},
author = {Brynjarsd{\'{o}}ttir, Jenni and O'Hagan, Anthony},
doi = {10.1088/0266-5611/30/11/114007},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brynjarsd{\'{o}}ttir, Ohagan - 2014 - Learning about physical parameters The importance of model discrepancy(2).pdf:pdf},
issn = {13616420},
journal = {Inverse Problems},
keywords = {calibration,computer models,extrapolation,model form error,simulation model,structural uncertainty,uncertainty quantification},
number = {11},
title = {{Learning about physical parameters: The importance of model discrepancy}},
volume = {30},
year = {2014}
}
@article{Cao2018,
abstract = {The purpose of model calibration is to make the model predictions closer to reality. The classical Kennedy–O'Hagan approach is widely used for model calibration, which can account for the inadequacy of the computer model while simultaneously estimating the unknown calibration parameters. In many applications, the phenomenon of censoring occurs when the exact outcome of the physical experiment is not observed, but is only known to fall within a certain region. In such cases, the Kennedy–O'Hagan approach cannot be used directly, and we propose a method to incorporate the censoring information when performing model calibration. The method is applied to study the compression phenomenon of liquid inside a bottle. The results show significant improvement over the traditional calibration methods, especially when the number of censored observations is large. Supplementary materials for this article are available online.},
annote = {2018-07-03
The authors have an exact method, which is very computationally expensive, and an approximate method, which is almost as good when the number of censored observations are large or the censored interval is small. The approximate method essentially estimates the censored observations as the mean of the censored interval with extra variance. It's not clear what advantage would attach to using this approach for calibration to desired observations.
This article has a useful bibliography.},
author = {Cao, Fang and Ba, Shan and Brenneman, William A. and Joseph, V. Roshan},
doi = {10.1080/00401706.2017.1345704},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2018 - Model Calibration With Censored Data.pdf:pdf},
journal = {Technometrics},
number = {2},
pages = {255--262},
publisher = {Taylor {\&} Francis},
title = {{Model calibration with censored data}},
url = {https://www.tandfonline.com/doi/full/10.1080/00401706.2017.1345704},
volume = {60},
year = {2018}
}
@article{Cauchy1847,
author = {Cauchy, Augustin},
journal = {Comptes Rendus Hebdomadaires des S{\'{e}}ances de L'Acad{\'{e}}mie des Sciences},
number = {July-December},
pages = {536--538},
title = {{M{\'{e}}thode g{\'{e}}n{\'{e}}rale pour la r{\'{e}}solution des syst{\`{e}}mes d'{\'{e}}quations simultan{\'{e}}es}},
volume = {25},
year = {1847}
}
@article{Cavelaars2000,
abstract = {Primary objectives: This paper aims to provide an overview of variations in average height between 10 European countries, and between socio-economic groups within these countries.Data and methods: Data on self-reported height of men and women aged 20-74 years were obtained from national health, level of living or multipurpose surveys for 1987-1994. Regression analyses were used to estimate height differences between educational groups and to evaluate whether the differences in average height between countries and between educational groups were smaller among younger than among older birth cohorts.Results: Men and women were on average tallest in Norway, Sweden, Denmark and the Netherlands and shortest in France, Italy and Spain (range for men: 170-179 cm; range for women: 160-167 cm). The differences in average height between northern and southern European countries were not smaller among younger than among older birth cohorts. In most countries average height increased linearly with increasing birth-year...},
author = {Cavelaars, A. E. J. M. and Kunst, A. E. and Geurts, J. J. M. and Crialesi, R. and Gr{\"{o}}tvedt, L. and Helmert, U. and Lahelma, E. and Lundberg, O. and Mielck, A. and Rasmussen, N. Kr. and Regidor, E. and Spuhler, Th. and Mackenbach, J. P.},
doi = {10.1080/03014460050044883},
issn = {0301-4460},
journal = {Annals of Human Biology},
number = {4},
pages = {407--421},
publisher = {Taylor {\&} Francis},
title = {{Persistent variations in average height between countries and between socio-economic groups: an overview of 10 European countries}},
url = {http://www.tandfonline.com/doi/full/10.1080/03014460050044883},
volume = {27},
year = {2000}
}
@article{Chang2014,
abstract = {The Gaussian process (GP) model provides a powerful methodology for calibrating a computer model in the presence of model uncertainties. However, if the data contain systematic experimental errors, then the GP model can lead to an unnecessarily complex adjustment of the computer model. In this work, we introduce an adjustment procedure that brings the computer model closer to the data by making minimal changes to it. This is achieved by applying a lasso-based variable selection on the systematic experimental error terms while fitting the GP model. Two real examples and simulations are presented to demonstrate the advantages of the proposed approach. This article has supplementary material available online.},
annote = {r2018-07-20
This is a good paper. The authors explore the idea of calibrating under the presence of experimental bias. Say that, e.g., one set of your experimental observations are biased due to some sort of error specific to that set. Then Kennedy-O'Hagan will just treat this as part of the model discrepancy. But it's not model discrepancy, it's experimental bias. The authors here add a whole new error term, which is controlled by LASSO-style regularization.},
author = {Chang, Chia-Jung and Joseph, V. Roshan},
doi = {10.1080/00401706.2013.850113},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Joseph - 2014 - Model Calibration Through Minimal Adjustments.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiments,Gaussian process,Prediction,Validation,Variable selection},
number = {4},
pages = {474--482},
publisher = {Taylor {\&} Francis},
title = {{Model calibration through minimal adjustments}},
url = {http://www.tandfonline.com/doi/full/10.1080/00401706.2013.850113},
volume = {56},
year = {2014}
}
@article{Chen2016,
author = {Chen, Hao and Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
doi = {10.1214/15-STS531},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2016 - Analysis Methods for Computer Experiments How to Assess and What Counts.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Correlation function,Gaussian process,kriging,prediction accuracy,regression},
number = {1},
pages = {40--60},
publisher = {Institute of Mathematical Statistics},
title = {{Analysis methods for computer experiments: how to assess and what counts?}},
url = {http://projecteuclid.org/euclid.ss/1455115913},
volume = {31},
year = {2016}
}
@article{Chib1995,
abstract = {We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.},
annote = {r2018-01
Useful tutorial on the Metropolis-Hastings algorithm which also very clearly lays out its theoretical underpinnings. Its relationship to the Accept/Reject method is described clearly.},
author = {Chib, Siddhartha and Greenberg, Edward},
doi = {10.2307/2684568},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chib, Greenberg - 1995 - Understanding the Metropolis-Hastings Algorithm.pdf:pdf},
issn = {00031305},
journal = {The American Statistician},
number = {4},
pages = {327},
publisher = {Taylor {\&} Francis, Ltd.American Statistical Association},
title = {{Understanding the Metropolis-Hastings algorithm}},
url = {http://www.jstor.org/stable/2684568?origin=crossref},
volume = {49},
year = {1995}
}
@article{Currin1991,
annote = {r2018-03-09
The article discusses the use of Gaussian process emulators of computer code, with emphasis on design and on selection of a correlation function. Some interesting points about correlation functions are made, such as how the appropriate choice of correlation function yield prediction functions that are either linear or cubic splines in each dimension. The article includes some nice tables comparing the results using various correlation functions, compared also against some polynomial models fitted by least squares.},
author = {Currin, Carla and Mitchell, Toby and Morris, Max and Ylvisaker, Don},
doi = {10.1080/01621459.1991.10475138},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Currin et al. - 1991 - Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experime.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {416},
pages = {953--963},
title = {{Bayesian prediction of deterministic functions, with applications to the design and analysis of computer experiments}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475138},
volume = {86},
year = {1991}
}
@article{Deb2002,
annote = {r2018-05-17
Surprisingly clear and accessible description of the NSGA-II algorithm.},
author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
doi = {10.1109/4235.996017},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deb et al. - 2002 - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
url = {http://ieeexplore.ieee.org/document/996017/},
volume = {6},
year = {2002}
}
@article{DelMoral2006,
annote = {r2018-06-04
I can't claim to have understood much of this. I hoped for more introduction to the notion of SMC, but those concepts were more or less assumed here. I should come back to this one after reading up on SMC.},
author = {{Del Moral}, Pierre and Doucet, Arnaud and Jasra, Ajay},
doi = {10.1111/j.1467-9868.2006.00553.x},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Del Moral, Doucet, Jasra - 2006 - Sequential Monte Carlo samplers.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {Importance sampling,Markov chain Monte Carlo methods,Ratio of normalizing constants,Resampling,Sequential Monte Carlo methods,Simulated annealing},
number = {3},
pages = {411--436},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Sequential Monte Carlo samplers}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2006.00553.x},
volume = {68},
year = {2006}
}
@book{Doucet2001,
abstract = {"Monte Carlo methods are revolutionizing the on-line analysis of data in fields as diverse as financial modeling, target tracking and computer vision. These methods, appearing under the names of bootstrap filters, condensation, optimal Monte Carlo filters, particle filters and survival of the fittest, have made it possible to solve numerically many complex, non-standard problems that were previously intractable. This book presents the first comprehensive treatment of these techniques, including convergence results and applications to tracking, guidance, automated target recognition, aircraft navigation, robot navigation, econometrics, financial modeling, neural networks, optimal control, optimal filtering, communications, reinforcement learning, signal enhancement, model averaging and selection, computer vision, semiconductor design, population biology, dynamic Bayesian networks, and time series analysis. This will be of great value to students, researchers and practitioners, who have some basic knowledge of probability. Arnaud Doucet received the Ph. D. degree from the University of Paris-XI Orsay in 1997. From 1998 to 2000, he conducted research at the Signal Processing Group of Cambridge University, UK. He is currently an assistant professor at the Department of Electrical Engineering of Melbourne University, Australia. His research interests include Bayesian statistics, dynamic models and Monte Carlo methods. Nando de Freitas obtained a Ph.D. degree in information engineering from Cambridge University in 1999. He is presently a research associate with the artificial intelligence group of the University of California at Berkeley. His main research interests are in Bayesian statistics and the application of on-line and batch Monte Carlo methods to machine learning. Neil Gordon obtained a Ph.D. in Statistics from Imperial College, University of London in 1993. He is with the Pattern and Information Processing group at the Defence Evaluation and Research Agency in the United Kingdom. His research interests are in time series, statistical data analysis, and pattern recognition with a particular emphasis on target tracking and missile guidance."-- I: Introduction. An introduction to sequential Monte Carlo methods / Arnaud Doucet, Nando de Freitas, and Neil Gordon -- II: Theoretical issues. Particle filters -- a theoretical perspective / Dan Crisan -- Interacting particle filtering with discrete observations / Pierre Del Moral and Jean Jacod -- III: Strategies for improving sequential Monte Carlo methods. Sequential Monte Carlo methods for optimal filtering / Christophe Andrieu, Arnaud Doucet, and Elena Punskaya -- Deterministic and stochastic particle filters in state-space models / Erik Bolviken and Geir Storvik -- RESAMPLE-MOVE filtering with cross-model jumps / Carlo Berzuini and Walter Gilks -- Improvement strategies for Monte Carlo particle filters / Simon Godsill and Tim Clapp -- Approximating and maximising the likelihood for a general state-space model / Markus Hurzeler and Hans R. Kunsch -- Monte Carlo smoothing and self-organising state-space model / Genshiro Kitagawa and Seisho Sato -- Combined parameter and state estimation in simulation-based filtering / Jane Liu and Mike West -- A theoretical framework for sequential importance sampling with resampling / Jun S. Liu, Rong Chen, and Tanya Logvinenko. Improving regularised particle filters / Christian Musso, Nadia Oudjane, and Francois Le Gland -- Auxiliary variable based particle filters / Michael K. Pitt and Neil Shephard -- Improved particle filters and smoothing / Photis Stavropoulos and D.M. Titterington -- IV: Applications. Posterior Cramer-Rao bounds for sequential estimation / Niclas Bergman -- Statistical models of visual shape and motion / Andrew Blake, Michael Isard, and John MacCormick -- Sequential Monte Carlo methods for neural networks / N de Freitas [and others] -- Sequential estimation of signals under model uncertainty / Petar M. Djuric -- Particle filters for mobile robot localization / Dieter Fox [and others] -- Self-organizing time series model / Tomoyuki Higuchi -- Sampling in factored dynamic systems / Daphne Koller and Uri Lerner -- In-situ ellipsometry solutions using sequential Monte Carlo / Alan D. Marrs -- Manoeuvring target tracking using a multiple-model bootstrap filter / Shaun McGinnity and George W. Irwin -- Rao-Blackwellised particle filtering for dynamic Bayesian networks / Kevin Murphy and Stuart Russell -- Particles and mixtures for tracking and guidance / David Salmond and Neil Gordon -- Monte Carlo techniques for automated target recognition / Anuj Srivastava [and others].},
author = {Doucet, Arnaud. and {De Freitas}, Nando. and Gordon, Neil},
isbn = {9781441928870},
pages = {581},
publisher = {Springer},
title = {{Sequential Monte Carlo methods in practice}},
year = {2001}
}
@article{Duane1987,
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
author = {Duane, Simon and Kennedy, A.D. and Pendleton, Brian J. and Roweth, Duncan},
doi = {10.1016/0370-2693(87)91197-X},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duane et al. - 1987 - Hybrid Monte Carlo.pdf:pdf},
issn = {0370-2693},
journal = {Physics Letters B},
number = {2},
pages = {216--222},
publisher = {North-Holland},
title = {{Hybrid Monte Carlo}},
url = {https://www.sciencedirect.com/science/article/pii/037026938791197X?via{\%}3Dihub},
volume = {195},
year = {1987}
}
@article{Gelman1992,
abstract = {The Gibbs sampler, Metropolis' algorithm, and simi-lar iterative simulation methods are related to rejection sampling and importance sampling, two methods which have been traditionally thought of as non-iterative. We explore connections between importance sampling, iter-ative simulation, and importance-weighted resampling (SIR), and present new algorithms that combine aspects of importance sampling, Metropolis' algorithm, and the Gibbs sampler.},
author = {Gelman, Andrew},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 1992 - Iterative and Non-Iterative Simulation Algorithms.pdf:pdf},
journal = {Computing Science and Statistics (Interface Proceedings)},
pages = {433--438},
title = {{Iterative and non-iterative simulation algorithms}},
url = {http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/347.pdf},
volume = {24},
year = {1992}
}
@book{Gelman2013,
abstract = {Third edition. "Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"-- Part I: Fundamentals of Bayesian inference. Probability and inference -- Single-parameter models -- Introduction to multiparameter models -- Asymptotics and connections to non-Bayesian approaches -- Hierarchical models -- Part II: Fundamentals of Bayesian data analysis. Model checking -- Evaluating, comparing, and expanding models -- Modeling accounting for data collection -- Decision analysis -- Part III: Advanced computation. Introduction to Bayesian computation -- Basics of Markov chain simulation -- Computationally efficient Markov chain simulation -- Modal and distributional approximations -- Part IV: Regression models. Introduction to regression models -- Hierarchical linear models -- Generalized linear models -- Models for robust inference -- Models for missing data -- Part V: Nonlinear and nonparametric models. Parametric nonlinear models -- Basis function models -- Gaussian process models -- Finite mixture models -- Dirichlet process models -- A. Standard probability distributions -- B. Outline of proofs of limit theorems -- Computation in R and Stan.},
address = {London},
annote = {Comprehensive reference resource for Bayesian data analysis methodologies.},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
edition = {3rd},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis.pdf:pdf;:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2013 - Bayesian data analysis(2).pdf:pdf},
isbn = {9781439840962},
pages = {675},
publisher = {CRC Press},
title = {{Bayesian data analysis}},
year = {2013}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, Stuart and Geman, Donald},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geman, Geman - 1984 - IEEE Transactions on Pattern Analysis and Machine Intelligence.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {721--741},
title = {{Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images}},
url = {http://www.dam.brown.edu/people/documents/stochasticrelaxation.pdf},
volume = {6},
year = {1984}
}
@article{Goldstein2009,
abstract = {We describe an approach, termed reified analysis, for linking the behaviour of mathematical models with inferences about the physical systems which the models represent. We describe the logical basis for the approach, based on coherent assessment of the implications of deficiencies in the mathematical model. We show how the statistical analysis may be carried out by specifying stochastic relationships between the model that we have, improved versions of the model that we might construct, and the system itself. We illustrate our approach with an example concerning the potential shutdown of the Thermohaline circulation in the Atlantic Ocean.},
annote = {To read},
author = {Goldstein, Michael and Rougier, Jonathan},
doi = {10.1016/J.JSPI.2008.07.019},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldstein, Rougier - 2009 - Reified Bayesian modelling and inference for physical systems(2).pdf:pdf},
issn = {0378-3758},
journal = {Journal of Statistical Planning and Inference},
number = {3},
pages = {1221--1239},
publisher = {North-Holland},
title = {{Reified Bayesian modelling and inference for physical systems}},
url = {https://www.sciencedirect.com/science/article/pii/S0378375808003303},
volume = {139},
year = {2009}
}
@article{Gramacy2012,
annote = {r2018-06-18
The paper makes a persuasive case for the inclusion of the nugget in building GP emulators. The argument is not based on computational stability or non-deterministic computer code, though both of those issues adduce in their favor. Instead, they argue, including a GP simply allows for better fits than interpolating models do. The nugget smooths out the GP in a way that is typically beneficial for the emulator.},
author = {Gramacy, Robert B. and Lee, Herbert K. H.},
doi = {10.1007/s11222-010-9224-x},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gramacy, Lee - 2012 - Cases for the nugget in modeling computer experiments.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
number = {3},
pages = {713--722},
publisher = {Springer US},
title = {{Cases for the nugget in modeling computer experiments}},
url = {http://link.springer.com/10.1007/s11222-010-9224-x},
volume = {22},
year = {2012}
}
@article{Gramacy2008,
abstract = {Motivated by a computer experiment for the design of a rocket booster, this article explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details that make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, we show that our approach is effective in other arenas as well.},
annote = {From Duplicate 2 (Bayesian Treed Gaussian Process Models With an Application to Computer Modeling - Gramacy, Robert B; Lee, Herbert K. H)

r2018-04-23
Looks like a great method for dealing with nonstationarity, which also can massively cut back on computational costs. Since you're only fitting a GP in each node of the tree, you're inverting much smaller matrices than you would be otherwise. A lot of this paper focuses on the details of implementation, and the authors have an R package (tgp).},
author = {Gramacy, Robert B and Lee, Herbert K. H},
doi = {10.1198/016214508000000689},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gramacy, Lee - 2008 - Bayesian Treed Gaussian Process Models With an Application to Computer Modeling.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer simulator,Heteroscedasticity,Nonparametric regression,Nonstationary spatial model,Recursive partitioning},
number = {483},
pages = {1119--1130},
publisher = {Taylor {\&} Francis},
title = {{Bayesian treed Gaussian process models with an application to computer modeling}},
url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000689},
volume = {103},
year = {2008}
}
@article{Haario2006,
annote = {r2018-05-25
This is a great paper. Delayed rejection and adaptive Metropolis sampling are both clearly described, as is the DRAM combination of the two. The explanation has theoretical depth but also is clear enough to suffice as a tutorial. Delayed rejection successfully deals with proposals that are too large, and AM deals with proposals that are too small. The combination seems very effective. DR is Markovian but AM is not; DRAM is not either. But like AM, it is ergodic under mild conditions: the target is bounded with bounded support.},
author = {Haario, Heikki and Laine, Marko and Mira, Antonietta and Saksman, Eero},
doi = {10.1007/s11222-006-9438-0},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario et al. - 2006 - DRAM Efficient adaptive MCMC.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
number = {4},
pages = {339--354},
publisher = {Kluwer Academic Publishers},
title = {{DRAM: Efficient adaptive MCMC}},
url = {http://link.springer.com/10.1007/s11222-006-9438-0},
volume = {16},
year = {2006}
}
@article{Haario2001,
abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis±Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis±Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
annote = {r2018-05-21
The authors make the case for -- essentially -- using the sample covariance of all previous draws as the Gaussian proposal covariance in MCMC. However, it's not MCMC, because it's now Markovian. But according to them under mild conditions it retains all desirable ergodic properties. Seems too good to be true, almost. This paper is an improvement over their 1999 paper, in which they use only a finite number of previous draws.},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.2307/3318737},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario, Saksman, Tamminen - 2001 - An adaptive Metropolis algorithm.pdf:pdf},
issn = {13507265},
journal = {Bernoulli},
number = {2},
pages = {223--242},
title = {{An adaptive Metropolis algorithm}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.bj/1080222083 http://www.jstor.org/stable/3318737?origin=crossref},
volume = {7},
year = {2001}
}
@article{Haario2005,
annote = {r2018-05-23
The authors extend their 2001 work to focus on high-dimensional cases. The extension is pretty basic: you just apply their AM algorithm componentwise. As in their earlier work, they refer to a scaling factor of 2.4 for the adaptive covariance matrix, and they cite Gelman 1996 for that.
There is an interesting paragraph at the end of section 2 where they talk about rotating the proposal distribution during the burn-in period to improve mixing. The process is well-explained for such a brief little paragraph. I found it helpful.
In reading the paper I found myself wondering: would it be possible to apply a Haario-style adaptive covariance approach to adaptively learn a Gaussian mixture proposal? Perhaps including learning how many Gaussian distributions are in the mixture? That could be very powerful for dealing with multimodality. Just a thought.},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.1007/BF02789703},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haario, Saksman, Tamminen - 2005 - Componentwise adaptation for high dimensional MCMC(2).pdf:pdf},
issn = {0943-4062},
journal = {Computational Statistics},
number = {2},
pages = {265--273},
publisher = {Springer-Verlag},
title = {{Componentwise adaptation for high dimensional MCMC}},
url = {http://link.springer.com/10.1007/BF02789703},
volume = {20},
year = {2005}
}
@article{Han2009,
abstract = {This article introduces a Bayesian methodology for the prediction for computer experiments having quantitative and qualitative inputs. The proposed model is a hierarchical Bayesian model with conditional Gaussian stochastic process components. For each of the qualitative inputs, our model assumes that the outputs corresponding to different levels of the qualitative input have “similar” functional behavior in the quantitative inputs. The predictive accuracy of this method is compared with the predictive accuracies of alternative proposals in examples. The method is illustrated in a biomechanical engineering application.},
annote = {To read},
author = {Han, Gang and Santner, Thomas J. and Notz, William I. and Bartel, Donald L.},
doi = {10.1198/tech.2009.07132},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2009 - Prediction for Computer Experiments Having Quantitative and Qualitative Input Variables.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Gaussian stochastic process model,Hierarchical Bayesian model,Product Gaussian correlation,Root mean squared prediction error},
number = {3},
pages = {278--288},
publisher = {Taylor {\&} Francis},
title = {{Prediction for computer experiments having quantitative and qualitative input variables}},
url = {http://www.tandfonline.com/doi/abs/10.1198/tech.2009.07132},
volume = {51},
year = {2009}
}
@article{Han2009a,
abstract = {Tuning and calibration are processes for improving the representativeness of a computer simulation code to a physical phenomenon. This article introduces a statistical methodology for simultaneously determining tuning and calibration parameters in settings where data are available from a computer code and the associated physical experiment. Tuning parameters are set by minimizing a discrepancy measure while the distribution of the calibration parameters are determined based on a hierarchical Bayesian model. The proposed Bayesian model views the output as a realization of a Gaussian stochastic process with hyper-priors. Draws from the resulting posterior distribution are obtained by the Markov chain Monte Carlo simulation. Our methodology is compared with an alternative approach in examples and is illustrated in a biomechanical engineering application. Supplemental materials, including the software and a user manual, are available online and can be requested from the first author.},
annote = {r2018-06-21
Nice article. The authors offer a clear algorithm for undertaking tuning and calibration together. They use an L2 discrepancy.},
author = {Han, Gang and Santner, Thomas J. and Rawlinson, Jeremy J.},
doi = {10.1198/TECH.2009.08126},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Han, Santner, Rawlinson - 2009 - Simultaneous Determination of Tuning and Calibration Parameters for Computer Experiments.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Gaussian stochastic process model,Hierarchical Bayesian model,Kriging},
number = {4},
pages = {464--474},
publisher = {Taylor {\&} Francis},
title = {{Simultaneous determination of tuning and calibration parameters for computer experiments}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2009.08126},
volume = {51},
year = {2009}
}
@article{Harari2017,
abstract = {We revisit the problem of determining the sample size for a Gaussian process emulator and provide a data analytic tool for exact sample size calculations that goes beyond the n = 10d rule of thumb and is based on an IMSPE-related criterion. This allows us to tie sample size and prediction accuracy to the anticipated roughness of the simulated data, and to propose an experimental process for computer experiments, with extension to a robust scheme.},
annote = {r2017-07-10
Who would have thought the n=10d rule would turn out to be an oversimplification? Here's an IMPSE-based alternative, along with a good takedown of the 10d rule.},
author = {Harari, Ofir and Bingham, Derek and Dean, Angela and Higdon, David and Author, Corresponding and Higdon, Dave},
doi = {10.5705/ss.202016.0217},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harari et al. - 2017 - Preprint Computer Experiments Prediction Accuracy, Sample Size and Model Complexity Revisited Complete List of Au.pdf:pdf},
journal = {Statistica Sinica},
keywords = {Preprint},
mendeley-tags = {Preprint},
title = {{Computer experiments: Prediction accuracy, sample size and model complexity revisited}},
url = {http://www.stat.sinica.edu.tw/statistica/},
year = {2017}
}
@article{Hastings1970,
abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
author = {Hastings, W.K.},
journal = {Biometrika},
number = {1},
pages = {97--109},
title = {{Monte Carlo sampling methods using Markov chains and their applications}},
url = {https://academic.oup.com/biomet/article-abstract/57/1/97/2721936},
volume = {57},
year = {1970}
}
@article{Heaton2017,
abstract = {The Gaussian process is an indispensable tool for spatial data analysts. The onset of the "big data" era, however, has lead to the traditional Gaussian process being computationally infeasible for modern spatial data. As such, various alternatives to the full Gaussian process that are more amenable to handling big spatial data have been proposed. These modern methods often exploit low rank structures and/or multi-core and multi-threaded computing environments to facilitate computation. This study provides, first, an introductory overview of several methods for analyzing large spatial data. Second, this study describes the results of a predictive competition among the described methods as implemented by different groups with strong expertise in the methodology. Specifically, each research group was provided with two training datasets (one simulated and one observed) along with a set of prediction locations. Each group then wrote their own implementation of their method to produce predictions at the given location and each which was subsequently run on a common computing environment. The methods were then compared in terms of various predictive diagnostics. Supplementary materials regarding implementation details of the methods and code are available for this article online.},
annote = {r2018-01-09
Very nice overview of methods for analyzing large spatial data, both established and new innovations, and good comparison of them with code available. The article is not so helpful for understanding these methods, though the references here are a valuable collection toward that end.},
archivePrefix = {arXiv},
arxivId = {1710.05013},
author = {Heaton, Matthew J. and Datta, Abhirup and Finley, Andrew and Furrer, Reinhard and Guhaniyogi, Rajarshi and Gerber, Florian and Gramacy, Robert B. and Hammerling, Dorit and Katzfuss, Matthias and Lindgren, Finn and Nychka, Douglas W. and Sun, Furong and Zammit-Mangion, Andrew},
eprint = {1710.05013},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heaton et al. - 2017 - Methods for Analyzing Large Spatial Data A Review and Comparison.pdf:pdf},
title = {{Methods for analyzing large spatial data: A review and comparison}},
url = {http://arxiv.org/abs/1710.05013},
year = {2017}
}
@article{Higdon2008a,
abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis repre...},
annote = {r2017-03-30; 2018-05-18
The authors' approach is based on Kennedy and O'Hagan 2001; latter use GP models. The current authors extend it to allow for highly multivariate output, keeping computational costs low enough for MCMC to be workable. High dimensionality for them still means less than 100. More than that is "beyond the scope of the approach given here". Although K{\&}O'H's approach was not "fully Bayesian", the current authors give it a "fully Bayesian" description, which better matches their own extension.
The authors use principal components to build an emulator out of a sum of orthogonal basis vectors each multiplied times a GP. There is a lot of mathematical detail given here.},
author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
doi = {10.1198/016214507000000888},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2008 - Computer Model Calibration Using High-Dimensional Output.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Computer experiments,Functional data analysis,Gaussian process,Prediction,Predictive science,Uncertainty quantification},
number = {482},
pages = {570--583},
publisher = {Taylor {\&} Francis},
title = {{Computer model calibration using high-dimensional output}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214507000000888},
volume = {103},
year = {2008}
}
@article{Higdon2004,
abstract = {We develop a statistical approach for characterizing uncertainty in predictions that are made with the aid of a computer simulation model. Typically, the computer simulation code models a physical system and requires a set of inputs---some known and specified, others unknown. A limited amount of field data from the true physical system is available to inform us about the unknown inputs and also to inform us about the uncertainty that is associated with a simulation-based prediction. The approach given here allows for the following:uncertainty regarding model inputs (i.e., calibration); accounting for uncertainty due to limitations on the number of simulations that can be carried out; discrepancy between the simulation code and the actual physical system; uncertainty in the observation process that yields the actual field data on the true physical system. The resulting analysis yields predictions and their associated uncertainties while accounting for multiple sources of uncertainty. We use a Bayesian form...},
annote = {r2018-01-15
Found this extremely helpful. There's a lot of overlap with the 2007 flyer plate paper, but here it is, in my opinion, explained more clearly and directly. After reading this, the flyer plate paper makes more sense to me. Anyway, it is mostly helpful in just laying out clearly the basics of the kind of analysis they are doing in the 2007 paper, and the kind we are doing in NSF-DEMS.},
author = {Higdon, Dave and Kennedy, Marc and Cavendish, James C. and Cafeo, John A. and Ryne, Robert D.},
doi = {10.1137/S1064827503426693},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Higdon et al. - 2004 - Combining Field Data and Computer Simulations for Calibration and Prediction.pdf:pdf},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {60G15,62F15,62M30,62P30,62P35,Gaussian process,calibration,computer experiments,model validation,predictability,simulator science,uncertainty quantification},
number = {2},
pages = {448--466},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Combining field data and computer simulations for calibration and prediction}},
url = {http://epubs.siam.org/doi/10.1137/S1064827503426693},
volume = {26},
year = {2004}
}
@article{Joseph2015,
abstract = {Engineering model development involves several simplifying assumptions for the purpose of mathematical tractability, which are often not realistic in practice. This leads to discrepancies in the model predictions. A commonly used statistical approach to overcome this problem is to build a statistical model for the discrepancies between the engineering model and observed data. In contrast, an engineering approach would be to find the causes of discrepancy and fix the engineering model using first principles. However, the engineering approach is time consuming, whereas the statistical approach is fast. The drawback of the statistical approach is that it treats the engineering model as a black box and therefore, the statistically adjusted models lack physical interpretability. This article proposes a new framework for model calibration and statistical adjustment. It tries to open up the black box using simple main effects analysis and graphical plots and introduces statistical models inside the engineering m...},
annote = {To read},
author = {Joseph, V. Roshan and Yan, Huan},
doi = {10.1080/00401706.2014.902773},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joseph, Yan - 2015 - Engineering-Driven Statistical Adjustment and Calibration.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiments,Functional ANOVA,Gaussian process,Nonlinear regression.},
number = {2},
pages = {257--267},
publisher = {Taylor {\&} Francis},
title = {{Engineering-driven statistical adjustment and calibration}},
url = {http://www.tandfonline.com/doi/full/10.1080/00401706.2014.902773},
volume = {57},
year = {2015}
}
@article{Karagiannis2017,
abstract = {Computer models, aiming at simulating a complex real system, are often calibrated in the light of data to improve performance. Standard calibration methods assume that the optimal values of calibration parameters are invariant to the model inputs. In several real world applications where models involve complex parametrizations whose optimal values depend on the model inputs, such an assumption can be too restrictive and may lead to misleading results. We propose a fully Bayesian methodology that produces input dependent optimal values for the calibration parameters, as well as it characterizes the associated uncertainties via posterior distributions. Central to methodology is the idea of formulating the calibration parameter as a step function whose uncertain structure is modeled properly via a binary treed process. Our method is particularly suitable to address problems where the computer model requires the selection of a sub-model from a set of competing ones, but the choice of the ‘best' sub-model may change with the input values. The method produces a selection probability for each sub-model given the input. We propose suitable reversible jump operations to facilitate the challenging computations. We assess the performance of our method against benchmark examples, and use it to analyze a real world application with a large-scale climate model.},
annote = {2018-01-30
Similar to state-aware, but this seems to focus on discrete input-dependent parameters.},
author = {Karagiannis, Georgios and Konomi, Bledar A. and Lin, Guang},
doi = {10.1016/J.SPASTA.2017.08.002},
file = {:C$\backslash$:/Users/carle/Desktop/1-s2.0-S2211675317301215-main.pdf:pdf},
journal = {Spatial Statistics},
publisher = {Elsevier},
title = {{On the Bayesian calibration of expensive computer models with input dependent parameters}},
url = {https://www.sciencedirect.com/science/article/pii/S2211675317301215},
year = {2017}
}
@article{Kennedy2006,
abstract = {In this paper we present a number of recent applications in which an emulator of a computer code is created using a Gaussian process model. Tools are then applied to the emulator to perform sensitivity analysis and uncertainty analysis. Sensitivity analysis is used both as an aid to model improvement and as a guide to how much the output uncertainty might be reduced by learning about specific inputs. Uncertainty analysis allows us to reflect output uncertainty due to unknown input parameters, when the finished code is used for prediction. The computer codes themselves are currently being developed within the UK Centre for Terrestrial Carbon Dynamics.},
annote = {r2018-04-17
There is some very brief discussion of uncertainty analysis and sensitivity analysis using GP emulators. Three examples are presented. The paper does not seem to me to illuminate much that is not more clearly presented by Kennedy and O'Hagan elsewhere.},
author = {Kennedy, Marc C. and Anderson, Clive W. and Conti, Stefano and O'Hagan, Anthony},
doi = {10.1016/J.RESS.2005.11.028},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy et al. - 2006 - Case studies in Gaussian process modelling of computer codes.pdf:pdf},
issn = {0951-8320},
journal = {Reliability Engineering {\&} System Safety},
number = {10-11},
pages = {1301--1309},
publisher = {Elsevier},
title = {{Case studies in Gaussian process modelling of computer codes}},
url = {https://www.sciencedirect.com/science/article/pii/S0951832005002395},
volume = {91},
year = {2006}
}
@article{Kennedy2001,
annote = {r2018-01-11; 2016-05-30
Very thorough and detailed development of the foundation of Bayesian calibration of computer models. Useful walkthrough of that foundation.

2018-04-16
The authors assert that theirs is the first attempt to model all the sources of uncertainty in calibration of computer models. This despite the fact that theirs is not a full Bayesian analysis; they use MLEs for some hyperparameters. The authors are explicit in considering calibration to be a matter of finding ``best-fitting" values for calibration parameters; that is, calibration is for them hopelessly confounded with minimizing model inadequacy. (For the same reason, they even suggest performing model calibration on parameters whose true physical value is actually known.) In the examples considered by the authors, they integrate by crude quadrature (rather than MCMC).},
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy, O'Hagan - 2001 - Bayesian calibration of computer models(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
number = {3},
pages = {425--464},
publisher = {Blackwell Publishers Ltd.},
title = {{Bayesian calibration of computer models}},
url = {http://doi.wiley.com/10.1111/1467-9868.00294},
volume = {63},
year = {2001}
}
@article{Ling2014,
abstract = {In the Kennedy and O'Hagan framework for Bayesian calibration of physics models, selection of an appropriate prior form for the model discrepancy function is a challenging issue due to the lack of physics knowledge regarding model inadequacy. Aiming to address the uncertainty arising from the selection of a particular prior, this paper first conducts a study on possible formulations of the model discrepancy function. A first-order Taylor series expansion-based method is developed to investigate the potential redundancy caused by adding a discrepancy function to the original physics model. Further, we propose a three-step (calibration, validation, and combination) approach in order to inform the decision on the construction of model discrepancy priors. In the validation step, a reliability-based metric is used to evaluate posterior model predictions in the validation domain. The validation metric serves as a quantitative measure of how well the discrepancy formulation captures the missing physics in the model. In the combination step, the posterior distributions of model parameters and discrepancy corresponding to different priors are combined into a single distribution based on the probabilistic weights derived from the validation step. The combined distribution acknowledges the uncertainty in the prior formulation of model discrepancy function.},
author = {Ling, You and Mullins, Joshua and Mahadevan, Sankaran},
doi = {10.1016/J.JCP.2014.08.005},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling, Mullins, Mahadevan - 2014 - Selection of model discrepancy priors in Bayesian calibration.pdf:pdf},
issn = {0021-9991},
journal = {Journal of Computational Physics},
pages = {665--680},
publisher = {Academic Press},
title = {{Selection of model discrepancy priors in Bayesian calibration}},
url = {https://www.sciencedirect.com/science/article/pii/S0021999114005518?via{\%}3Dihub},
volume = {276},
year = {2014}
}
@article{Liu2009,
annote = {r2018-01-09
"Modularizing", ie keeping separate the distinct components of a Bayesian model, can be a useful way to fudge the analysis to keep poor components of the model from ruining the whole thing. The paper is particularly helpful in the treatment of discrepancy in a model. Useful references are given on pages 123 and 127; the latter gives four papers responsible for the "introduction" of Gaussian process response-surface methodology (GASP) for constructing emulators.},
author = {Liu, F. and Bayarri, Maria J. and Berger, J. O.},
doi = {10.1214/09-BA404},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Bayarri, Berger - 2009 - Modularization in Bayesian analysis, with emphasis on analysis of computer models(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Complex computer models,Confounding,Emulators,Identifiability,MCMC mixing,Partial likelihood,Random effects},
number = {1},
pages = {119--150},
publisher = {International Society for Bayesian Analysis},
title = {{Modularization in Bayesian analysis, with emphasis on analysis of computer models}},
url = {http://projecteuclid.org/euclid.ba/1340370392},
volume = {4},
year = {2009}
}
@article{Liu2008,
abstract = {A major question for the application of computer models is Does the computer model adequately represent reality? Viewing the computer models as a potentially biased representation of reality, Bayarri et al. [M. Bayarri, J. Berger, R. Paulo, J. Sacks, J. Cafeo, J. Cavendish, C. Lin, J. Tu, A framework for validation of computer models, Technometrics 49 (2) (2007) 138–154] develop the simulator assessment and validation engine (SAVE) method as a general framework for answering this question. In this paper, we apply the SAVE method to the challenge problem which involves a thermal computer model designed for certain devices. We develop a statement of confidence that the devices modeled can be applied in intended situations.},
annote = {This paper is essentially a case study in applying the methodology they promulgate in their 2007 paper "A framework for validation of computer models". They call this model the SAVE method: simulator assessment and validation engine.},
author = {Liu, F. and Bayarri, Maria J. and Berger, J.O. and Paulo, R. and Sacks, J.},
doi = {10.1016/J.CMA.2007.05.032},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2008 - A Bayesian analysis of the thermal challenge problem.pdf:pdf},
issn = {0045-7825},
journal = {Computer Methods in Applied Mechanics and Engineering},
number = {29-32},
pages = {2457--2466},
publisher = {North-Holland},
title = {{A Bayesian analysis of the thermal challenge problem}},
url = {https://www.sciencedirect.com/science/article/pii/S0045782507005075},
volume = {197},
year = {2008}
}
@book{Liu2001a,
abstract = {Preface -- 1. Introduction and examples -- 2. Basic principles : rejection, weighting, and others -- 3. Theory of sequential Monte Carlo -- 4. Sequential Monte Carlo in action -- 5. Metropolis algorithm and beyond -- 6. The Gibbs sampler -- 7. Cluster algorithms for the Ising model -- 8. General conditional sampling -- 9. Molecular dynamics and hybrid Monte Carlo -- 10. Multilevel sampling and optimization methods -- 11. Population-based Monte Carlo methods -- 12. Markov chains and their convergence -- 13. Selected theoretical topics -- A. Basics in probability and statistics -- References -- Author index -- Subject index.},
author = {Liu, Jun S.},
isbn = {0387763694},
pages = {343},
publisher = {Springer},
title = {{Monte Carlo strategies in scientific computing}},
year = {2001}
}
@book{Loeppky2006,
abstract = {Computer models to simulate physical phenomena are now widely available in engineering and science. Before relying on a computer model, a natural first step is often to compare its output with physical or field data, to assess whether the computer model reliably represents the real world. Field data, when available, can also be used to calibrate or tune unknown parameters in the computer model. Calibration is particularly problematic in the presence of systematic discrepancies between the computer model and field observations. We introduce a likelihood alternative to previous Bayesian methodology for estimation of calibration or tuning parameters. In an important special case, we show that maximum likelihood estimation will asymptotically find values of the calibration parameter that give an unbiased computer model, if such a model exists. However, the calibration parameters are not necessarily estimable. We also show in some settings that calibration or tuning need to take into account the end-use prediction strategy. Depending on the strategy, the choice of the parameter values may be critical or unimportant.},
address = {University of British Columbia},
annote = {r2018-04-06
Very clearly written. The authors show that MLE calibration is asymptotically unbiased -- not in the sense that the calibration parameter estimate is unbiased, but in the sense that the calibration parameter estimate provides that the discrepancy function is 0; also this asymptotically the case in that it holds if you can draw indefinitely many times from the simulation.},
author = {Loeppky, Jason L. and Bingham, Derek and Welch, William J.},
booktitle = {Technical Report},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Loeppky, Bingham, Welch - 2006 - Computer model calibration or tuning in practice.pdf:pdf},
mendeley-groups = {Calibration},
publisher = {Department of Statistics},
title = {{Computer model calibration or tuning in practice}},
url = {https://www.researchgate.net/profile/Jason{\_}Loeppky/publication/228936502{\_}Computer{\_}model{\_}calibration{\_}or{\_}tuning{\_}in{\_}practice/links/0c960525da9e07f2d1000000.pdf},
year = {2006}
}F
@article{McCall2005,
abstract = {Genetic algorithms (GAs) are a heuristic search and optimisation technique inspired by natural evolution. They have been successfully applied to a wide range of real-world problems of significant complexity. This paper is intended as an introduction to GAs aimed at immunologists and mathematicians interested in immunology. We describe how to construct a GA and the main strands of GA theory before speculatively identifying possible applications of GAs to the study of immunology. An illustrative example of using a GA for a medical optimal control problem is provided. The paper also includes a brief account of the related area of artificial immune systems.},
annote = {r2018-05-10
The notion of genetic algorithms is described in an introductory way. There is some very slight discussion of the theoretical background of genetic algorithms. As a tutorial it is helpful. Specific toolboxes (e.g. in MATLAB) are recommended.},
author = {McCall, John},
doi = {10.1016/J.CAM.2004.07.034},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCall - 2005 - Genetic algorithms for modelling and optimisation.pdf:pdf},
issn = {0377-0427},
journal = {Journal of Computational and Applied Mathematics},
number = {1},
pages = {205--222},
publisher = {North-Holland},
title = {{Genetic algorithms for modelling and optimisation}},
url = {https://www.sciencedirect.com/science/article/pii/S0377042705000774},
volume = {184},
year = {2005}
}
@article{McKay1979,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
annote = {Latin hypercube sampling is introduced and discussed.},
author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
doi = {10.1080/00401706.1979.10489755},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McKay, Beckman, Conover - 1979 - Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Co.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
number = {2},
pages = {239--245},
publisher = {Taylor {\&} Francis Group},
title = {{Comparison of three methods for selecting values of input variables in the analysis of output from a computer code}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1979.10489755},
volume = {21},
year = {1979}
}
@article{Metropolis1953,
abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
doi = {10.1063/1.1699114},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
number = {6},
pages = {1087--1092},
publisher = {American Institute of PhysicsAIP},
title = {{Equation of state calculations by fast computing machines}},
url = {http://aip.scitation.org/doi/10.1063/1.1699114},
volume = {21},
year = {1953}
}
@article{Mori1973,
abstract = {Having noted an important role of image stress in work hardening of dispersion hardened materials, (1,3) the present paper discusses a method of calculating the average internal stress in the matrix of a material containing inclusions with transformation strain. It is shown that the average stress in the matrix is uniform throughout the material and independent of the position of the domain where the average treatment is carried out. It is also shown that the actual stress in the matrix is the average stress plus the locally fluctuating stress, the average of which vanishes in the matrix. Average elastic energy is also considered by taking into account the effects of the interaction among the inclusions and of the presence of the free boundary.},
annote = {The Mori-Tanaka model is introduced and discussed.},
author = {Mori, T and Tanaka, K},
doi = {10.1016/0001-6160(73)90064-3},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mori, Tanaka - 1973 - Average stress in matrix and average elastic energy of materials with misfitting inclusions.pdf:pdf},
issn = {0001-6160},
journal = {Acta Metallurgica},
number = {5},
pages = {571--574},
publisher = {Pergamon},
title = {{Average stress in matrix and average elastic energy of materials with misfitting inclusions}},
url = {https://www.sciencedirect.com/science/article/pii/0001616073900643?via{\%}3Dihub},
volume = {21},
year = {1973}
}
@article{Muehlenstaedt2017,
annote = {r2017-07-10
The authors propose an appropriate distance metric and demonstrate its use in creating LHC designs for experiments with functional input. I wonder how much its utility depends on the degree of knowledge we have about the shape of the functional input.},
author = {Muehlenstaedt, Thomas and Fruth, Jana and Roustant, Olivier},
doi = {10.1007/s11222-016-9672-z},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muehlenstaedt, Fruth, Roustant - 2017 - Computer experiments with functional inputs and scalar outputs by a norm-based approach.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
number = {4},
pages = {1083--1097},
publisher = {Springer US},
title = {{Computer experiments with functional inputs and scalar outputs by a norm-based approach}},
url = {http://link.springer.com/10.1007/s11222-016-9672-z},
volume = {27},
year = {2017}
}
@incollection{Neal2011,
abstract = {Markov chain Monte Carlo (MCMC) originated with the classic paper of Metropolis et al. (1953), where it was used to simulate the distribution of states for a system of idealized molecules. Not long after, another approach to molecular simulation was introduced (Alder and Wainwright, 1959), in which the motion of the molecules was deterministic, following Newton's laws of motion, which have an elegant formalization as Hamiltonian dynamics. For finding the properties of bulk materials, these approaches are asymptotically equivalent, since even in a deterministic simulation, each local region of the material experiences effectively random influences from distant regions. Despite the large overlap in their appli-cation areas, the MCMC and molecular dynamics approaches have continued to coexist in the following decades (see Frenkel and Smit, 1996). In 1987, a landmark paper by Duane, Kennedy, Pendleton, and Roweth united the MCMC and molecular dynamics approaches. They called their method " hybrid Monte Carlo, " which abbreviates to " HMC, " but the phrase " Hamiltonian Monte Carlo, " retain-ing the abbreviation, is more specific and descriptive, and I will use it here. Duane et al. applied HMC not to molecular simulation, but to lattice field theory simulations of quan-tum chromodynamics. Statistical applications of HMC began with my use of it for neural network models (Neal, 1996a). I also provided a statistically-oriented tutorial on HMC in a review of MCMC methods (Neal, 1993, Chapter 5). There have been other applications of HMC to statistical problems (e.g. Ishwaran, 1999; Schmidt, 2009) and statistically-oriented reviews (e.g. Liu, 2001, Chapter 9), but HMC still seems to be underappreciated by statisticians, and perhaps also by physicists outside the lattice field theory community. This review begins by describing Hamiltonian dynamics. Despite terminology that may be unfamiliar outside physics, the features of Hamiltonian dynamics that are needed for HMC are elementary. The differential equations of Hamiltonian dynamics must be dis-cretized for computer implementation. The " leapfrog " scheme that is typically used is quite simple. Following this introduction to Hamiltonian dynamics, I describe how to use it to con-struct an MCMC method. The first step is to define a Hamiltonian function in terms of the probability distribution we wish to sample from. In addition to the variables we are inter-ested in (the " position " variables), we must introduce auxiliary " momentum " variables, which typically have independent Gaussian distributions. The HMC method alternates simple updates for these momentum variables with Metropolis updates in which a new state is proposed by computing a trajectory according to Hamiltonian dynamics, imple-mented with the leapfrog method. A state proposed in this way can be distant from the 113 114 Handbook of Markov Chain Monte Carlo current state but nevertheless have a high probability of acceptance. This bypasses the slow exploration of the state space that occurs when Metropolis updates are done using a simple random-walk proposal distribution. (An alternative way of avoiding random walks is to use short trajectories but only partially replace the momentum variables between trajectories, so that successive trajectories tend to move in the same direction.) After presenting the basic HMC method, I discuss practical issues of tuning the leapfrog stepsize and number of leapfrog steps, as well as theoretical results on the scaling of HMC with dimensionality. I then present a number of variations on HMC. The acceptance rate for HMC can be increased for many problems by looking at " windows " of states at the beginning and end of the trajectory. For many statistical problems, approximate computa-tion of trajectories (e.g. using subsets of the data) may be beneficial. Tuning of HMC can be made easier using a " short-cut " in which trajectories computed with a bad choice of stepsize take little computation time. Finally, " tempering " methods may be useful when multiple isolated modes exist.},
address = {New York},
author = {Neal, Radford M},
booktitle = {Handbook of Markov chain Monte Carlo},
chapter = {5},
editor = {Brooks, S. and Gelman, A. and Jones, G.L. and Meng, X.-L.},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2011 - MCMC Using Hamiltonian Dynamics.pdf:pdf},
pages = {113--162},
publisher = {CRC Press},
title = {{MCMC using Hamiltonian dynamics}},
url = {http://www.mcmchandbook.net/HandbookChapter5.pdf},
year = {2011}
}
@incollection{Neal,
abstract = {Gaussian processes are a natural way of specifying prior distributions over functions of one or more input variables. When such a function defines the mean response in a regression model with Gaussian errors, inference can be done using matrix computations, which are feasible for datasets of up to about a thousand cases. The covariance function of the Gaussian process can be given a hierarchical prior, which allows the model to discover high-level properties of the data, such as which inputs are relevant to predicting the response. Inference for these covariance hyperparameters can be done using Markov chain sampling. Classification models can be defined using Gaussian processes for underlying latent values, which can also be sampled within the Markov chain. Gaussian processes are in my view the simplest and most obvious way of defining flexible Bayesian regression and classification models, but despite some past usage, they appear to have been rather neglected as a general-purpose technique. This may be partly due to a confusion between the properties of the function being modeled and the properties of the best predictor for this unknown function. In this paper, I hope to persuade you that Gaussian processes are a fruitful way of defining prior distributions for flexible regression and classification models in which the regression or class probability functions are not limited to simple parametric forms. The basic idea goes back many years in a regression context, but is nevertheless not widely appreciated. The use of general Gaussian process models for classification is more recent, and to my knowledge the work presented here is the first that implements an exact Bayesian approach. One attraction of Gaussian processes is the variety of covariance functions one can choose from, which lead to functions with different degrees of smoothness, or different sorts of additive structure. I will describe some of these possibilities, while also noting the limitations of Gaussian processes. I then discuss computations for Gaussian process models, starting with the basic matrix operations involved. For classification models, one must integrate over the ''latent values'' underlying each case. I show how this can be done using Markov chain Monte Carlo methods. In a full-fledged Bayesian treatment, one must also integrate over the posterior distribution for the hyperparameters of the covariance function, which can also be done using Markov chain sampling. I show how this all works for a synthetic three-way classification problem. The methods I describe are implemented in software that is available from my web page, at http://www.cs.utoronto.ca/radford/. Gaussian processes and related methods have been used in various contexts for many years. Despite this past usage, and despite the fundamental simplicity of the idea, Gaussian process models appear to have been little appreciated by most Bayesians. I speculate that this could be partly due to a confusion between the properties one expects of the true function being modeled and those of the best predictor for this unknown function. Clarity in this respect is particularly important when thinking of flexible models such as those based on Gaussian processes.},
address = {New York},
annote = {r2018-04-05
The paper is a pretty basic introduction to the use of Gaussian processes for regression and classification. There's a nice example near the beginning building the concept of GP regression from a case of linear regression. Correspondingly, there is some discussion of covariate matrices producing linear estimates.},
author = {Neal, Radford M},
booktitle = {Bayesian statistics 6},
editor = {Bernardo, J M and Berger, J O and Dawid, A P and Smith, A F M},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - Unknown - Regression and Classification Using Gaussian Process Priors.pdf:pdf},
pages = {475--501},
publisher = {Oxford University Press},
title = {{Regression and classification using Gaussian process priors}},
url = {http://www.cs.toronto.edu/{~}radford/ftp/val6gp.pdf},
volume = {6},
year = {1998}
}
@article{Neala,
abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
author = {Neal, Radford M.},
doi = {10.2307/3448413},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2003 - Slice Sampling.pdf:pdf},
journal = {The Annals of Statistics},
number = {3},
pages = {705--767},
publisher = {Institute of Mathematical Statistics},
title = {{Slice sampling}},
url = {http://www.jstor.org/stable/3448413},
volume = {31},
year = {2003}
}
@article{Oakley2004,
annote = {r2018-04-20
This is a useful paper for getting a handle on the varieties of traditional sensitivity analysis and references related to them. Its focus, though, and where it is most useful, is its discussion on performing Bayesian global sensitivity analyses of computer models that builds on the framework of Kennedy and O'Hagan 2001. A major benefit is the very small number of model runs required to complete a comprehensive sensitivity analysis.},
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
doi = {10.1111/j.1467-9868.2004.05304.x},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oakley, O'Hagan - 2004 - Probabilistic sensitivity analysis of complex models a Bayesian approach(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {Bayesian inference,Computer model,Gaussian process,Sensitivity analysis,Uncertainty analysis},
number = {3},
pages = {751--769},
publisher = {Blackwell Publishing},
title = {{Probabilistic sensitivity analysis of complex models: a Bayesian approach}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2004.05304.x},
volume = {66},
year = {2004}
}
@article{Oakley2017,
abstract = {We calibrate a stochastic computer simulation model of “moderate” computational expense. The simulator is an imperfect representation of reality, and we recognize this discrepancy to ensure a reliable calibration. The calibration model combines a Gaussian process emulator of the likelihood surface with importance sampling. Changing the discrepancy specification changes only the importance weights, which lets us investigate sensitivity to different discrepancy specifications at little computational cost. We present a case study of a natural history model that has been used to characterize UK bowel cancer incidence. Datasets and computer code are provided as supplementary material.},
archivePrefix = {arXiv},
arxivId = {1403.5196},
author = {Oakley, Jeremy E. and Youngman, Benjamin D.},
doi = {10.1080/00401706.2015.1125391},
eprint = {1403.5196},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oakley, Youngman - 2017 - Calibration of Stochastic Computer Simulators Using Likelihood Emulation.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
number = {1},
title = {{Calibration of stochastic computer simulators using likelihood emulation}},
volume = {59},
year = {2017}
}
@article{OHagan1978,
abstract = {The optimal design problem is tackled in the framework of a new model and new objectives. A regression model is proposed in which the regression function is permitted to take any form over the space X of independent variables. The design objective is based on fitting a simplified function for prediction. The approach is Bayesian throughout. The new designs are more robust than conventional ones. They also avoid the need to limit artificially design points to a predetermined subset of X. New solutions are also offered for the problems of smoothing, curve fitting and the selection of regressor variables.},
annote = {r2018-03-16; 2017-04-20
O'Hagan walks the reader through the use of Gaussian process models for smoothing, curve fitting, and prediction. Much time is spent on design; it is the focus of the work. However the fundamental mathematics of GP models is laid out probably as clearly and thoroughly as it is possible to do. This includes the formulae for multivariate GP models, as well as GP models with uninformative prior means.},
author = {O'Hagan, Anthony},
doi = {10.2307/2984861},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan, Kingman - 1978 - Curve Fitting and Optimal Design for Prediction(2).pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B},
number = {1},
pages = {1--42},
publisher = {WileyRoyal Statistical Society},
title = {{Curve fitting and optimal design for prediction}},
url = {http://www.jstor.org/stable/2984861},
volume = {40},
year = {1978}
}
@article{Ohagan2006,
abstract = {The Bayesian approach to quantifying, analysing and reducing uncertainty in the application of complex process models is attracting increasing attention amongst users of such models. The range and power of the Bayesian methods is growing and there is already a sizeable literature on these methods. However, most of it is in specialist statistical journals. The purpose of this tutorial is to introduce the more general reader to the Bayesian approach.},
annote = {r2018-01-11
Helpful tutorial, though very entry-level. But I found his discussion of sensitivity analysis helpful, and also his list of ongoing areas of research related to Bayesian analysis of computer codes.},
author = {O'Hagan, Anthony},
doi = {10.1016/j.ress.2005.11.025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Hagan - 2006 - Bayesian analysis of computer code outputs A tutorial.pdf:pdf},
journal = {Reliability Engineering and System Safety},
keywords = {Bayesian statistics,Calibration,Dimensionality reduction,Emulator,Gaussian process,Roughness,Screening,Sensitivity analysis,Smoothness,Uncertainty analysis,Validation},
pages = {1290--1300},
title = {{Bayesian analysis of computer code outputs: A tutorial}},
url = {https://ac.els-cdn.com/S0951832005002383/1-s2.0-S0951832005002383-main.pdf?{\_}tid=5fff22dc-f649-11e7-b6db-00000aacb35d{\&}acdnat=1515618247{\_}514575f75406f47f4a7989abf17ff2a5},
volume = {91},
year = {2006}
}
@article{Park2008,
abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
author = {Park, Trevor and Casella, George},
doi = {10.1198/016214508000000337},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Casella - 2008 - The Bayesian Lasso.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Empirical Bayes,Gibbs sampler,Hierarchical model,Inverse Gaussian,Linear regression,Penalized regression,Scale mixture of normals},
number = {482},
pages = {681--686},
publisher = {Taylor {\&} Francis},
title = {{The Bayesian Lasso}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214508000000337},
volume = {103},
year = {2008}
}
@article{Paulo2012,
abstract = {The problem of calibrating computer models that produce multivariate output is considered, with a particular emphasis on the situation where the model is computationally demanding. The proposed methodology builds on Gaussian process-based response-surface approximations to each of the components of the output of the computer model to produce an emulator of the multivariate output. This emulator is then combined in a statistical model involving field observations, which is then used to produce calibration strategies for the parameters of the computer model. The results of applying this methodology to a simulated example and to a real application are presented.},
annote = {r2018-04-12
I can't claim to have understood or retained much of this paper. However, the authors do list references discussing the use of a nugget for computational and other reasons when building the covariance of a GP emulator.},
author = {Paulo, Rui and Garc{\'{i}}a-Donato, Gonzalo and Palomo, Jes{\'{u}}s},
doi = {10.1016/j.csda.2012.05.023},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulo, Garc{\'{i}}a-Donato, Palomo - 2012 - Calibration of computer models with multivariate output.pdf:pdf},
journal = {Computational Statistics and Data Analysis},
pages = {3959--3974},
title = {{Calibration of computer models with multivariate output}},
url = {www.elsevier.com/locate/csda},
volume = {56},
year = {2012}
}
@article{Plumlee2017,
abstract = {Bayesian calibration is used to study computer models in the presence of both a calibration parameter and model bias. The parameter in the predominant methodology is left undefined. This results in an issue, where the posterior of the parameter is suboptimally broad. There has been no generally accepted alternatives to date. This article proposes using Bayesian calibration, where the prior distribution on the bias is orthogonal to the gradient of the computer model. Problems associated with Bayesian calibration are shown to be mitigated through analytic results in addition to examples. Supplementary materials for this article are available online.},
author = {Plumlee, Matthew},
doi = {10.1080/01621459.2016.1211016},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plumlee - 2017 - Bayesian Calibration of Inexact Computer Models.pdf:pdf},
isbn = {0162-1459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian processes,Identifiability,Kriging,Model inadequacy,Orthogonal processes,Uncertainty quantification},
number = {519},
pages = {1274--1285},
publisher = {Taylor {\&} Francis},
title = {{Bayesian calibration of inexact computer models}},
url = {https://doi.org/10.1080/01621459.2016.1211016},
volume = {112},
year = {2017}
}
@article{Plumlee2016,
abstract = {Gaussian processes models are widely adopted for nonparameteric/semi-parametric modeling. Identifiability issues occur when the mean model contains polynomials with unknown coefficients. Though resulting prediction is unaffected, this leads to poor estimation of the coefficients in the mean model, and thus the estimated mean model loses interpretability. This paper introduces a new Gaussian process model whose stochastic part is orthogonal to the mean part to address this issue. This paper also discusses applications to multi-fidelity simulations using data examples.},
archivePrefix = {arXiv},
arxivId = {1611.00203},
author = {Plumlee, Matthew and Joseph, V. Roshan},
eprint = {1611.00203},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plumlee, Joseph - 2016 - Orthogonal Gaussian process models.pdf:pdf},
title = {{Orthogonal Gaussian process models}},
url = {http://arxiv.org/abs/1611.00203},
year = {2016}
}
@article{Pratola2016,
abstract = {Complex natural phenomena are increasingly investigated by the use of a complex computer simulator. To leverage the advantages of simulators, observational data need to be incorporated in a probabilistic framework so that uncertainties can be quantified. A popular framework for such experiments is the statistical computer model calibration experiment. A limitation often encountered in current statistical approaches for such experiments is the difficulty in modeling high-dimensional observational datasets and simulator outputs as well as high-dimensional inputs. As the complexity of simulators seems to only grow, this challenge will continue unabated. In this article, we develop a Bayesian statistical calibration approach that is ideally suited for such challenging calibration problems. Our approach leverages recent ideas from Bayesian additive regression Tree models to construct a random basis representation of the simulator outputs and observational data. The approach can flexibly handle high-dimensional...},
author = {Pratola, M. T. and Higdon, D. M.},
doi = {10.1080/00401706.2015.1049749},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pratola, Higdon - 2016 - Bayesian Additive Regression Tree Calibration of Complex High-Dimensional Computer Models.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Catastrophe model,Climate change,Markov chain Monte Carlo,Nonparametric,Treaty verification,Uncertainty quantification},
number = {2},
pages = {166--179},
publisher = {Taylor {\&} Francis},
title = {{Bayesian additive regression tree calibration of complex high-dimensional computer models}},
url = {http://www.tandfonline.com/doi/full/10.1080/00401706.2015.1049749},
volume = {58},
year = {2016}
}
@article{Pratola2018,
abstract = {Inference on large-scale models is of great interest in modern science. Examples include deterministic simulators of fluid dynamics to recover the source of a pollutant, or stochastic agent-based simulators to infer features of consumer behaviour. When computational con-straints prohibit model evaluation at all but a small ensemble of parameter settings, exact in-ference becomes infeasible. In such cases, emulation of the simulator enables the interrogation of a surrogate model at arbitrary parameter values. Combining emulators with observational data to estimate parameters and predict a real-world process is known as computer model calibration. The choice of the emulator model is a critical aspect of calibration. Existing approaches treat the mathematical model as implemented on computer as an unknown but deterministic response surface. However, in many cases the underlying mathematical model, or the simulator approximating the mathematical model, are not determinsitic and in fact have some uncertainty associated with their output. In this paper, we propose a Bayesian statistical calibration model for stochastic simulators. The approach is motivated by two applied problems: a deterministic mathematical model of intra-cellular signalling whose im-plementation on computer nonetheless has discretization uncertainty, and a stochastic model of river water temperature commonly used in hydrology. We show the proposed approach is able to map the uncertainties of such non-deterministic simulators through to the resulting in-ference while retaining computational feasibility. Supplementary computer code and datasets are provided online.},
author = {Pratola, Matthew and Chkrebtii, Oksana},
doi = {10.5705/ss.202016.0403},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pratola, Chkrebtii - 2018 - Bayesian Calibration of Multistate Stochastic Simulators.pdf:pdf},
journal = {Statistica Sinica},
pages = {693--719},
title = {{Bayesian calibration of multistate stochastic simulators}},
url = {http://www.stat.sinica.edu.tw/statistica/},
volume = {28},
year = {2018}
}
@article{Qian2008a,
abstract = {Standard practice when analyzing data from different types of experiments is to treat data from each type separately. By borrowing strength across multiple sources, an integrated analysis can produce bet-ter results. Careful adjustments must be made to incorporate the systematic differences among various experiments. Toward this end, some Bayesian hierarchical Gaussian process models are proposed. The heterogeneity among different sources is accounted for by performing flexible location and scale ad-justments. The approach tends to produce prediction closer to that from the high-accuracy experiment. The Bayesian computations are aided by the use of Markov chain Monte Carlo and sample average ap-proximation algorithms. The proposed method is illustrated with two examples, one with detailed and approximate finite elements simulations for mechanical material design and the other with physical and computer experiments for modeling a food processor.},
annote = {r2017-08-22; 2018-06-15
Clearly stated model for integrating high and low data. Nice conditional distributions are given. The authors explore many tweaks to make the model suit various situations. Their focus is on prediction of the high data. The authors use a GP multiplied by the low data as a scale correction, and add to it a GP for location correction. They note at the end that you could adapt this for calibration.},
author = {Qian, Peter Z G and Jeff, C F and Stewart, Wu H Milton},
doi = {10.1198/004017008000000082},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Jeff, Stewart - Unknown - Bayesian Hierarchical Modeling for Integrating Low-Accuracy and High-Accuracy Experiments.pdf:pdf},
journal = {Technometrics},
keywords = {Computer experiments,Gaussian process,Kriging,Markov chain Monte Carlo,Sto-chastic programming},
number = {2},
pages = {192--204},
title = {{Bayesian hierarchical modeling for integrating low-accuracy and high-accuracy experiments}},
url = {https://www.tandfonline.com/doi/pdf/10.1198/004017008000000082},
volume = {50},
year = {2008}
}
@article{Qian2008,
abstract = {Modeling experiments with qualitative and quantitative factors is an important issue in computer modeling. We propose a framework for building Gaussian process models that incorporate both types of factors. The key to the development of these new models is an approach for constructing correlation functions with qualitative and quantitative factors. An iterative estimation procedure is developed for the proposed models. Modern optimization techniques are used in the estimation to ensure the validity of the constructed correlation functions. The proposed method is illustrated with an example involving a known function and a real example for modeling the thermal distribution of a data center.},
annote = {r2017-07-13
The authors describe and provide theoretical grounding for a framework that allows one to use qualitative input to determine which model to apply to the quantitative input.},
author = {Qian, Peter Z. G and Wu, Huaiqing and Wu, C. F. Jeff},
doi = {10.1198/004017008000000262},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Wu, Wu - 2008 - Gaussian Process Models for Computer Experiments With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Cokriging,Design of experiments,Kriging,Multivariate Gaussian process,Semidefinite programming},
number = {3},
pages = {383--396},
publisher = {Taylor {\&} Francis},
title = {{Gaussian process models for computer experiments with qualitative and quantitative factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017008000000262},
volume = {50},
year = {2008}
}
@article{Ranjan2011,
abstract = {For many expensive deterministic computer simulators, the outputs do not have replication error and the desired metamodel (or statistical emulator) is an interpolator of the observed data. Realizations of Gaussian spatial processes (GP) are commonly used to model such simulator outputs. Fitting a GP model to n data points requires the computation of the inverse and determinant of n×n correlation matrices, R, that are sometimes computationally unstable due to near-singularity of R. This happens if any pair of design points are very close together in the input space. The popular approach to overcome near-singularity is to introduce a small nugget (or jitter) parameter in the model that is estimated along with other model parameters. The inclusion of a nugget in the model often causes unnecessary over-smoothing of the data. In this article, we propose a lower bound on the nugget that minimizes the over-smoothing and an iterative regularization approach to construct a predictor that further improves the inter...},
author = {Ranjan, Pritam and Haynes, Ronald and Karsten, Richard},
doi = {10.1198/TECH.2011.09141},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ranjan, Haynes, Karsten - 2011 - A Computationally Stable Approach to Gaussian Process Interpolation of Deterministic Computer Simulatio.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiment,Matrix inverse approximation,Regularization},
number = {4},
pages = {366--378},
publisher = {Taylor {\&} Francis},
title = {{A computationally stable approach to Gaussian process interpolation of deterministic computer simulation data}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.09141},
volume = {53},
year = {2011}
}
@book{Rasmussen2006,
address = {Cambridge},
author = {Rasmussen, Carl Edward and Williams, Christopher K. I. and Sutton, Richard S and Barto, Andrew G and Spirtes, Peter and Glymour, Clark and Scheines, Richard and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen et al. - 2006 - Gaussian Processes for Machine Learning.pdf:pdf},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning}},
url = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
year = {2006}
}
@article{Reich2011,
abstract = {Numerous studies have linked ambient air pollution and adverse health outcomes. Many studies of this nature relate outdoor pollution levels measured at a few monitoring sta-tions with health outcomes. Recently, computational methods have been developed to model the distribution of personal exposures, rather than ambient concentration, and then relate the exposure distribution to the health outcome. Although these methods show great promise, they are limited by the computational demands of the exposure model. We propose a method to alle-viate these computational burdens with the eventual goal of implementing a national study of the health effects of air pollution exposure. Our approach is to develop a statistical emulator for the exposure model, i.e. we use Bayesian density estimation to predict the conditional exposure distribution as a function of several variables, such as temperature, human activity and physical characteristics of the pollutant. This poses a challenging statistical problem because there are many predictors of the exposure distribution and density estimation is notoriously difficult in high dimensions. To overcome this challenge, we use stochastic search variable selection to identify a subset of the variables that have more than just additive effects on the mean of the exposure distribution. We apply our method to emulate an ozone exposure model in Philadelphia.},
author = {Reich, Brian J and Kalendra, Eric and Storlie, Curtis B and Bondell, Howard D and Fuentes, Montserrat},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reich et al. - 2011 - Variable selection for high dimensional Bayesian density estimation application to human exposure simulation.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B},
keywords = {Air pollution,Bayesian non-parametrics,High dimensional data,Kernel stick breaking prior,Stochastic computer models},
mendeley-tags = {Air pollution,Bayesian non-parametrics,High dimensional data,Kernel stick breaking prior,Stochastic computer models},
number = {1},
pages = {47--66},
title = {{Variable selection for high dimensional Bayesian density estimation: application to human exposure simulation}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=1623f0be4274fbc4{\&}attid=0.1{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ{\_}UbAt2BrwgVUj1lcHJUEkOShp8PtN89-aNlaNvQsCJaW2NITzwkEcsCVdionWFCTle6sjGOgMazUya82qZdOB9qxa5Mkq3nnX4z},
volume = {61},
year = {2011}
}
@article{Roberts1997,
abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ϱ. When the proposal variance is appropriately scaled accord-ing to n, the sequence of stochastic processes formed by the first compo-nent of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward effi-ciency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
author = {Roberts, G O and Gelman, A and Gilks, W R},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roberts, Gelman, Gilks - 1997 - WEAK CONVERGENCE AND OPTIMAL SCALING OF RANDOM WALK METROPOLIS ALGORITHMS 1.pdf:pdf},
journal = {The Annals of Applied Probability},
number = {1},
pages = {110--120},
title = {{Weak convergence and optimal scaling of random walk Metropolis algorithms}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aoap/1034625254},
volume = {7},
year = {1997}
}
@article{Sacks1989,
abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic - rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient prediction. WIth this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
annote = {r2018-03-13
Foundational with respect to much of the work done in computer model calibration. The case is made that despite their deterministic nature, computer models are a proper subject of statistical study, for which we may perform uncertainty analysis. The authors walk through the process of building a Gaussian process emulator of computer code for this purpose. Various covariance functions are discussed. The authors fit covariance function parameters through MLE. Roughly half the paper is devoted to questions of design.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby J. and Wynn, Henry P.},
doi = {10.1214/ss/1177012413},
eprint = {1011.1669},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sacks et al. - 1989 - Design and Analysis of Computer Experiments.pdf:pdf},
isbn = {0387954201},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,computer-aided design,experimental design,kriging,response surface,spatial statistics},
number = {4},
pages = {409--423},
pmid = {20948974},
title = {{Design and analysis of computer experiments}},
url = {http://projecteuclid.org/euclid.ss/1177012413},
volume = {4},
year = {1989}
}
@article{Sahinidis2004,
abstract = {A large number of problems in production planning and scheduling, location, transportation, finance, and engineering design require that decisions be made in the presence of uncertainty. Uncertainty, for instance, governs the prices of fuels, the availability of electricity, and the demand for chemicals. A key difficulty in optimization under uncertainty is in dealing with an uncertainty space that is huge and frequently leads to very large-scale optimization models. Decision-making under uncertainty is often further complicated by the presence of integer decision variables to model logical and other discrete decisions in a multi-period or multi-stage setting. This paper reviews theory and methodology that have been developed to cope with the complexity of optimization problems under uncertainty. We discuss and contrast the classical recourse-based stochastic programming, robust stochastic programming, probabilistic (chance-constraint) programming, fuzzy programming, and stochastic dynamic programming. The advantages and shortcomings of these models are reviewed and illustrated through examples. Applications and the state-of-the-art in computations are also reviewed. Finally, we discuss several main areas for future development in this field. These include development of polynomial-time approximation schemes for multi-stage stochastic programs and the application of global optimization algorithms to two-stage and chance-constraint formulations.},
author = {Sahinidis, Nikolaos V.},
doi = {10.1016/J.COMPCHEMENG.2003.09.017},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahinidis - 2004 - Optimization under uncertainty state-of-the-art and opportunities.pdf:pdf},
issn = {0098-1354},
journal = {Computers {\&} Chemical Engineering},
number = {6-7},
pages = {971--983},
publisher = {Pergamon},
title = {{Optimization under uncertainty: state-of-the-art and opportunities}},
url = {https://www.sciencedirect.com/science/article/pii/S0098135403002369},
volume = {28},
year = {2004}
}
@book{Santner2003a,
abstract = {The computer has become an increasingly popular tool for exploring the relationship between a measured response and factors thought to affect the response. In many cases, the basis of a computer model is a mathematical theory that implicitly relates the response to the factors. A computer model becomes possible given suitable numerical methods for accurately solving the mathematical system and appropriate computer hardware and software to implement the numerical methods. For example, in many engineering applications, the relationship is described by a dynamical system and the numerical method is a finite element code. The resulting computer "simulator" can generate the response corresponding to any given set of values of the factors. This allows one to use the code to conduct a "computer experiment" to explore the relationship between the response and the factors. In some cases, computer experimentation is feasible when a properly designed physical experiment (the gold standard for establishing cause and effect) is impossible; the number of input variables may be too large to consider performing a physical experiment, or power studies may show it is economically prohibitive to run an experiment on the scale required to answer a given research question. This book describes methods for designing and analyzing experiments that are conducted using a computer code rather than a physical experiment. It discusses how to select the values of the factors at which to run the code (the design of the computer experiment) in light of the research objectives of the experimenter. It also provides techniques for analyzing the resulting data so as to achieve these research goals. It illustrates these methods with code that is available to the reader at the companion web site for the book. Thomas Santner has been a professor in the Department of Statistics at The Ohio State University since 1990. At Ohio State, he has served as department Chair and Director of the department's Statistical Consulting Service. Previously, he was a professor in the School of Operations Research and Industrial Engineering at Cornell University. He is a Fellow of the American Statistical Association and the Institute of Mathematical Statistics, and is an elected ordinary member of the International Statistical Institute. He visited Ludwig Maximilians Universitt in Munich, Germany on a Fulbright Scholarship in 1996-97. Brian Williams has been an Associate Statistician at the RAND Corporation since 2000. His research interests include experimental design, computer experiments, Bayesian inference, spatial statistics and statistical computing. He holds a Ph. D. in statistics from The Ohio State University. William Notz is a professor in the Department of Statistics at The Ohio State University. At Ohio State, he has served as acting department chair, associate dean of the College of Mathematical and Physical Sciences, and as director of the department's Statistical Consulting Service. He has also served as Editor of the journal Technometrics and is a Fellow of the American Statistical Association. Physical Experiments and Computer Experiments -- Basic Elements of Computer Experiments -- Analyzing Output from Computer Experiments-Predicting Output from Training Data -- Space Filling Designs for Computer Experiments -- Criteria Based Designs for Computer Experiments -- Other Issues.},
address = {New York},
annote = {r2017-07
Ch. 1: Gives some examples of computer models, and discusses the relevant differences between computer and physical experiments. For example, replication is ordinarily not an issue for computer experiments, as it is in physical experiments.
Ch. 2: Includes a basic introduction to Gaussian process modeling.
Ch. 3: Discusses basics of making predictions using Gaussian process models: BLUPs, MSPE, etc.
Ch. 4: Continuation of chapter 3. Focuses on predictive distributions, rather than point estimates.
Ch. 5: Discusses latin hypercube sampling, stratified sampling, uniform designs, and other space-filling designs.
Ch. 6: Discusses entropy-based designs, MSPE-based designs, and other optimizing designs.
Ch. 7: Discusses various methods of sensitivity analysis of computer models.},
author = {Santner, Thomas J. and Williams, Brian J. and Notz, William I.},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santner, Williams, Notz - 2003 - The Design and Analysis of Computer Experiments.pdf:pdf},
isbn = {1475737998},
pages = {286},
publisher = {Springer},
title = {{The design and analysis of computer experiments}},
year = {2003}
}
@article{Savitsky2011,
abstract = {This paper presents a unified treatment of Gaussian process mod-els that extends to data from the exponential dispersion family and to survival data. Our specific interest is in the analysis of data sets with predictors that have an a priori unknown form of possibly nonlinear associations to the re-sponse. The modeling approach we describe incorporates Gaussian processes in a generalized linear model framework to obtain a class of nonparametric regression models where the covariance matrix depends on the predictors. We consider, in particular, continuous, categorical and count responses. We also look into models that account for survival outcomes. We explore alterna-tive covariance formulations for the Gaussian process prior and demonstrate the flexibility of the construction. Next, we focus on the important problem of selecting variables from the set of possible predictors and describe a gen-eral framework that employs mixture priors. We compare alternative MCMC strategies for posterior inference and achieve a computationally efficient and practical approach. We demonstrate performances on simulated and bench-mark data sets.},
annote = {r2018-03-22
This paper provides a thorough and helpful discussion of Gaussian process regression before coming to focus specifically on the problem of variable selection in a Gaussian process regression. This is valuable because it allows one to undertake variable selection when one has little notion of what might be the parametric form of the relationship of the potential covariates to the response. The selection is accomplished via spike-and-slab mixture priors. The authors discuss two distinct MCMC schemas for carrying out the variable selection, comparing them with each other and with other methods.},
author = {Savitsky, Terrance and Vannucci, Marina and Sha, Naijun},
doi = {10.1214/11-STS354},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Savitsky, Vannucci, Sha - 2011 - Variable Selection for Nonparametric Gaussian Process Priors Models and Computational Strategies.pdf:pdf},
journal = {Statistical Science},
keywords = {Bayesian variable selection,Gaussian processes,MCMC,generalized linear models,latent variables,nonparametric regression,survival data},
number = {1},
pages = {130--149},
title = {{Variable selection for nonparametric Gaussian process priors: models and computational strategies}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=16240215469252dc{\&}attid=0.2{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ{\_}oO5xZ1vTd1RjySHOmyEdI9Fs9roTk900L4w7A4AOqQqJ7gx-ee8n0NjU-NJqgOTx6kYDzgjORBJI9uYjIFhrR6s6d{\_}3b{\_}f29rI},
volume = {26},
year = {2011}
}
@article{Shahriari2016,
annote = {r2018-07-31
The article is very helpful as an introduction to and literature review of bayesian optimization. It gets rather technical and difficult to follow, eventually, but ramps up to that pretty slowly.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
doi = {10.1109/JPROC.2015.2494218},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
number = {1},
pages = {148--175},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {http://ieeexplore.ieee.org/document/7352306/},
volume = {104},
year = {2016}
}
@article{Shang2013,
abstract = {A generalized Gaussian process model (GGPM) is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the ex-ponential family distribution (EFD). In this paper, we consider efficient algorithms for approximate inference on GGPMs using the general form of the EFD. A particular GP model and its associ-ated inference algorithms can then be formed by changing the parameters of the EFD, thus greatly simplifying its creation for task-specific output domains. We demonstrate the efficacy of this frame-work by creating several new GP models for regressing to non-negative reals and to real intervals. We also consider a closed-form Taylor approximation for efficient inference on GGPMs, and elab-orate on its connections with other model-specific heuristic closed-form approximations. Finally, we present a comprehensive set of experiments to compare approximate inference algorithms on a wide variety of GGPMs.},
annote = {r2018-03-28
It's not clear to me what sort of value the work in this paper has. It's nice to provide an umbrella generalization of existing methods, but what does that get us, exactly? The paper is both very big and very difficult to read and I very much lament the authors' decision not to include illustrative examples of simple cases of GP regression/classification seen through the lens of their umbrella methodology.},
author = {Shang, Lifeng and Chan, Antoni B},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang, Chan - 2013 - On Approximate Inference for Generalized Gaussian Process Models.pdf:pdf},
journal = {Technical Report -City University of Hong Kong},
keywords = {Bayesian generalized linear models,Gaussian processes,approximate inference,exponential family,non-parametric regression},
title = {{On approximate inference for generalized Gaussian process models}},
url = {https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2{\&}ik=59d1cbdc4d{\&}view=att{\&}th=16240215469252dc{\&}attid=0.1{\&}disp=inline{\&}safe=1{\&}zw{\&}saddbat=ANGjdJ9Ez1jf30QAg{\_}EeS0yMiGWRGMl72wEwIYmWA9PXn99Yh5xJHY0PJxOB-zXBBRTdN3IzVsgIjtCzYl8PQuBsikm1PnEaV3vmk7KdY},
year = {2013}
}
@article{Sorokowska2017,
abstract = {Human spatial behavior has been the focus of hundreds of previous research studies. However, the conclusions and generalizability of previous studies on interpersonal distance preferences were limited by some important methodological and sampling issues. The objective of the present study was to compare preferred interpersonal distances across the world and to overcome the problems observed in previous studies. We present an extensive analysis of interpersonal distances over a large data set (N = 8,943 participants from 42 countries). We attempted to relate the preferred social, personal, and intimate distances observed in each country to a set of individual characteristics of the participants, and some attributes of their cultures. Our study indicates that individual characteristics (age and gender) influence interpersonal space preferences and that some variation in results can be explained by temperature in a given region. We also present objective values of preferred interpersonal distances in differe...},
author = {Sorokowska, Agnieszka and Sorokowski, Piotr and Hilpert, Peter and others},
doi = {10.1177/0022022117698039},
issn = {0022-0221},
journal = {Journal of Cross-Cultural Psychology},
keywords = {cultural psychology,culture,interpersonal distance,spatial behavior},
number = {4},
pages = {577--592},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Preferred interpersonal distances: A global comparison}},
url = {http://journals.sagepub.com/doi/10.1177/0022022117698039},
volume = {48},
year = {2017}
}
@article{Stevens2018,
abstract = {Purpose Partitioned analysis is an increasingly popular approach for modeling complex systems with behaviors governed by multiple, interdependent physical phenomena. Yielding accurate representations of reality from partitioned models depends on the availability of all necessary constituent models representing relevant physical phenomena. However, there are many engineering problems where one or more of the constituents may be unavailable because of lack of knowledge regarding the underlying principles governing the behavior or the inability to experimentally observe the constituent behavior in an isolated manner through separate-effect experiments. This study aims to enable partitioned analysis in such situations with an incomplete representation of the full system by inferring the behavior of the missing constituent. Design/methodology/approach This paper presents a statistical method for inverse analysis infer missing constituent physics. The feasibility of the method is demonstrated using a physics-ba...},
annote = {r2018-05-14
The paper describes the application of state-aware calibration to a weakly coupled system. It also spends some time extolling the virtues of subiterations in a Gibbs routine, i.e., sampling one set of parameters many times before returning to sample a more expensive parameter.},
author = {Stevens, Garrison N. and Atamturktur, Sez and Brown, D. Andrew and Williams, Brian J. and Unal, Cetin},
doi = {10.1108/EC-07-2016-0264},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stevens et al. - 2018 - Statistical inference of empirical constituents in partitioned analysis from integral-effect experiments.pdf:pdf},
issn = {0264-4401},
journal = {Engineering Computations},
keywords = {Bayesian inference,Coupled systems,Empirical surrogate,Emulator training,Metamodel,Multi-discipline modeling},
number = {2},
pages = {672--691},
publisher = {Emerald Publishing Limited},
title = {{Statistical inference of empirical constituents in partitioned analysis from integral-effect experiments}},
url = {http://www.emeraldinsight.com/doi/10.1108/EC-07-2016-0264},
volume = {35},
year = {2018}
}
@article{Storlie2015,
abstract = {It has become commonplace to use complex computer models to predict outcomes in regions where data do not exist. Typically these models need to be calibrated and validated using some experimental data, which often consists of multiple correlated outcomes. In addition, some of the model parameters may be categorical in nature, such as a pointer variable to alternate models (or submodels) for some of the physics of the system. Here, we present a general approach for calibration in such situations where an emulator of the computationally demanding models and a discrepancy term from the model to reality are represented within a Bayesian smoothing spline (BSS) ANOVA framework. The BSS-ANOVA framework has several advantages over the traditional Gaussian process, including ease of handling categorical inputs and correlated outputs, and improved computational efficiency. Finally, this framework is then applied to the problem that motivated its design; a calibration of a computational fluid dynamics (CFD) model of...},
annote = {r2017-07-13
This work sees itself as an advance on the work of Qian et al 2008 and Zhou et al 2011 in that it focuses specifically on calibration (rather than sensitivity analysis or uncertainty analysis). The authors support the use of "Bayesian Smoothing Spline ANOVA", which may offer some advantages in computational efficiency. But the authors also, for comparison, extend the approach of Higdon 2008 for use with qualitative input.},
author = {Storlie, Curtis B. and Lane, William A. and Ryan, Emily M. and Gattiker, James R. and Higdon, David M.},
doi = {10.1080/01621459.2014.979993},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2015 - Calibration of Computational Models With Categorical Parameters and Correlated Outputs via Bayesian Smoothing Sp.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Categorical inputs,Emulator,Inverse problem,Model calibration,Multiple outputs,Uncertainty quantification},
number = {509},
pages = {68--82},
publisher = {Taylor {\&} Francis},
title = {{Calibration of computational models with categorical parameters and correlated outputs via Bayesian smoothing spline ANOVA}},
url = {http://www.tandfonline.com/doi/full/10.1080/01621459.2014.979993},
volume = {110},
year = {2015}
}
@article{Storlie2013,
annote = {From Duplicate 1 (Analysis of computationally demanding models with continuous and categorical inputs - Storlie, Curtis B.; Reich, Brian J.; Helton, Jon C.; Swiler, Laura P.; Sallaberry, Cedric J.)

To read},
author = {Storlie, Curtis B. and Reich, Brian J. and Helton, Jon C. and Swiler, Laura P. and Sallaberry, Cedric J.},
doi = {10.1016/j.ress.2012.11.018},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Storlie et al. - 2013 - Analysis of computationally demanding models with continuous and categorical inputs.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering {\&} System Safety},
pages = {30--41},
title = {{Analysis of computationally demanding models with continuous and categorical inputs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0951832012002487},
volume = {113},
year = {2013}
}
@article{Subramanian2011,
abstract = {Background Adult height is a useful biological measure of long term population health and well being. We examined the cohort differences and socioeconomic patterning in adult height in low- to middle-income countries.   Methods/Findings We analyzed cross-sectional, representative samples of 364538 women aged 25-49 years drawn from 54 Demographic and Health Surveys (DHS) conducted between 1994 and 2008. Linear multilevel regression models included year of birth, household wealth, education, and area of residence, and accounted for clustering by primary sampling units and countries. Attained height was measured using an adjustable measuring board. A yearly change in birth cohorts starting with those born in 1945 was associated with a 0.0138 cm (95{\%} CI 0.0107, 0.0169) increase in height. Increases in heights in more recent birth year cohorts were largely concentrated in women from the richer wealth quintiles. 35 of the 54 countries experienced a decline (14) or stagnation (21) in height. The decline in heights was largely concentrated among the poorest wealth quintiles. There was a strong positive association between height and household wealth; those in two richest quintiles of household wealth were 1.988 cm (95{\%} CI 1.886, 2.090) and 1.018 cm (95{\%} CI 0.916, 1.120) taller, compared to those in the poorest wealth quintile. The strength of the association between wealth and height was positive (0.05 to 1.16) in 96{\%} (52/54) countries.   Conclusions Socioeconomic inequalities in height remain persistent. Height has stagnated or declined over the last decades in low- to middle-income countries, particularly in Africa, suggesting worsening nutritional and environmental circumstances during childhood.},
author = {Subramanian, S. V. and {\"{O}}zaltin, Emre and Finlay, Jocelyn E.},
doi = {10.1371/journal.pone.0018962},
editor = {Konigsberg, Lyle},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Subramanian, {\"{O}}zaltin, Finlay - 2011 - Height of Nations A Socioeconomic Analysis of Cohort Differences and Patterns among Women in 54 Lo.pdf:pdf},
issn = {1932-6203},
journal = {PLoS ONE},
number = {4},
pages = {e18962},
publisher = {Public Library of Science},
title = {{Height of nations: A socioeconomic analysis of cohort differences and patterns among women in 54 low- to middle-income countries}},
url = {http://dx.plos.org/10.1371/journal.pone.0018962},
volume = {6},
year = {2011}
}
@article{Thompson2008,
author = {Thompson, Ambler and Taylor, Barry N.},
journal = {Special Publication (NIST SP) - 811},
title = {{Guide for the use of the International System of Units (SI)}},
url = {https://www.nist.gov/publications/guide-use-international-system-units-si},
year = {2008}
}
@article{Thompson1995,
abstract = {The computer model 'SIMULEX' is designed to simulate the escape movement of thousands of individual people through large, geometri-cally complex building spaces. The model is intended for use both as a research and design tool to analyse the evacuation of large populations through a wide range of building environments. The computer program assigns a variety of attributes to each individual in the building population. These attributes include a co-ordinate position, angle of orientation, and a walking speed for each person. Specific algorithms that facilitate the simulation of escape movement include distance mapping, wayfinding, overtaking, route deviation, and adjustments to individual speeds due to the proximity of crowd members. These algorithms contribute to a computer package that displays the building plan and the position and progress of individual building occupants as they walk towards, and through the exits.},
author = {Thompson, Peter A and Marchant, Eric W},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thompson, Marchant - 1995 - A Computer Model for the Evacuation of Large Building Populations.pdf:pdf},
isbn = {0379-7112(95)00019-4},
journal = {Fire Safety Journal},
pages = {131--148},
title = {{A computer model for the evacuation of large building populations}},
url = {https://ac.els-cdn.com/037971129500019P/1-s2.0-037971129500019P-main.pdf?{\_}tid=ff6d6436-fcde-4a4c-a599-3562d0129240{\&}acdnat=1522335585{\_}cf9429d0f531f551cc6db4d0697d1163},
volume = {24},
year = {1995}
}
@article{Tierney1994,
author = {Tierney, Luke},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney - 1994 - Markov Chains for Exploring Posterior Distributions.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1701--1728},
title = {{Markov chains for exploring posterior distributions}},
url = {http://links.jstor.org/sici?sici=0090-5364{\%}28199412{\%}2922{\%}3A4{\%}3C1701{\%}3AMCFEPD{\%}3E2.0.CO{\%}3B2-6},
volume = {22},
year = {1994}
}
@article{Tuo2017,
abstract = {Identification of model parameters in computer simulations is an important topic in computer experiments. We propose a new method, called the projected kernel calibration method, to estimate these model parameters. The proposed method is proven to be asymptotic normal and semi-parametric efficient. As a frequentist method, the proposed method is as efficient as the {\$}L{\_}2{\$} calibration method proposed by Tuo and Wu [Ann. Statist. 43 (2015) 2331-2352]. On the other hand, the proposed method has a natural Bayesian version, which the {\$}L{\_}2{\$} method does not have. This Bayesian version allows users to calculate the credible region of the calibration parameters without using a large sample approximation. We also show that, the inconsistency problem of the calibration method proposed by Kennedy and O'Hagan [J. R. Stat. Soc. Ser. B. Stat. Methodol. 63 (2001) 425-464] can be rectified by a simple modification of the kernel matrix.},
annote = {To read},
archivePrefix = {arXiv},
arxivId = {1705.03422},
author = {Tuo, Rui},
eprint = {1705.03422},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuo - 2017 - Adjustments to Computer Models via Projected Kernel Calibration.pdf:pdf},
title = {{Adjustments to computer models via projected kernel calibration}},
url = {http://arxiv.org/abs/1705.03422},
year = {2017}
}
@article{Tuo2016,
abstract = {Calibration parameters in deterministic computer experiments are those attributes that cannot be measured or are not available in physical experiments. Kennedy and O'Hagan [M.C. Kennedy and A. O'Hagan, J. R. Stat. Soc. Ser. B Stat. Methodol., 63 (2001), pp. 425--464] suggested an approach to estimating them by using data from physical experiments and computer simulations. A theoretical framework is given which allows us to study the issues of parameter identifiability and estimation. We define the {\$}L{\_}2{\$}-consistency for calibration as a justification for calibration methods. It is shown that a simplified version of the original Kennedy--O'Hagan (KO) method leads to asymptotically {\$}L{\_}2{\$}-inconsistent calibration. This {\$}L{\_}2{\$}-inconsistency can be remedied by modifying the original estimation procedure. A novel calibration method, called {\$}L{\_}2{\$} calibration, is proposed, proven to be {\$}L{\_}2{\$}-consistent, and enjoys optimal convergence rate. A numerical example and some mathematical analysis are used to illustrate th...},
annote = {r2018-07-09
A lot of this article goes over my head, but it seems important enough to be worth delving back into it again sometime soon. The author makes the case that, contra Kennedy-O'Hagan, the discrepancy should be orthogonal to the gradient of the computer model. I don't really get why, or how, but there are some very impressive figures in there. File under "whoa if true".},
author = {Tuo, Rui and Wu, C. F. Jeff},
doi = {10.1137/151005841},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuo, Jeff Wu - 2016 - A Theoretical Framework for Calibration in Computer Models Parametrization, Estimation and Convergence Properties.pdf:pdf},
issn = {2166-2525},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
keywords = {62A01,62F12,62P30,Gaussian process,computer experiments,reproducing kernel Hilbert space,uncertainty quantification},
number = {1},
pages = {767--795},
publisher = {Society for Industrial and Applied Mathematics},
title = {{A theoretical framework for calibration in computer models: Parametrization, estimation and convergence properties}},
url = {http://epubs.siam.org/doi/10.1137/151005841},
volume = {4},
year = {2016}
}
@article{Tuo2015,
abstract = {Many computer models contain unknown parameters which need to be estimated using physical observations. Kennedy and O'Hagan (2001) shows that the calibration method based on Gaussian process models proposed by Kennedy and O'Hagan (2001) may lead to unreasonable estimate for imperfect computer models. In this work, we extend their study to calibration problems with stochastic physical data. We propose a novel method, called the {\$}L{\_}2{\$} calibration, and show its semiparametric efficiency. The conventional method of the ordinary least squares is also studied. Theoretical analysis shows that it is consistent but not efficient. Numerical examples show that the proposed method outperforms the existing ones.},
annote = {r2018-06-20
The authors propose a calibration method as an explicit competitor to Kennedy-O'Hagan calibration. The paper is very analysis-heavy, and mostly opaque to me.},
archivePrefix = {arXiv},
arxivId = {1507.07280},
author = {Tuo, Rui and Wu, C. F. Jeff},
doi = {10.1214/15-AOS1314},
eprint = {1507.07280},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuo, Jeff Wu - 2015 - Efficient calibration for imperfect computer models.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
number = {6},
title = {{Efficient calibration for imperfect computer models}},
volume = {43},
year = {2015}
}
@article{Vehtari2017,
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
doi = {10.1007/s11222-016-9696-4},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehtari, Gelman, Gabry - 2017 - Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
number = {5},
pages = {1413--1432},
publisher = {Springer US},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
url = {http://link.springer.com/10.1007/s11222-016-9696-4},
volume = {27},
year = {2017}
}
@article{Wang2009,
abstract = {Computer models are mathematical representations of real systems developed for understanding and investigating the systems. They are particularly useful when physical experiments are either cost- prohibitive or time-prohibitive. Before a computer model is used, it often must be validated by comparing the computer outputs with physical experiments. This article proposes a Bayesian approach to validating computer models that overcomes several difficulties of the frequentist approach proposed by Oberkampf and Barone. Kennedy and O'Hagan proposed a similar Bayesian approach. A major difference between their approach and ours is that theirs focuses on directly deriving the posterior of the true output, whereas our approach focuses on first deriving the posteriors of the computer model and model bias (difference between computer and true outputs) separately, then deriving the posterior of the true output. As a result, our approach provides a clear decomposition of the expected prediction error of the true outpu...},
annote = {r2018-06-25
The article's approach is very similar to the Kennedy-O'Hagan approach. The authors compare it to that approach throughout the paper. This paper's approach computes the posterior of the true output as the sum of posteriors of the computer model and the model bias, which is helpful for analyzing the posterior.},
author = {Wang, Shuchun and Chen, Wei and Tsui, Kwok-Leung},
doi = {10.1198/TECH.2009.07011},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Chen, Tsui - 2009 - Bayesian Validation of Computer Models.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Bayesian,Computer model,Gaussian process,Model bias,Model validation,Physical experiments},
number = {4},
pages = {439--451},
publisher = {Taylor {\&} Francis},
title = {{Bayesian validation of computer models}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2009.07011},
volume = {51},
year = {2009}
}
@article{Williams2006,
abstract = {A flyer plate experiment involves forcing a plane shock wave through stationary test samples of material and measuring the free surface velocity of the target as a function of time. These experiments are conducted to learn about the behavior of materials subjected to high strain rate environments. Computer simulations of flyer plate experiments are conducted with a two-dimensional hydro- dynamic code developed under the Advanced Strategic Computing (ASC) program at Los Alamos National Laboratory. This code incorporates physical models that contain parameters having uncertain values. The objectives of the analyses pre- sented in this paper are to assess the sensitivity of free surface velocity to variations in the uncertain inputs, to constrain the values of these inputs to be consistent with experiment, and to predict free surface velocity based on the constrained inputs. We implement a Bayesian approach that combines detailed physics simulations with experimental data for the desired statistical inference (Kennedy and O'Hagan 2001; Higdon, Kennedy, Cavendish, Cafeo, and Ryne 2004).},
annote = {r2018-01-12; 2017-11-01
Section 2.3 was especially helpful for building a GP emulator of a computationally expensive emulator. The rest of the paper deals with related topics such as using the emulator for sensitivity analysis. I just wonder if the sort of approach they use here is going to be appropriate for us, given how many observations we have. They use only 20 simulation observations, we use 504 times three simulator outputs.},
author = {Williams, Brian and Higdon, Dave and Gattiker, Jim and Moore, Leslie and McKay, Michael and Keller-McNulty, Sallie},
doi = {10.1214/06-BA125},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams et al. - 2006 - Combining experimental data and computer simulations, with an application to flyer plate experiments(2).pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Gaussian process,calibration,computer experiments,flyer plate experiments,model validation,predictability,predictive science,sensitivity analysis,uncertainty quantification},
number = {4},
pages = {765--792},
publisher = {International Society for Bayesian Analysis},
title = {{Combining experimental data and computer simulations, with an application to flyer plate experiments}},
url = {http://projecteuclid.org/euclid.ba/1340370942},
volume = {1},
year = {2006}
}
@article{Wong2014,
abstract = {This paper considers the computer model calibration problem and provides a general frequentist solution. Under the proposed framework, the data model is semi-parametric with a nonparametric discrepancy function which accounts for any discrepancy between the physical reality and the computer model. In an attempt to solve a fundamentally important (but often ignored) identifiability issue between the computer model parameters and the discrepancy function, this paper proposes a new and identifiable parametrization of the calibration problem. It also develops a two-step procedure for estimating all the relevant quantities under the new parameterization. This estimation procedure is shown to enjoy excellent rates of convergence and can be straightforwardly implemented with existing software. For uncertainty quantification, bootstrapping is adopted to construct confidence regions for the quantities of interest. The practical performance of the proposed methodology is illustrated through simulation examples and an application to a computational fluid dynamics model.},
annote = {From Duplicate 1 (A frequentist approach to computer model calibration - Wong, Raymond K. W.; Storlie, Curtis B.; Lee, Thomas C. M.)

To read},
archivePrefix = {arXiv},
arxivId = {1411.4723},
author = {Wong, Raymond K. W. and Storlie, Curtis B. and Lee, Thomas C. M.},
doi = {10.1111/rssb.12182},
eprint = {1411.4723},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Storlie, Lee - 2014 - A Frequentist Approach to Computer Model Calibration(2).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B)},
keywords = {Bootstrap,Inverse problem,Model misspecification,Semiparametric modelling,Surrogate model,Uncertainty analysis},
number = {2},
pages = {635--648},
title = {{A frequentist approach to computer model calibration}},
url = {http://doi.wiley.com/10.1111/rssb.12182 http://arxiv.org/abs/1411.4723},
volume = {79},
year = {2014}
}
@article{Zhang2015,
abstract = {ABSTRACTIn this article, we review and reexamine approaches to modeling computer experiments with qualitative and quantitative input variables. For those not familiar with models for computer experiments, we begin by showing, in a simple setting, that a standard model for computer experiments can be viewed as a generalization of regression models. We then review models that include both quantitative and quantitative variables and present some alternative parameterizations. Two are based on indicator functions and allow one to use standard quantitative inputs-only models. Another parameterization provides additional insight into possible underlying factorial structure. Finally, we use two examples to illustrate the benefits of these alternative models},
annote = {Very helpful overview focusing on the work of Han et al 2009, Qian et al 2008, and Zhou et al 2011.},
author = {Zhang, Yulei and Notz, William I.},
doi = {10.1080/08982112.2015.968039},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Notz - 2015 - Computer Experiments with Qualitative and Quantitative Variables A Review and Reexamination.pdf:pdf},
issn = {0898-2112},
journal = {Quality Engineering},
keywords = {GaSP model,Gaussian correlation function,best linear unbiased predictor,computer experiments,indicator variables,qualitative input},
number = {1},
pages = {2--13},
publisher = {Taylor {\&} Francis},
title = {{Computer experiments with qualitative and quantitative variables: A review and reexamination}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08982112.2015.968039},
volume = {27},
year = {2015}
}
@article{Zhou2011,
abstract = {We propose a flexible yet computationally efficient approach for building Gaussian process models for computer experiments with both qualitative and quantitative factors. This approach uses the hypersphere parameterization to model the correlations of the qualitative factors, thus avoiding the need of directly solving optimization problems with positive definite constraints. The effectiveness of the proposed method is successfully illustrated by several examples.},
annote = {r2017-07-14
The computational efficiency of the technique described in Qian et al 2008 is vastly improved here. However, this version still requires m(m-1)/2 parameters when the qualitative factor has m levels. For a single factor of 1000 materials, that is almost 500,000 variables; for two factors of 20 and 50 levels resp., it is 1415 parameters. Restrictions of the covariance matrix may allow reduction in the number of parameters without too far mitigating the utility of the framework.},
author = {Zhou, Qiang and Qian, Peter Z. G. and Zhou, Shiyu},
doi = {10.1198/TECH.2011.10025},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Qian, Zhou - 2011 - A Simple Approach to Emulation for Computer Models With Qualitative and Quantitative Factors.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {Computer experiment,Hypersphere decomposition,Kriging},
number = {3},
pages = {266--273},
publisher = {Taylor {\&} Francis},
title = {{A simple approach to emulation for computer models with qualitative and quantitative factors}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.10025},
volume = {53},
year = {2011}
}
@article{Zuev2012a,
abstract = {Estimation of small failure probabilities is one of the most important and challenging computational problems in reliability engineering. The failure probability is usually given by an integral over a high-dimensional uncertain parameter space that is difficult to evaluate numerically. This paper focuses on enhancements to Subset Simulation (SS), proposed by Au and Beck, which provides an efficient algorithm based on MCMC (Markov chain Monte Carlo) simulation for computing small failure probabilities for general high-dimensional reliability problems. First, we analyze the Modified Metropolis algorithm (MMA), an MCMC technique, which is used in SS for sampling from high-dimensional conditional distributions. The efficiency and accuracy of SS directly depends on the ergodic properties of the Markov chains generated by MMA, which control how fast the chain explores the parameter space. We present some observations on the optimal scaling of MMA for efficient exploration, and develop an optimal scaling strategy for this algorithm when it is employed within SS. Next, we provide a theoretical basis for the optimal value of the conditional failure probability p0, an important parameter one has to choose when using SS. We demonstrate that choosing any $p_0\in[0.1,0.3]$ will give similar efficiency as the optimal value of p0. Finally, a Bayesian post-processor SS+ for the original SS method is developed where the uncertain failure probability that one is estimating is modeled as a stochastic variable whose possible values belong to the unit interval. Simulated samples from SS are viewed as informative data relevant to the system's reliability. Instead of a single real number as an estimate, SS+ produces the posterior PDF of the failure probability, which takes into account both prior information and the information in the sampled data. This PDF quantifies the uncertainty in the value of the failure probability and it may be further used in risk analyses to incorporate this uncertainty. To demonstrate SS+, we consider its application to two different reliability problems: a linear reliability problem and reliability analysis of an elasto-plastic structure subjected to strong seismic ground motion. The relationship between the original SS and SS+ is also discussed.},
author = {Zuev, Konstantin M. and Beck, James L. and Au, Siu-Kui and Katafygiotis, Lambros S.},
doi = {10.1016/J.COMPSTRUC.2011.10.017},
file = {:C$\backslash$:/Users/carle/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zuev et al. - 2012 - Bayesian post-processor and other enhancements of Subset Simulation for estimating failure probabilities in high di.pdf:pdf},
issn = {0045-7949},
journal = {Computers {\&} Structures},
pages = {283--296},
publisher = {Pergamon},
title = {{Bayesian post-processor and other enhancements of Subset Simulation for estimating failure probabilities in high dimensions}},
url = {https://www.sciencedirect.com/science/article/pii/S0045794911002720},
volume = {92-93},
year = {2012}
}
@book{MATLAB2017,
year = {2017},
author = {{\sc Matlab}},
title = {Version 9.2.0 (R2017a)},
publisher = {The MathWorks, Inc.},
address = {Natick, Massachusetts}
}
@article{Gelfand1990,
author = {Gelfand, Alan E. and Smith, Adrian F. M.},
doi = {10.1080/01621459.1990.10476213},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {410},
pages = {398--409},
title = {{Sampling-based approaches to calculating marginal densities}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10476213},
volume = {85},
year = {1990}
}
@article{Farajpour2013,
author = {Farajpour, Ismail and Atamturktur, Sez},
doi = {10.1061/(ASCE)CP.1943-5487.0000233},
issn = {0887-3801},
journal = {Journal of Computing in Civil Engineering},
month = {jul},
number = {4},
pages = {407--418},
title = {{Error and Uncertainty Analysis of Inexact and Imprecise Computer Models}},
url = {http://ascelibrary.org/doi/10.1061/{\%}28ASCE{\%}29CP.1943-5487.0000233},
volume = {27},
year = {2013}
}
@article{Hemez2011,
abstract = {Activities such as global sensitivity analysis, statistical effect screening, uncertainty propagation, or model calibration have become integral to the Verification and Validation (V{\&}V) of numerical models and computer simulations. One of the goals of V{\&}V is to assess prediction accuracy and uncertainty, which feeds directly into reliability analysis or the Quantification of Margin and Uncertainty (QMU) of engineered systems. Because these analyses involve multiple runs of a computer code, they can rapidly become computationally expensive. An alternative to Monte Carlo-like sampling is to combine a design of computer experiments to meta-modeling, and replace the potentially expensive computer simulation by a fast-running emulator. The surrogate can then be used to estimate sensitivities, propagate uncertainty, and calibrate model parameters at a fraction of the cost it would take to wrap a sampling algorithm or optimization solver around the physics-based code. Doing so, however, offers the risk to develop an incorrect emulator that erroneously approximates the “true-but-unknown” sensitivities of the physics-based code. We demonstrate the extent to which this occurs when Gaussian Process Modeling (GPM) emulators are trained in high-dimensional spaces using too-sparsely populated designs-of-experiments. Our illustration analyzes a variant of the Rosenbrock function in which several effects are made statistically insignificant while others are strongly coupled, therefore, mimicking a situation that is often encountered in practice. In this example, using a combination of GPM emulator and design-of-experiments leads to an incorrect approximation of the function. A mathematical proof of the origin of the problem is proposed. The adverse effects that too-sparsely populated designs may produce are discussed for the coverage of the design space, estimation of sensitivities, and calibration of parameters. This work attempts to raise awareness to the potential dangers of not allocating enough resources when exploring a design space to develop fast-running emulators.},
author = {Hemez, Fran{\c{c}}ois M. and Atamturktur, Sezer},
doi = {10.1016/J.RESS.2011.02.015},
issn = {0951-8320},
journal = {Reliability Engineering {\&} System Safety},
month = {sep},
number = {9},
pages = {1220--1231},
publisher = {Elsevier},
title = {{The dangers of sparse sampling for the quantification of margin and uncertainty}},
url = {https://www.sciencedirect.com/science/article/pii/S0951832011000731},
volume = {96},
year = {2011}
}
@article{VanBuren2013,
author = {{Van Buren}, Kendra L. and Mollineaux, Mark G. and Hemez, Fran{\c{c}}ois M. and Atamturktur, Sezer},
doi = {10.1002/we.1522},
issn = {10954244},
journal = {Wind Energy},
keywords = {Bayesian inference,analysis correlation,sensitivity analysis,test,uncertainty quantification,verification and validation},
month = {jul},
number = {5},
pages = {741--758},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Simulating the dynamics of wind turbine blades: part II, model validation and uncertainty quantification}},
url = {http://doi.wiley.com/10.1002/we.1522},
volume = {16},
year = {2013}
}
@article{VanBuren2014,
abstract = {Several plausible modeling strategies are available to develop numerical models for simulating the dynamics of wind turbine blades. While the modeling strategy is typically selected according to expert judgment, the “best” modeling approach is unknown to the model developer. Thus, comparing plausible modeling strategies through a systematic and rigorous approach becomes necessary. This manuscript departs from the conventional approach that selects the model with the highest fidelity-to-data; and instead explores the trade-off between fidelity of model predictions to experiments and robustness of model predictions to model imprecision and inexactness. Exploring robustness in addition to fidelity lends credibility to the model, ensuring model predictions can be trusted even when lack-of-knowledge in the modeling assumptions and/or input parameters result in unforeseen errors and uncertainties. This concept is demonstrated on the CX-100 wind turbine blade in an experimental configuration with large masses added to load the blade in bending during vibration testing. The finite element model of the blade is built with shell elements and validated against experimental evidence, while the large masses are modeled according to two different, but plausible strategies using (i) a combination of point-mass and spring elements, and (ii) solid elements. These two modeling strategies are evaluated considering both the fidelity of the natural frequency predictions against experiments, and the robustness of the predicted natural frequencies to uncertainties in the input parameters. By considering robustness during model selection, the authors determine the extent to which prediction accuracy deteriorates as the lack-of-knowledge increases. The findings suggest the model with solid elements offers a higher degree of fidelity-to-data and robustness to uncertainties, thus providing a superior modeling strategy than the model with point masses and stiffening springs.},
author = {{Van Buren}, Kendra L. and Atamturktur, Sez and Hemez, Fran{\c{c}}ois M.},
doi = {10.1016/J.YMSSP.2013.10.010},
issn = {0888-3270},
journal = {Mechanical Systems and Signal Processing},
month = {feb},
number = {1-2},
pages = {246--259},
publisher = {Academic Press},
title = {{Model selection through robustness and fidelity criteria: Modeling the dynamics of the CX-100 wind turbine blade}},
url = {https://www.sciencedirect.com/science/article/pii/S088832701300513X},
volume = {43},
year = {2014}
}
@software{ansys,
  author = {{ANSYS, Inc.}},
  title = {ANSYS\textsuperscript{\textregistered} Academic Research Mechanical, Release 18.1},
  year = {2017}
}
@article{Ezzat2018,
annote = {r2019-09-13
The paper focuses on questions of DoE in the context of calibrating a system wherein the calibration parameter is a function of the other inputs. But their proposed design methods are not limited to such cases. The paper is valuable for its literature review and summaries of related work. Their own proposal seems fine. It's a bit hard for me to evaluate.},
author = {Ezzat, Ahmed Aziz and Pourhabib, Arash and Ding, Yu},
doi = {10.1080/00401706.2017.1377638},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Ezzat, Pourhabib, Ding - 2018 - Sequential Design for Functional Calibration of Computer Models.pdf:pdf},
issn = {15372723},
journal = {Technometrics},
keywords = {Calibration,Computer experiments,Functional calibration,Physical experiments,Sequential design},
mendeley-groups = {Calibration},
month = {jul},
number = {3},
pages = {286--296},
publisher = {American Statistical Association},
title = {{Sequential Design for Functional Calibration of Computer Models}},
volume = {60},
year = {2018}
}
@techreport{Jones1998,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
booktitle = {Journal of Global Optimization},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Jones, Schonlau, Welch - 1998 - Efficient Global Optimization of Expensive Black-Box Functions.pdf:pdf},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
pages = {455--492},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
url = {http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/{\$}FILE/Jones98.pdf},
volume = {13},
year = {1998}
}
@article{Chevalier2014,
abstract = {Stepwise uncertainty reduction (SUR) strategies aim at$\backslash$nconstructing a sequence of points for evaluating a function f in$\backslash$nsuch a way that the residual uncertainty about a quantity of$\backslash$ninterest progressively decreases to zero. Using such strategies$\backslash$nin the framework of Gaussian process modeling has been shown to$\backslash$nbe efficient for estimating the volume of excursion of f above a$\backslash$nfixed threshold. However, SUR strategies remain cumbersome to use$\backslash$nin practice because of their high computational complexity, and$\backslash$nthe fact that they deliver a single point at each iteration. In$\backslash$nthis article we introduce several multipoint sampling criteria,$\backslash$nallowing the selection of batches of points at which f can be$\backslash$nevaluated in parallel. Such criteria are of particular interest$\backslash$nwhen f is costly to evaluate and several CPUs are simultaneously$\backslash$navailable. We also manage to drastically reduce the computational$\backslash$ncost of these strategies through the use of closed form formulas.$\backslash$nWe illustrate their performances in various numerical$\backslash$nexperiments, including a nuclear safety test case. Basic notions$\backslash$nabout kriging, auxiliary problems, complexity calculations, R$\backslash$ncode, and data are available online as supplementary materials.},
annote = {r2019-10-04
The paper describes how to use SUR with closed formulae that make it computationally tractable, even when selecting multiple evaluation points simultaneously (to take advantage of parallel processing). For CTO, this would seem to be citable as an important paper in the development of SUR for Bayesian optimization. This paper considers only univariate objective functions (and their focus is anyway on failure probability, not on optimization).},
author = {Chevalier, Cl{\'{e}}ment and Bect, Julien and Ginsbourger, David and Vazquez, Emmanuel and Picheny, Victor and Richet, Yann},
doi = {10.1080/00401706.2013.860918},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Chevalier et al. - 2014 - Fast Parallel Kriging-Based Stepwise Uncertainty Reduction With Application to the Identification of an Excurs.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
number = {4},
title = {{Fast Parallel Kriging-Based Stepwise Uncertainty Reduction With Application to the Identification of an Excursion Set}},
volume = {56},
year = {2014}
}
@article{Binois2019,
abstract = {An ongoing aim of research in multiobjective Bayesian optimization is to extend its applicability to a large number of objectives. While coping with a limited budget of evaluations, recovering the set of optimal compromise solutions generally requires numerous observations and is less interpretable since this set tends to grow larger with the number of objectives. We thus propose to focus on a specific solution originating from game theory, the Kalai-Smorodinsky solution, which possesses attractive properties. In particular, it ensures equal marginal gains over all objectives. We further make it insensitive to a monotonic transformation of the objectives by considering the objectives in the copula space. A novel tailored algorithm is proposed to search for the solution, in the form of a Bayesian optimization algorithm: sequential sampling decisions are made based on acquisition functions that derive from an instrumental Gaussian process prior. Our approach is tested on three problems with respectively four, six, and ten objectives. The method is available in the package GPGame available on CRAN at https://cran.r-project.org/package=GPGame.},
annote = {r2019-05-21
Interesting paper. They offer a way to use game theory to single out a compromise point in multiobjective optimization (from among the Pareto efficient points) that is invariant under monotone transformation.},
archivePrefix = {arXiv},
arxivId = {1902.06565},
author = {Binois, Micka{\"{e}}l and Picheny, Victor and Taillandier, Patrick and Habbal, Abderrahmane},
eprint = {1902.06565},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Binois et al. - 2019 - The Kalai-Smorodinski solution for many-objective Bayesian optimization.pdf:pdf},
mendeley-groups = {Calibration},
month = {feb},
title = {{The Kalai-Smorodinski solution for many-objective Bayesian optimization}},
url = {http://arxiv.org/abs/1902.06565},
year = {2019}
}
@article{Picheny2019,
abstract = {Game theory finds nowadays a broad range of applications in engineering and machine learning. However, in a derivative-free, expensive black-box context, very few algorithmic solutions are available to find game equilibria. Here, we propose a novel Gaussian-process based approach for solving games in this context. We follow a classical Bayesian optimization framework, with sequential sampling decisions based on acquisition functions. Two strategies are proposed, based either on the probability of achieving equilibrium or on the stepwise uncertainty reduction paradigm. Practical and numerical aspects are discussed in order to enhance the scalability and reduce computation time. Our approach is evaluated on several synthetic game problems with varying number of players and decision space dimensions. We show that equilibria can be found reliably for a fraction of the cost (in terms of black-box evaluations) compared to classical, derivative-based algorithms. The method is available in the R package GPGame available on CRAN at https://cran.r-project.org/package=GPGame.},
annote = {r2019-05-07
Interesting paper describing the use of Bayesian optimization to find Nash equilibria. Some nice references related to GP regression that would be good for me to follow up on.},
author = {Picheny, Victor and Binois, Mickael and Habbal, Abderrahmane},
doi = {10.1007/s10898-018-0688-0},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Picheny, Binois, Habbal - 2019 - A Bayesian optimization approach to find Nash equilibria(2).pdf:pdf},
journal = {Journal of Global Optimization},
mendeley-groups = {Calibration},
month = {jan},
number = {1},
pages = {171--192},
publisher = {Springer US},
title = {{A Bayesian optimization approach to find Nash equilibria}},
url = {http://link.springer.com/10.1007/s10898-018-0688-0},
volume = {73},
year = {2019}
}
@article{Picheny2015,
abstract = {Optimization of expensive computer models with the help of Gaussian process emulators in now commonplace. However, when several (competing) objectives are considered, choosing an appropriate sampling strategy remains an open question. We present here a new algorithm based on stepwise uncertainty reduction principles to address this issue. Optimization is seen as a sequential reduction of the volume of the excursion sets below the current best solutions, and our sampling strategy chooses the points that give the highest expected reduction. Closed-form formulae are provided to compute the sampling criterion, avoiding the use of cumbersome simulations. We test our method on numerical examples, showing that it provides an efficient trade-off between exploration and intensification.},
annote = {r2019-07-19
The authors lay out a reasonably intuitive and well-motivated algorithm for stepwise uncertainty reduction in multi-objective optimization. Essentially at each step you pick the new evaluation location by finding the point expected to most lower the hypervolume of the difference between the currently sampled Pareto front and the true Pareto front. Computationally expensive, requires numerical integration.},
author = {Picheny, Victor},
doi = {10.1007/s11222-014-9477-x},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Picheny - 2015 - Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction(2).pdf:pdf},
isbn = {1122201494},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {EGO,Excursion sets,Kriging,Pareto front},
number = {6},
pages = {1265--1280},
publisher = {Springer US},
title = {{Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction}},
url = {http://dx.doi.org/10.1007/s11222-014-9477-x},
volume = {25},
year = {2015}
}
@article{Geman1996,
abstract = {We present a new approach for tracking roads from satellite images, and thereby illustrate a general computational strategy ("active testing") for tracking 1D structures and other recognition tasks in computer vision. Our approach is related to recent work in active vision on "where to look next" and motivated by the "divide-and-conquer" strategy of parlor games such as "Twenty Questions." We choose "tests" (matched filters for short road segments) one at a time in order to remove as much uncertainty as possible about the "true hypothesis" (road position) given the results of the previous tests. The tests are chosen on-line based on a statistical model for the joint distribution of tests and hypotheses. The problem of minimizing uncertainty (measured by entropy) is formulated in simple and explicit analytical terms. To execute this entropy testing rule we then alternate between data collection and optimization: At each iteration new image data are examined and a new entropy minimization problem is solved (exactly), resulting in a new image location to inspect, and so forth. We report experiments using panchromatic SPOT satellite imagery with a ground resolution of ten meters: Given a starting point and starting direction, we are able to rapidly track highways in southern France over distances on the order of one hundred kilometers without manual intervention. {\textcopyright}1996 IEEE.},
author = {Geman, Donald and Jedynak, Bruno},
doi = {10.1109/34.476006},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {D{\'{e}}cision tree, model-based tracking, active testing, roads, spot images},
number = {1},
pages = {1--14},
title = {{An active testing model for tracking roads in satellite images}},
volume = {18},
year = {1996}
}
@article{Villemonteix2009,
abstract = {In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizers entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on stepwise uncertainty reduction, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the Efficient Global Optimization (EGO) algorithm. An empirical comparison is carried out between our criterion and expected improvement, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization), is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise. {\textcopyright} 2008 Springer Science+Business Media, LLC.},
archivePrefix = {arXiv},
arxivId = {cs/0611143},
author = {Villemonteix, Julien and Vazquez, Emmanuel and Walter, Eric},
doi = {10.1007/s10898-008-9354-2},
eprint = {0611143},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Villemonteix, Vazquez, Walter - 2009 - An informational approach to the global optimization of expensive-to-evaluate functions.pdf:pdf},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Gaussian process,Global optimization,Kriging,Robust optimization,Stepwise uncertainty reduction},
month = {aug},
number = {4},
pages = {509--534},
primaryClass = {cs},
title = {{An informational approach to the global optimization of expensive-to-evaluate functions}},
volume = {44},
year = {2009}
}
@techreport{MiguelHernandez-Lobato2016,
abstract = {We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization.},
author = {{Miguel Hern{\'{a}}ndez-Lobato}, Jos{\'{e}} and Gelbart, Michael A and Adams, Ryan P and Hoffman, Matthew W and Ghahramani, Zoubin and Hern{\'{a}}ndez-Lobato, J M and Gelbart, M A and Adams, R P and Hoffman, M W and {Ghahramani Hern{\'{a}}ndez-Lobato}, Z},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Miguel Hern{\'{a}}ndez-Lobato et al. - 2016 - A General Framework for Constrained Bayesian Optimization using Information-based Search.pdf:pdf},
keywords = {Bayesian optimization,constraints,predictive entropy search * Authors contributed equally},
pages = {1--53},
title = {{A General Framework for Constrained Bayesian Optimization using Information-based Search}},
volume = {17},
year = {2016}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
month = {dec},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@misc{Ehrett2019,
    title={Coupling material and mechanical design processes via computer model calibration},
    author={Carl Ehrett and D. Andrew Brown and Evan Chodora and Christopher Kitchens and Sez Atamturktur},
    year={2019},
    eprint={1907.09553},
    archivePrefix={arXiv},
    primaryClass={stat.AP}
}
@article{Regis2004,
abstract = {We develop an approach for the optimization of continuous costly functions that uses a space-filling experimental design and local function approximation to reduce the number of function evaluations in an evolutionary algorithm. Our approach is to estimate the objective function value of an offspring by fitting a function approximation model over the k nearest previously evaluated points, where k = (d + 1)(d + 2)/2 and d is the dimension of the problem. The estimated function values are used to screen offspring to identify the most promising ones for function evaluation. To fit function approximation models, a symmetric Latin hypercube design (SLHD) is used to determine initial points for function evaluation. We compared the performance of an evolution strategy (ES) with local quadratic approximation, an ES with local cubic radial basis function (RBF) interpolation, an ES whose initial parent population comes from an SLHD, and a conventional ES. These algorithms were applied to a twelve-dimensional (12-D) groundwater bioremediation problem involving a complex nonlinear finite-element simulation model. The performances of these algorithms were also compared on the Dixon-Szeg{\"{o}} test functions and on the ten-dimensional (10-D) Rastrigin and Ackley test functions. All comparisons involve analysis of variance (ANOVA) and the computation of simultaneous confidence intervals. The results indicate that ES algorithms with local approximation were significantly better than conventional ES algorithms and ES algorithms initialized by SLHDs on all Dixon-Szeg{\"{o}} test functions except for Goldstein-Price. However, for the more difficult 10-D and 12-D functions, only the cubic RBF approach was successful in improving the performance of ah ES. Moreover, the results also suggest that the cubic RBF approach is superior to the quadratic approximation approach on all test functions and the  difference in performance is statistically significant for all test functions with dimension d ≥ 4. {\textcopyright} 2004 IEEE.},
author = {Regis, Rommel G. and Shoemaker, Christine A.},
doi = {10.1109/TEVC.2004.835247},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Regis, Shoemaker - 2004 - Local function approximation in evolutionary algorithms for the optimization of costly functions.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
number = {5},
pages = {490--505},
title = {{Local function approximation in evolutionary algorithms for the optimization of costly functions}},
volume = {8},
year = {2004}
}
@book{Nocedal2006,
abstract = {2nd ed. 'Numerical Optimization' presents a comprehensive description of the effective methods in continuous optimization. The book includes chapters on nonlinear interior methods {\&} derivative-free methods for optimization. It is useful for graduate students, researchers and practitioners. Fundamentals of Unconstrained Optimization -- Line Search Methods -- Trust-Region Methods -- Conjugate Gradient Methods -- Quasi-Newton Methods -- Large-Scale Unconstrained Optimization -- Calculating Derivatives -- Derivative-Free Optimization -- Least-Squares Problems -- Nonlinear Equations -- Theory of Constrained Optimization -- Linear Programming: The Simplex Method -- Linear Programming: Interior-Point Methods -- Fundamentals of Algorithms for Nonlinear Constrained Optimization -- Quadratic Programming -- Penalty and Augmented Lagrangian Methods -- Sequential Quadratic Programming -- Interior-Point Methods for Nonlinear Programming.},
author = {Nocedal, Jorge. and Wright, Stephen J.},
isbn = {9780387303031},
pages = {664},
publisher = {Springer},
title = {{Numerical optimization}},
year = {2006}
}
@book{Lee2007,
abstract = {This book explores how developing solutions with heuristic tools offers two major advantages: shortened development time and more robust systems. It begins with an overview of modern heuristic techniques and goes on to cover specific applications of heuristic approaches to power system problems, such as security assessment, optimal power flow, power system scheduling and operational planning, power generation expansion planning, reactive power planning, transmission and distribution planning, network reconfiguration, power system control, and hybrid systems of heuristic methods. {\textcopyright} 2008 the Institute of Electrical and Electronics Engineers, Inc.},
author = {Lee, Kwang Y. and El-Sharkawi, Mohamed A.},
booktitle = {Modern Heuristic Optimization Techniques: Theory and Applications to Power Systems},
doi = {10.1002/9780470225868},
isbn = {9780471457114},
month = {jun},
pages = {1--586},
publisher = {John Wiley and Sons},
title = {{Modern Heuristic Optimization Techniques: Theory and Applications to Power Systems}},
year = {2007}
}
@book{Branke2008,
abstract = {Multiobjective optimization deals with solving problems having not only one, but multiple, often conflicting, criteria. Such problems can arise in practically every field of science, engineering and business, and the need for efficient and reliable solution methods is increasing. The task is challenging due to the fact that, instead of a single optimal solution, multiobjective optimization results in a number of solutions with different trade-offs among criteria, also known as Pareto optimal or efficient solutions. Hence, a decision maker is needed to provide additional preference information and to identify the most satisfactory solution. Depending on the paradigm used, such information may be introduced before, during, or after the optimization process. Clearly, research and application in multiobjective optimization involve expertise in optimization as well as in decision support.This state-of-the-art survey originates from the International Seminar on Practical Approaches to Multiobjective Optimization, held in Dagstuhl Castle, Germany, in December 2006, which brought together leading experts from various contemporary multiobjective optimization fields, including evolutionary multiobjective optimization (EMO), multiple criteria decision making (MCDM) and multiple criteria decision aiding (MCDA).This book gives a unique and detailed account of the current status of research and applications in the field of multiobjective optimization. It contains 16 chapters grouped in the following 5 thematic sections: Basics on Multiobjective Optimization; Recent Interactive and Preference-Based Approaches; Visualization of Solutions; Modelling, Implementation and Applications; and Quality Assessment, Learning, and Future Challenges.},
editor = {Branke, J{\"{u}}rgen and Deb, Kalyanmoy and Miettinen, Kaisa and Slowinski, Roman},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Unknown - 2008 - Multiobjective Optimization Interactive and Evolutionary Approaches.pdf:pdf},
isbn = {3540889078},
issn = {03029743},
number = {January},
publisher = {Springer},
title = {{Multiobjective Optimization: Interactive and Evolutionary Approaches}},
volume = {5252 LNCS},
year = {2008}
}
@article{Deb2002,
annote = {r2018-05-17
Surprisingly clear and accessible description of the NSGA-II algorithm.},
author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
doi = {10.1109/4235.996017},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Deb et al. - 2002 - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf:pdf},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
mendeley-groups = {Calibration},
month = {apr},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
url = {http://ieeexplore.ieee.org/document/996017/},
volume = {6},
year = {2002}
}
@article{Kim2004,
abstract = {Multi-objective optimization methods are essential to resolve real-world problems as most involve several types of objects. Several multi-objective genetic algorithms have been proposed. Among them, SPEA2 and NSGA-II are the most successful. In the present study, two new mechanisms were added to SPEA2 to improve its searching ability a more effective crossover mechanism and an archive mechanism to maintain diversity of the solutions in the objective and variable spaces. The new SPEA2 with these two mechanisms was named SPEA2+. To clarify the characteristics and effectiveness of the proposed method, SPEA2+ was applied to several test functions. In the comparison of SPEA2+ with SPEA2 and NSGA-II, SPEA2+ showed good results and the effects of the new mechanism were clarified. From these results, it was concluded that SPEA2+ is a good algorithm for multi-objective optimization problems. {\textcopyright} Springer-Verlag 2004.},
author = {Kim, Mifa and Hiroyasu, Tomoyuki and Miki, Mitsunori and Watanabe, Shinya},
doi = {10.1007/978-3-540-30217-9_75},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {742--751},
title = {{SPEA2+: Improving the performance of the strength pareto evolutionary algorithm 2}},
volume = {3242},
year = {2004}
}
@misc{Bonyadi2017,
abstract = {This paper reviews recent studies on the Particle SwarmOptimization (PSO) algorithm. The review has been focused on high impact recent articles that have analyzed and/or modified PSO algorithms. This paper also presents some potential areas for future study.},
author = {Bonyadi, Mohammad Reza and Michalewicz, Zbigniew},
booktitle = {Evolutionary Computation},
doi = {10.1162/EVCO_r_00180},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Bonyadi, Michalewicz - 2017 - Particle swarm optimization for single objective continuous space problems A review.pdf:pdf},
issn = {15309304},
keywords = {Constrained optimization,Invariance,Local convergence,Parameter selection,Particle swarm optimization,Stability analysis,Topology},
month = {mar},
number = {1},
pages = {1--54},
publisher = {MIT Press Journals},
title = {{Particle swarm optimization for single objective continuous space problems: A review}},
volume = {25},
year = {2017}
}
@article{Mason2017,
abstract = {Particle swarm optimisation (PSO) is a bio-inspired swarm based approach to solving optimisation problems. The algorithm functions as a result of particles traversing and evaluating the problem space, eventually converging on the optimum solution. This paper applies a number of PSO variants to the dynamic economic emission dispatch (DEED) problem. The DEED problem is a multi-objective optimisation problem in which the goal is to optimise two conflicting objectives: cost and emissions. The PSO variants tested include: the standard PSO (SPSO), the PSO with avoidance of worst locations (PSO AWL), and also a selection of different topologies including the PSO with a gradually increasing directed neighbourhood (PSO GIDN). The aim of the paper is to test the performance of different variants of the PSO AWL against variants of the SPSO on the DEED problem. The results show that the PSO AWL outperforms the SPSO for every topology implemented. The results are also compared to state of the art genetic algorithm (NSGA-II) and multi-agent eeinforcement learning (MARL). This paper then examines the performance of each PSO algorithm when the power demand is modified to form a triangle wave. The purpose of this experiment was to analyse the performance of different PSO variants on an increasingly constrained problem.},
author = {Mason, Karl and Duggan, Jim and Howley, Enda},
doi = {10.1016/j.neucom.2017.03.086},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Constrained optimisation,Dynamic economic emission dispatch,Multi-objective,Particle swarm optimisation,Power generation,Swarm intelligence},
month = {dec},
pages = {188--197},
publisher = {Elsevier B.V.},
title = {{Multi-objective dynamic economic emission dispatch using particle swarm optimisation variants}},
volume = {270},
year = {2017}
}
@incollection{Robert2004,
address = {New York, NY},
author = {Robert, Christian P. and Casella, George},
booktitle = {Monte Carlo Statistical Methods},
chapter = {5},
doi = {10.1007/978-1-4757-4145-2_5},
edition = {2},
pages = {157--204},
publisher = {Springer New York},
title = {{Monte Carlo Optimization}},
url = {http://link.springer.com/10.1007/978-1-4757-4145-2{\_}5},
year = {2004}
}
@article{Zhou2011b,
author = {Zhou, Aimin and Qu, Bo-Yang and Li, Hui and Zhao, Shi-Zheng and Suganthan, Ponnuthurai Nagaratnam and Zhang, Qingfu},
doi = {10.1016/j.swevo.2011.03.001},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Zhou et al. - 2011 - Multiobjective evolutionary algorithms A survey of the state of the art.pdf:pdf},
issn = {22106502},
journal = {Swarm and Evolutionary Computation},
mendeley-groups = {Calibration},
month = {mar},
number = {1},
pages = {32--49},
title = {{Multiobjective evolutionary algorithms: A survey of the state of the art}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2210650211000058},
volume = {1},
year = {2011}
}
@article{Deb2006,
abstract = {In optimization studies including multi-objective optimization, the main focus is placed on finding the global optimum or global Pareto-optimal solutions, representing the best possible objective values. However, in practice, users may not always be interested in finding the so-called global best solutions, particularly when these solutions are quite sensitive to the variable perturbations which cannot be avoided in practice. In such cases, practitioners are interested in finding the robust solutions which are less sensitive to small perturbations in variables. Although robust optimization is dealt with in detail in single-objective evolutionary optimization studies, in this paper, we present two different robust multi-objective optimization procedures, where the emphasis is to find a robust frontier, instead of the global Pareto-optimal frontier in a problem. The first procedure is a straightforward extension of a technique used for single-objective optimization and the second procedure is a more practical approach enabling a user to set the extent of robustness desired in a problem. To demonstrate the differences between global and robust multi-objective optimization principles and the differences between the two robust optimization procedures suggested here, we develop a number of constrained and unconstrained test problems having two and three objectives and show simulation results using an evolutionary multi-objective optimization (EMO) algorithm. Finally, we also apply both robust optimization methodologies to an engineering design problem. {\textcopyright} 2006 by the Massachusetts Institute of Technology.},
annote = {r2019-09-30
The authors describe two different forms of robust optimization. The first, they say, is a direct extension of ideas already floating around for robust single-objective optimization. Here, the central idea is that instead of finding the global PF of the original objective function f, you instead find the global PF of a different objective function, f{\_}eff, for which the value of f{\_}eff(x) is given by the mean of f(x) in a $\backslash$delta-neighborhood around the argument x. $\backslash$delta is a parameter that must be chosen/tuned. The second form of robust optimization is their innovation. Here, one does still get the PF of the original objective f, but subject to a new constraint, that ||f(x) - f{\_}p(x)|| / ||f(x)|| $\backslash$leq $\backslash$eta, for some user-defined $\backslash$eta. f{\_}p here is either f{\_}eff, or else is the maximum value of f in the delta-neighborhood around x, or whatever else best fits the goals of the analyst. They tout the fact that a user can define $\backslash$eta to fit her goals, but they downplay (ignore?) the fact that she still has to select $\backslash$delta, too. They show that the two kinds of robust optimization can get pretty different results, in some cases.
One issue with this that comes to mind is that in its manifestation here it seems to take acount in a very rudimentary way of prior knowledge about the uncertainty of the inputs. You just define a $\backslash$delta, maybe a different one for each input but even then it seems that you just get a uniform sampling or a LHC over the region defined by $\backslash$delta. Our approach does this better. But then, it seems to me like it would probably be pretty easy to improve this approach in that area too.
Less clear is how you'd get this approach to include code uncertainty, the way our approach does.
Maybe the central difference is that our approach returns an estimate of the global PF with quantified uncertainty, whereas this approach doesn't even attempt to provide an estimate of the global PF, much less with quantified uncertainties. Really the two approaches seem like night and day, considered in this way.},
author = {Deb, Kalyanmoy and Gupta, Himanshu},
doi = {10.1162/evco.2006.14.4.463},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Deb, Gupta - 2006 - Introducing robustness in multi-objective optimization(2).pdf:pdf},
issn = {10636560},
journal = {Evolutionary Computation},
keywords = {Evolutionary algorithms,Global and local optimal solutions,Multi-objective optimization,Pareto-optimal solutions,Robust solutions},
mendeley-groups = {Calibration},
month = {dec},
number = {4},
pages = {463--494},
title = {{Introducing robustness in multi-objective optimization}},
volume = {14},
year = {2006}
}
@Inbook{Dean2017,
author="Dean, Angela
and Voss, Daniel
and Dragulji{\'{c}}, Danel",
title="Response Surface Methodology",
bookTitle="Design and Analysis of Experiments",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="565--614",
abstract="Experiments for fitting a predictive model involving several continuous variables are known as response surface experiments. The objectives of response surface methodology include the determination of variable settings for which the mean response is optimized and the estimation of the response surface in the vicinity of this good location. The first part this chapter discusses first-order designs and first-order models, including lack of fit and the path of steepest ascent to locate the optimum. The second part of the chapter introduces second-order designs and models for exploring the vicinity of the optimum location. The application of response surface methodology is demonstrated through a real experiment. The concepts introduced in this chapter are illustrated through the use of SAS and R software.",
isbn="978-3-319-52250-0",
doi="10.1007/978-3-319-52250-0_16",
url="https://doi.org/10.1007/978-3-319-52250-0_16"
}
@inproceedings{Olalotiti-Lawal2015,
annote = {r2019-03-27
This paper presents a method for probabilistic estimation of a Pareto front for a multiobjective optimization problem using MCMC. Sound familiar?},
author = {Olalotiti-Lawal, Feyi and Datta-Gupta, Akhil},
booktitle = {SPE Annual Technical Conference and Exhibition},
doi = {10.2118/175144-MS},
file = {:C$\backslash$:/Users/carle/Documents/Mendeley/Olalotiti-Lawal, Datta-Gupta - 2015 - A Multi-Objective Markov Chain Monte Carlo Approach for History Matching and Uncertainty Quanti(2).pdf:pdf},
isbn = {978-1-61399-376-7},
keywords = {Differential Evolution,Grid Connectivity Transform,Markov Chain Monte Carlo,Subsurface Uncertainty Quantification},
mendeley-groups = {Calibration},
month = {sep},
publisher = {Society of Petroleum Engineers},
title = {{A Multi-Objective Markov Chain Monte Carlo Approach for History Matching and Uncertainty Quantification}},
url = {http://www.onepetro.org/doi/10.2118/175144-MS},
year = {2015}
}
@article{Mockus1978,
  title={The application of Bayesian methods for seeking the extremum},
  author={Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  journal={Towards global optimization},
  volume={2},
  number={117-129},
  pages={2},
  year={1978}
}
@article{Brown2018,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/44841922},
 abstract = {Standard methods in computer model calibration treat the calibration parameters as constant throughout the domain of control inputs. In many applications, systematic variation may cause the best values for the calibration parameters to change across different settings. When not accounted for in the code, this variation can make the computer model inadequate. We propose a framework for modeling the calibration parameters as functions of the control inputs to account for a computer model's incomplete system representation in this regard, while simultaneously allowing for possible constraints imposed by prior expert opinion. We demonstrate how inappropriate modeling assumptions can mislead a researcher into thinking a calibrated model is in need of an empirical discrepancy term when it is only needed to allow for a functional dependence of the calibration parameters on the inputs. We apply our approach to plastic deformation of a visco-plastic selfconsistent material in which the critical resolved shear stress is known to vary with temperature.},
 author = {D. Andrew Brown and Sez Atamturktur},
 journal = {Statistica Sinica},
 number = {2},
 pages = {721--742},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {NONPARAMETRIC FUNCTIONAL CALIBRATION OF COMPUTER MODELS},
 volume = {28},
 year = {2018}
}
@book{Fletcher2013,
  title={Practical methods of optimization},
  author={Fletcher, Roger},
  year={2013},
  publisher={John Wiley \& Sons}
}
@article{Gelman1992a,
abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
author = {Gelman, Andrew and Rubin, Donald B.},
doi = {10.1214/ss/1177011136},
journal = {Statistical Science},
mendeley-groups = {Calibration},
number = {4},
pages = {457--472},
publisher = {Institute of Mathematical Statistics},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
url = {http://projecteuclid.org/euclid.ss/1177011136},
volume = {7},
year = {1992}
}
@article{Abaqus2012,
  title={Abaqus 6.12 documentation},
  author={Simulia, Dassault Systemes},
  journal={Providence, Rhode Island, US},
  volume={261},
  year={2012}
}
@article{Olalotiti2018,
  title={A multiobjective Markov chain Monte Carlo approach for history matching and uncertainty quantification},
  author={Olalotiti-Lawal, Feyi and Datta-Gupta, Akhil},
  journal={Journal of Petroleum Science and Engineering},
  volume={166},
  pages={759--777},
  year={2018},
  publisher={Elsevier}
}
@article{Saibaba2019,
author = {Saibaba, A. K. and Bardsley, J. and Brown, D. A. and Alexanderian, A.},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
number = {3},
pages = {1105--1131},
title = {{Efficient marginalization-based MCMC methods for hierarchical Bayesian inverse problems}},
volume = {7},
year = {2019}
}