\chapter{Introduction}

\section{Computer model calibration}

\subsection{Computer experiments} \label{computer_experiments}

Suppose that one wishes to improve one's understanding of, say, the movement of people in a crowd escaping from a building in a crisis situation. 
This is an example of an area in which field data, i.e.\ real-world observations, are extremely difficult to acquire. 
Merely assembling a crowd of research subjects in one place is costly and difficult. 
Asking them to flee a building may result in behaviors which are unlike those in real crisis situations -- but which may nonetheless present unacceptable physical risk to the subjects. 
Inducing them to flee through the generation of a (real or apparent) crisis is similarly infeasible. 
Observational data are likewise scarce here, since panic-inducing crises are by their nature difficult to predict and chaotic in ways that hinder the orderly collection of data.

Such problems can also hinder much more modest research aims.
The cost of a task as simple as measuring, e.g., the time required for balls of a variety of sizes to fall from a variety of heights, can be prohibitive depending upon one's available resources.
In the face of these difficulties, computer models offer a third alternative to the choice between attempting field data collection and giving up on the hope of progress. 
Using existing theory concerning human psychology and movement, it is possible to construct a computer model simulating the behavior of people evacuating from a large building~\cite{Thompson1995}, to allow one to observe simulated evacuation behaviors.
With such a model, a user might input floor plans for each floor of a building, the locations of staircases and exits, the locations of occupants, the dimensions of their bodies, routes to exits, and so on. 
The user could then observe how long it takes each simulated occupant to exit the building.
Similarly, a simple physics-based model can simulate the height-time curve of balls falling through the air \citep{Gattiker2017}, with the user specifying the height of the fall and the radius of the ball.
Thus, computer models provide a means to collect data which might otherwise be inaccessible. 

The study of computer models from a statistical perspective calls for specialized tools and techniques. 
Gaussian processes (GPs) are popular for modeling the output of computer code when the code is too computationally expensive to allow for repeated evaluation. 
The GP then provides a computationally inexpensive emulator for the computer code. 
There are three reasons the popularity of GPs as emulators: (1) The use of a GP does not require detailed foreknowledge of the approximate parametric form of the computer model. Researchers often lack such foreknowledge in the case of complex computer models. (2) GPs easily interpolate the observed data. This is an advantage when the observations come from deterministic computer code that is free of observation error. (3) The variance of a GP provides a natural form of uncertainty quantification. 

\subsection{Computer model calibration} \label{computer_model_calibration}

Again considering the hypothetical model of evacuation behavior, suppose a user wishes to compare two different proposed building codes to be enforced in a given area. 
One might use average interpersonal distance in a crisis evacuation as an input parameter for such a model, both to settle the initial physical distribution of people throughout the building and to influence their behavior during evacuation.
It is well-established that customs of interpersonal distance vary across locales \cite{Sorokowska2017}. 
These values may be unknown for the case of a particular region, particularly in a crisis setting. 
Thus we may wish to find the true values for interpersonal distance in that region. 
With respect to the physics-based simulator, the model may require the user to estimate the drag coefficient of the ball.
In both cases, the user will need to \textit{calibrate} these parameters in the model.

Broadly, in model calibration, we may consider a model to be of the form $\eta(\mathbf x,\boldsymbol \theta)$, where $(\mathbf x,\boldsymbol \theta)$ comprise all inputs to the model. Input vector $\mathbf x$ is the collection of inputs that are known and/or under the control of the researcher (in the evacuation example, this might include the building layout; in the freefall example, it might include the height of the fall).  
The vector of calibration inputs $\boldsymbol \theta$ is the collection of parameters the values of which are unknown. 
These must be estimated for successful simulation. 
Thus where $f$ describes the true system and $\mathbf y$ our observations of that system, consider the model to be 
\begin{equation} \label{eq:model_gen}
	y(\mathbf x)=f(\mathbf x)+\epsilon(\mathbf x)=\eta(\mathbf x,\boldsymbol \theta) + \delta(\mathbf x)+\epsilon(\mathbf x)
\end{equation} 
where $\delta(\cdot)$ describes the model discrepancy -- i.e., the bias of the model as an estimate of the real system -- and $\epsilon(\cdot)$ is a mean-zero observation error, often i.i.d.\ Gaussian. 
To undertake model calibration, one must have access to at least some observations of the real system, using them to ``tune'' the values of the calibration parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Computer models are often imperfect reflections of the phenomena they purport to represent.
%Often these imperfections take the form of a need for \textit{calibration}.
%That is, the performance of a computer model is often dependent upon certain settings or assignments of values in the model, which we may call calibration parameters. %
%For example, in a model designed to estimate the height-time curve of a ball in free fall from a given height, the model output will depend on (among other things) the drag coefficient $C$ of the ball \cite{Gattiker2017}.
%Given experimental data describing some observed height-time curves of the ball in question, one can use some calibration methodology to attempt to estimate $C$.

Though the above examples relate to cases of estimating the true value of some parameter, calibration need not take this form.
Calibration can also take the form of setting values which optimize the model's faithfulness to reality, even when the values being set do not correspond to any particular real features.
For example, a classification model may return scores which constitute some measure of confidence of category membership.
A probability calibration of the model would involve mapping these confidence scores to values in the interval $[0,1]$ such that the resulting transformed values are plausible estimates of the probability of category membership \cite{Niculescu2005}.

Given a set of data which may be used for calibration, the available calibration methods are diverse. 
Often, calibration is approached in an \textit{ad hoc} manner, by manually searching for a set of values for the parameters of interest that cause the model output to come acceptably close to the observed data.
This manual approach is often improved by automating it as a grid search or stochastic search, where some measure of model performance is optimized in a brute force manner.

Even among more sophisticated approaches, calibration methods often yield only a point estimate of the appropriate value(s) for the calibration parameter(s).
As a result, much interest in the past two decades has centered on Bayesian methods for model calibration. 
The appeal of a Bayesian approach to model calibration lies in the fact that the calibration parameters are a source of uncertainty for the model. 
This uncertainty should be quantified so that its effect on the model can be made explicit. 
One can thus use Bayesian methods to arrive at a posterior distribution of the calibration parameters which both balances our prior knowledge about the calibration parameters with what can be learned from the available data and also allows for accurate uncertainty quantification on the model outputs. 

% Lit review of computer model calibration

\citet{Trucano2006} provide a high-level overview of the relationship of model calibration to both model validation and sensitivity analysis.
Much of the field of computer model calibration takes place either within the framework of, or as a response to, the work of \citet{Kennedy2001}.
This work established the dominant paradigm for Bayesian computer model calibration.
In this dissertation we refer to that paradigm as the KOH framework.
In KOH, one weds a set of experimental data to a computer model that stands in need of calibration, often mediated through a Gaussian process (GP) surrogate model when the original model is of high computational complexity.
Systematic discrepancy between the model and the true system can also be represented by a GP, and priors are placed on the calibration parameters.
GP hyperparameters are either estimated via maximum likelihood estimation (MLE) or else are also assigned prior distributions.
Via integration or some form of Markov-chain Monte Carlo (MCMC, \cite{Gelfand1990}), a posterior distribution on the calibration parameters is obtained.

Of course, not all work in computer model calibration derives from the KOH framework.
For example, \citet{Craig1997} provide an influential demonstration using Bayes linear methods to calibrate a model through a combination of expert judgments (about means and variances of model parameters) and information from runs of low-fidelity versions of the model.
Contemporaneously to \citet{Kennedy2001}, \citet{Cox2001} offer a similar calibration framework from a frequentist perspective, including the use of GPs as metamodels for computational efficiency. 
The authors use MLEs of calibration parameters and GP hyperparameters.
Similarly, and more in response to the KOH framework, \citet{Loeppky2006} also provide an MLE-based alternative to KOH that is designed to improve the identifiability of the calibration parameters when model discrepancy is present (i.e., when the model is a systematically biased representation of the target phenomenon).
More recently, \citet{Wong2014} describe general framework for a semi-parametric frequentist approach to model calibration, using bootstrapping to provide uncertainty quantification.

Far more common in the field of computer model calibration are extensions and refinements of the KOH framework.
\citet{Higdon2004} provide such an extension, adding uncertainty quantification to KOH calibration, to estimate remaining uncertainty regarding the values of the calibration parameter(s).
\citet{Williams2006} further refine and exemplify this approach.
\citet{Bayarri2007a} extend KOH to effect simultaneous validation and calibration of a computer model, while \citet{Bayarri2007b} apply this approach to functional data, using a hierarchical representation of coefficients of a wavelet basis.
\citet{Paulo2012} further extend the work of \citet{Bayarri2007a} to the case of multivariate model output.
While most work in this area assumes that the computer model is deterministic, \citet{Pratola2018} provide an example of non-deterministic model calibration.

\citet{Paulo2012} are part of a diverse array of projects aiming to shore up a weakness of the KOH framework: its difficulty in accommodating high-dimensional data and/or large data sets in a computationally efficient way.
\citet{Higdon2008a} focus on meeting the computational challenges of high-dimensional, large data sets by using principal components basis representations for dimensionality reduction.
Whereas that work is concerned with the dimensionality of model outputs, \citet{Drignei2012} are concerned rather with the model inputs.
They increase the numerical stability of model calibration by using a global sensitivity analysis to reduce the model inputs' dimensionality.
\citet{Bhat2010} demonstrate the application of the KOH framework to multivariate spatial output, and \citet{Pratola2013} apply KOH to nonstationary spatial-temporal field output.
\citet{Higdon2013} adapt KOH to employ an ensemble Kalman filter rather than GPs, for improved computational efficiency.
\citet{Yuan2013} offer a sequential approach to model calibration, in which each evaluation of the high-fidelity model is selected using the previous evaluations to minimize the resulting calibration uncertainty. 
Such an approach reduces the total number of required model evaluations and thereby the total computational cost of the calibration procedure.

Other works have endeavored to address another known weakness of KOH: poor identifiability of the calibration parameters in the face of model discrepancy.
With respect to this problem, \citet{Brynjarsdottir2014} emphasize the importance of strong priors on the model discrepancy term when performing model calibration.
And for the purpose of improving identifiability over previous approaches, \citet{Gu2018} propose a novel stochastic process that combines elements of GPs with $L_2$ calibration (in which the calibration parameter is chosen so as to minimize the $L_2$ norm of the discrepancy term).

Some works in the area of computer model calibration are premised upon broadening the conception of calibration beyond the idea of relating experimental data to a computer model of the experimental phenomenon.
Model calibration can be seen more generally as a method for relating two or more different sources of data with varying costs and varying levels of fidelity.
In this vein \citet{Kennedy2000} and \citet{Qian2006} explore methods for integrating ``high'' and ``low'' models to build a computationally efficient surrogate.
\citet{Goh2013} extend KOH to cases of more than two different levels of fidelity.

%	3. Comparison of calibration and design (Aug. 24)

\section{Calibration versus model-assisted design}

``Design'' used here means not experimental design, but rather refers to making engineering choices to try to achieve a desired outcome.
Thus the sort of design considered here is related to the field of optimization.
Where calibration involves setting input parameters to induce the model to approximate reality, design involves choosing input parameters to induce an engineered system to behave in some desired way.
To understand the way the present work applies methods of computer model calibration, it is helpful to consider the relationship between the two tasks of calibration and design.

At the highest level, calibration can be conceived as follows.
One has a model $\eta: \mathbb{R}^{p+q} \to \mathbb{R}^m$, as well as a set of data $\mathbf{y}$ and corresponding $p-$dimensional inputs $\mathbf{x_y}$.
We can thus consider $\eta$ to be a function of two vectors, $\mathbf x\in\mathbb R^p$ and $\mathbf t\in\mathbb R^q$.
Often (but not exclusively) $\mathbf y$ is a set of observed experimental outcomes from a real phenomenon $f:\mathbb{R}^p\to\mathbb{R}^m$ where $\eta$ is a model of $f$.
One wishes to use $\mathbf y$ to select values $\boldsymbol{\theta}$ such that setting $\mathbf t=\boldsymbol{\theta}$ induces the output of $\eta(\mathbf {x_y},\boldsymbol{\theta})$ to approximate $\mathbf y$.
The selection of these values is the calibration of $\eta$.

Similarly, model-assisted design can be conceived as follows.
One has a model $\eta: \mathbb{R}^{p+q} \to \mathbb{R}^m$, as well as a set of data $\mathbf{y}$ and corresponding $p-$dimensional inputs $\mathbf{x_y}$.
We can thus consider $\eta$ to be a function of two vectors, $\mathbf x\in\mathbb R^p$ and $\mathbf t\in\mathbb R^q$.
Often (but not exclusively) $\mathbf y$ is a set of target outcomes one wishes to achieve in some system $f:\mathbb{R}^p\to\mathbb{R}^m$ where $\eta$ is a model of $f$.
One wishes to use $\mathbf y$ to select values $\boldsymbol{\theta}$ such that setting $\mathbf t=\boldsymbol{\theta}$ induces the output of $\eta(\mathbf {x_y},\boldsymbol{\theta})$ to approximate $\mathbf y$.
The selection of these values is model-assisted design using $\eta$.

Seen in this way, calibration and design are surprisingly similar undertakings. 
This raises the possibility of applying tools from one domain to problems in the other.
One of the primary goals of the present work is to adapt the KOH framework for model-assisted design, and to demonstrate its utility and flexibility in that context.

%	4. Introduction to model-assisted design (Aug. 24)

\section{Model-assisted design}

The present work thus must be situated in the context of model-assisted design.
In model-assisted design, one optimizes a model of the system of interest with respect to some objective function.
The resulting optimal model inputs are treated as estimates of the optimal design settings for the system.
The present work is intended to be applicable to problems of multi-objective design.

In the field of model- and metamodel-assisted design, \citet{Sacks1989} provide a very influential discussion of strategies to accommodate computer models of high computational complexity with GP surrogates.
Following in this vein, \citeauthor{Santner2003a}'s \citeyearpar{Santner2003a} foundational work serves as a focal point for much subsequent discussion of how best to learn from computer models in conjunction with physical experiments, for purposes including but not limited to engineering design.
\citet{Currin1988} and \citet{Currin1991} develop a similar approach to that of Sacks et al 1989, but with a Bayesian interpretation.
\citet{Mitchell1992} demonstrate the resulting methodology.
\citet{Craig2001} discuss Bayesian methods for forecasting using a computer model under uncertainty.
\citet{Simpson2008} provide an overview and retrospective of progress in the area of model- and metamodel-assisted design optimization since the work of \citet{Sacks1989}, and highlight the value of employing multiple models with varying degrees of fidelity.
\citet{Bartz2017} give a similar, more up-to-date discussion, focusing on discrete optimization problems.
\cite{Westermann2019} offer a comprehensive and illustrative discussion of the use of metamodels for optimal design of sustainable buildings, including a focus on sensitivity analysis and uncertainty quantification.

\section{Gaussian process metamodels}

The present work shares with many KOH-based, Bayesian optimization (BO)-based, and other approaches a reliance on GP metamodels as surrogates for computationally expensive models.
Long popular in geostatistics (where GP regression is referred to as \textit{kriging}; \cite{Cressie2015}), the use of GPs as computer model surrogates was popularized by \citet{Sacks1989}.
GPs are attractive in this context due to their flexibility, and the ease with which they can interpolate observations of deterministic computer model output while providing closed-form expressions for uncertainty quantification.
The application of GPs in this domain is further explored by \citet{Santner2003a}, who include discussion of the choice of correlation function to suit the desired smoothness properties, and also include a discussion of hierarchical Gaussian random field models for cases when the user is not prepared to specify the desired smoothness.
Diagnostic methods for validating a GP surrogate model are explored by \citet{Bastos2009}.
Looking at broader applications than metamodeling, \citeauthor{Rasmussen2006}'s \citeyearpar{Rasmussen2006} work is a seminal text for practitioners seeking to employ GPs in a machine learning context.

Particularly relevant to the area of model calibration are explorations of the use of GPs to integrate multiple models with varying degrees of fidelity and computational complexity.
\citet{Qian2008a} propose a set of Bayesian hierarchical GP models to accommodate a case when low- and high-accuracy data is available.
With similar aims, \citet{Cumming2009} describe methods for combining low-\ and high-accuracy information into a multiscale emulator, as well as providing a design strategy for using the low-fidelity model to select the points at which to evaluate the high-fidelity model.
Expanding beyond the paradigm of two levels of fidelity, \citet{Goh2013} use GPs to build a model based on the results of models with several different degrees of fidelity.

Researchers have endeavored to expand the applicability of GP metamodels in numerous ways.
To accommodate models that cannot be represented by stationary GPs, \cite{Gramacy2008} implement a nonstationary GP metamodel using treed partitioning.
\citet{Qian2008} discuss the application of GP metamodels to discrete input spaces.
\citet{Gratiet2016} provide a comparative analysis of GPs and polynomial chaos expansions as metamodels in the context of global sensitivity analysis.
Other work has focused on a primary weakness of GPs -- their poor scalability for large or high-dimensional data sets.
\citet{Snelson2006} provide an influential method for generating sparse GPs to accommodate large data sets.
\citet{Liu2020} review a variety of methods for producing scalable GPs, divided broadly into the categories of global approximations that are based on the full data set, and local approximations that achieve scalability by relying on only a subset of the data at each point in the domain.

\section{Optimization}

The mathematical underpinnings of model-assisted engineering design are found in the field of optimization.
A primary strength of the present work is its success in quantifying uncertainty while undertaking calibration and design.
This is an area on which much recent innovation in optimization/design has focused.
The influential work of \citet{Rockafellar1991} describes an algorithm for using a limited set of observations (and hence under uncertainty as to appropriate stochastic model for the system) to solve a multiperiod optimization problem.
\citet{Sahinidis2004} offer a comprehensive overview of optimization under uncertainty, analyzing the strengths and weaknesses of a diverse array of approaches.
\citet{Jin2003} provide a comparative analysis of a variety of metamodeling techniques with respect to their performance when optimizing under uncertainty.
\citet{Peherstorfer2018} offer a survey of methods for optimization under uncertainty that take advantage of multiple models with varying levels of fidelity and computational expense.

Approaches to optimization and design vary enormously, but most methods can be classed as either gradient-based, evolutionary, or as a form of Bayesian optimization.
\citet{Ruder2016} offers a brief but comprehensive overview of gradient descent optimization algorithms.
\citet{Peitz2018} propose a gradient-based approach for multi-objective optimization under uncertainty in which descent directions are chosen so as to account for approximation error in the available information about the gradient of the objective function.
This leads one to identify a collection of subsets of the design space which contain the Pareto set for the objective function.
\citet{Vasilopoulos2019} demonstrate a method for using gradients to locate an approximate point along the Pareto front and then ``trace'' the Pareto front to explore it efficiently.

Evolutionary algorithms have become popular methods for optimization in recent times, partly due to their ability to treat the objective function as a black box, without requiring gradient information.
This is a feature shared with many forms of BO and with the present work.
\citet{Jin2003} provide a survey of work on the use of evolutionary optimization under uncertainty.
\citet{Zhou2011b} provide a similar survey, focusing on multiobjective evolutionary algorithms.
\citet{Deb2006} describe two approaches for achieving multi-objective optimization solutions that are robust to small perturbations in the input space.

The present work is a Bayesian approach to optimization problems, and thus might be considered a form of BO.
However, this is misleading, as the term BO is widely used to refer more specifically to a set of techniques loosely following in the footsteps of \citet{Jones1998}.
In the present work, ``BO'' refers more specifically to this subset of Bayesian approaches to optimization.
Other Bayesian approaches that do not fall within that umbrella include the work of \citet{Pelikan1999} and \citet{Pelikan2005}, who describe an application of a Bayesian framework to evolutionary optimization.

\citet{Jones1998} use a GP-based response surface approximation to define an acquisition function for sequential selection of evaluation points of the objective function.
\citet{Vazquez2009}, \citet{Bect2012}, and \citet{Chevalier2014} extend the approach of Jones, developing a stepwise uncertainty reduction methodology for sequential evaluation of the objective function to minimize resulting uncertainty regarding the optimum.
This approach, when applicable (i.e. when it is possible to evaluate the objective function adaptively), can significantly reduce the total number of objective function evaluations required for optimization.

BO's utility has been enhanced by a number of works aimed at practitioners and researchers seeking to apply BO in diverse areas.
\citet{Shahriari2016} introduce the core concepts of BO, and demonstrate its usage in a variety of applications, while \citet{Frazier2018} offers a practical tutorial on the use of BO techniques.
\citet{Snoek2012} demonstrate the application of BO to optimization of the hyperparameters of machine learning models.
\citet{Calandra2016} provide a demonstrative evaluation of Bayesian optimization methodologies as applied to optimization under uncertainty in the specific domain of robot locomotion parametrization.
\citet{Picheny2019} demonstrate how Bayesian optimization methods can be used to identify Nash equilibria in a game theoretic context.

In addition, many efforts have been made to extend the capabilities of BO and to address its shortcomings.
\citet{Lizotte2008} reviews the weaknesses of BO -- high computational complexity, poorly understood approximation of the response surface to the objective function, and failure to take advantage of the differentiability of objective functions -- and demonstrates how to overcome or mitigate these weaknesses.
\citet{Letham2019} develop strategies for applying BO in contexts that suffer from high levels of noise, and \citet{Snoek2015} use neural networks for basis function regression of the objective function in place of GPs, to improve the computational efficiency of BO.

\section{Applications}

In the present work we adapt the Bayesian model calibration framework stemming from the work of \citet{Kennedy2001} to apply both to calibration and to model-assisted design. 
We apply our methodology to two practical applications.
One involves the engineering design of a wind turbine blade of fixed outer geometry.
The blade is a constructed using a composite material, with a fixed choice of matrix and filler materials.
Other properties of the composite may be varied to achieve performance and cost goals for the system.
Specifically, we assist in the selection of the \textit{volume fraction} of the material (the ratio of filler to matrix, by volume) as well as the thickness (in millimeters) of the blade material.
This sort of material selection would traditionally be performed in an \textit{ad hoc} manner, satisficing using a list of available materials that are designed separate from this or any particular application.
We wed material design with the goals of the engineering design application, so that our search space is not limited to pre-existing or previously studied composites.
The engineering goals for our application are the simultaneous minimization of the tip deflection of the blade (in meters) when under load, the twist angle (in radians) of the blade under load, and the cost (in USD) of the composite used for the blade.
This application is thus an example of multiobjective optimization. 

The other practical application considered in the present work is a vibration isolation design problem.
This is a system in which a one mass oscillator is anchored by two leaf springs.
An impulse force is applied to the oscillator, inducing vibration in the system.
The behavior of the resulting system may be measured via its vertical displacement over time, as well as its amplitude response over frequencies.
The finite element model of the system must be calibrated with respect to the elastic modulus of the leaf springs, and the calibrated system must be used for engineering design to minimize resulting vibration.

The following chapters are organized as follows.
In chapter two, we adapt the KOH framework and demonstrate its potential as a technique for model-assisted design.
We apply the resulting methodology to the wind turbine blade application.
As that system involves multiobjective optimization, it provides an opportunity to demonstrate our method's ability to explore the Pareto front of a system while providing uncertainty quantification of the resulting estimates.
In chapter three, we expand the framework developed in chapter two, in order to effect both calibration and engineering design simultaneously.
The ability to perform both of these tasks simultaneously, within a single model and with a single set of experimental observations, is a novel benefit of our approach.
We demonstrate the resulting methodology in the vibration isolation design application.
Chapter four concludes with a discussion of the results of chapters two and three, and thoughts about future directions for research in this area.
